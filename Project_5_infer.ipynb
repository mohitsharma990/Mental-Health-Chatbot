{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "lB0A6bwfk1PR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from tokenizers import Tokenizer\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_r9U2Dd9k1PS"
      },
      "source": [
        "## 1. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQZfoni7k1PT",
        "outputId": "7410253e-6862-4718-a229-b4028b10cbc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "MODEL_PATH = 'empathetic-transformer-basic-best (2).pt'\n",
        "TOKENIZER_PATH = 'wp_tokenizer.json'\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "MAX_LEN = 100\n",
        "DEFAULT_STRATEGY = \"topk\"\n",
        "DEFAULT_K = 15\n",
        "DEFAULT_P = 0.92\n",
        "DEFAULT_TEMPERATURE = 0.75\n",
        "\n",
        "PAD_TOKEN = \"<PAD>\"\n",
        "SOS_TOKEN = \"<SOS>\"\n",
        "EOS_TOKEN = \"<EOS>\"\n",
        "UNK_TOKEN = \"<UNK>\"\n",
        "\n",
        "print(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oKyXkCYk1PU"
      },
      "source": [
        "## 2. Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "935je8Guk1PU"
      },
      "outputs": [],
      "source": [
        "def text_to_sequence_tokenizer(text, tokenizer):\n",
        "    \"\"\"Encodes text to a list of token IDs, adding SOS/EOS.\"\"\"\n",
        "    encoded = tokenizer.encode(text)\n",
        "    return [SOS_IDX] + encoded.ids + [EOS_IDX]\n",
        "\n",
        "def sequence_to_text_tokenizer(sequence, tokenizer):\n",
        "    \"\"\"Decodes a list of token IDs back to text, removing special tokens.\"\"\"\n",
        "    ids = [idx for idx in sequence if idx not in [PAD_IDX, SOS_IDX, EOS_IDX]]\n",
        "    return tokenizer.decode(ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6pQjuFok1PU"
      },
      "source": [
        "## 3. Generation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "qYaQfaczk1PV"
      },
      "outputs": [],
      "source": [
        "def generate_response_sampling(\n",
        "    sentence,\n",
        "    model,\n",
        "    tokenizer,\n",
        "    device,\n",
        "    max_len=MAX_LEN,\n",
        "    strategy=\"topk\",\n",
        "    k=10,\n",
        "    p=0.9,\n",
        "    temperature=0.8\n",
        "    ):\n",
        "    \"\"\"Generates a response using the transformer model with sampling.\"\"\"\n",
        "    assert strategy in ['greedy', 'topk', 'topp'], \"Strategy must be 'greedy', 'topk', or 'topp'\"\n",
        "    assert temperature > 0, \"Temperature must be positive\"\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    tokens = text_to_sequence_tokenizer(sentence, tokenizer)\n",
        "\n",
        "    if len(tokens) > max_len:\n",
        "         print(f\"Warning: Input sentence truncated to {max_len} tokens.\")\n",
        "         tokens = tokens[:max_len]\n",
        "\n",
        "    src_tensor = torch.LongTensor(tokens).unsqueeze(0).to(device)\n",
        "    src_mask = model.make_src_mask(src_tensor)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        enc_src = model.encoder(src_tensor, src_mask)\n",
        "\n",
        "    trg_indices = [SOS_IDX]\n",
        "\n",
        "    for i in range(max_len - 1):\n",
        "        trg_tensor = torch.LongTensor(trg_indices).unsqueeze(0).to(device)\n",
        "        trg_mask = model.make_trg_mask(trg_tensor)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
        "\n",
        "        pred_token_logits = output[-1, 0, :]\n",
        "\n",
        "        pred_token_logits = pred_token_logits / temperature\n",
        "\n",
        "        pred_token_probs = F.softmax(pred_token_logits, dim=-1)\n",
        "\n",
        "        next_token_id = -1\n",
        "        if strategy == 'greedy':\n",
        "            next_token_id = torch.argmax(pred_token_probs).item()\n",
        "        elif strategy == 'topk':\n",
        "            topk_probs, topk_indices = torch.topk(pred_token_probs, k=min(k, pred_token_probs.size(-1))) # Ensure k is not > vocab size\n",
        "            mask = torch.zeros_like(pred_token_probs)\n",
        "            mask.scatter_(0, topk_indices, 1.0)\n",
        "            filtered_probs = pred_token_probs * mask\n",
        "            sum_filtered_probs = torch.sum(filtered_probs)\n",
        "            if sum_filtered_probs > 1e-9:\n",
        "                 filtered_probs = filtered_probs / sum_filtered_probs\n",
        "                 next_token_id = torch.multinomial(filtered_probs, num_samples=1).item()\n",
        "            else:\n",
        "                 print(\"Warning: Top-k resulted in zero probability sum. Using EOS.\")\n",
        "                 next_token_id = EOS_IDX\n",
        "        elif strategy == 'topp':\n",
        "            sorted_probs, sorted_indices = torch.sort(pred_token_probs, descending=True)\n",
        "            cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "            sorted_indices_to_remove = cumulative_probs > p\n",
        "            sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()\n",
        "            sorted_indices_to_remove[0] = 0\n",
        "            indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "            pred_token_probs[indices_to_remove] = 0.0\n",
        "            sum_filtered_probs = torch.sum(pred_token_probs)\n",
        "            if sum_filtered_probs > 1e-9:\n",
        "                 filtered_probs = pred_token_probs / sum_filtered_probs\n",
        "                 next_token_id = torch.multinomial(filtered_probs, num_samples=1).item()\n",
        "            else:\n",
        "                 print(\"Warning: Top-p resulted in zero probability sum. Using most likely token.\")\n",
        "                 next_token_id = sorted_indices[0].item()\n",
        "\n",
        "        trg_indices.append(next_token_id)\n",
        "\n",
        "        if next_token_id == EOS_IDX:\n",
        "            break\n",
        "\n",
        "    trg_tokens_text = sequence_to_text_tokenizer(trg_indices, tokenizer)\n",
        "\n",
        "    return trg_tokens_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5dnryFTk1PV"
      },
      "source": [
        "## 4. Load Tokenizer and Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0UQ_tGUk1PW",
        "outputId": "d85384c0-008b-4db4-da4c-b521a3536794"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading tokenizer from wp_tokenizer.json...\n",
            "Tokenizer loaded.\n",
            "Tokenizer Vocabulary Size: 20000\n",
            "Special Token IDs: PAD=0, SOS=1, EOS=2, UNK=3\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(f\"Loading tokenizer from {TOKENIZER_PATH}...\")\n",
        "tokenizer = Tokenizer.from_file(TOKENIZER_PATH)\n",
        "print(\"Tokenizer loaded.\")\n",
        "\n",
        "INPUT_DIM = tokenizer.get_vocab_size()\n",
        "OUTPUT_DIM = INPUT_DIM\n",
        "print(f\"Tokenizer Vocabulary Size: {INPUT_DIM}\")\n",
        "\n",
        "PAD_IDX = tokenizer.token_to_id(PAD_TOKEN)\n",
        "SOS_IDX = tokenizer.token_to_id(SOS_TOKEN)\n",
        "EOS_IDX = tokenizer.token_to_id(EOS_TOKEN)\n",
        "UNK_IDX = tokenizer.token_to_id(UNK_TOKEN)\n",
        "\n",
        "if None in [PAD_IDX, SOS_IDX, EOS_IDX, UNK_IDX]:\n",
        "     print(\"Error: One or more special tokens not found in tokenizer vocab!\")\n",
        "else:\n",
        "    print(f\"Special Token IDs: PAD={PAD_IDX}, SOS={SOS_IDX}, EOS={EOS_IDX}, UNK={UNK_IDX}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_SUBSET_SIZE = None\n",
        "MAX_LEN = 60\n",
        "VOCAB_SIZE = 20000\n",
        "TOKENIZER_FILE = \"wp_tokenizer.json\"\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.0005\n",
        "N_EPOCHS = 50\n",
        "CLIP = 1.0\n",
        "PATIENCE = 10\n",
        "\n",
        "D_MODEL = 256\n",
        "N_HEADS = 8\n",
        "ENC_LAYERS = 3\n",
        "DEC_LAYERS = 3\n",
        "PF_DIM = 512\n",
        "DROPOUT = 0.3\n",
        "\n",
        "SEED = 1234\n",
        "random.seed(SEED)\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "PAD_TOKEN = \"<PAD>\"\n",
        "SOS_TOKEN = \"<SOS>\"\n",
        "EOS_TOKEN = \"<EOS>\"\n",
        "UNK_TOKEN = \"<UNK>\"\n",
        "SPECIAL_TOKENS = [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN, UNK_TOKEN]\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Injects positional information into the input embeddings.\"\"\"\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multi-Head Attention mechanism.\"\"\"\n",
        "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "\n",
        "        self.fc_q = nn.Linear(d_model, d_model)\n",
        "        self.fc_k = nn.Linear(d_model, d_model)\n",
        "        self.fc_v = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.fc_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.register_buffer(\"scale\", torch.sqrt(torch.FloatTensor([self.head_dim])))\n",
        "\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            query: Tensor, shape [query_len, batch_size, d_model]\n",
        "            key: Tensor, shape [key_len, batch_size, d_model]\n",
        "            value: Tensor, shape [value_len, batch_size, d_model] (value_len == key_len)\n",
        "            mask: Tensor, shape [batch_size, 1, query_len, key_len] or broadcastable.\n",
        "                  Masks positions where attention should be zero (e.g., padding).\n",
        "        \"\"\"\n",
        "        batch_size = query.shape[1]\n",
        "\n",
        "        Q = self.fc_q(query)\n",
        "        K = self.fc_k(key)\n",
        "        V = self.fc_v(value)\n",
        "\n",
        "        Q = Q.view(query.shape[0], batch_size, self.n_heads, self.head_dim).permute(1, 2, 0, 3)\n",
        "        K = K.view(key.shape[0], batch_size, self.n_heads, self.head_dim).permute(1, 2, 0, 3)\n",
        "        V = V.view(value.shape[0], batch_size, self.n_heads, self.head_dim).permute(1, 2, 0, 3)\n",
        "\n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale.to(Q.device)\n",
        "\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, -1e10)\n",
        "\n",
        "        attention = torch.softmax(energy, dim=-1)\n",
        "\n",
        "        attention = self.dropout(attention)\n",
        "\n",
        "        x = torch.matmul(attention, V)\n",
        "\n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        x = x.view(batch_size, query.shape[0], self.d_model)\n",
        "\n",
        "        x = x.permute(1, 0, 2)\n",
        "\n",
        "        x = self.fc_o(x)\n",
        "\n",
        "        return x, attention\n",
        "\n",
        "class PositionwiseFeedforward(nn.Module):\n",
        "    \"\"\"Position-wise Feedforward Network.\"\"\"\n",
        "    def __init__(self, d_model, pf_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(d_model, pf_dim)\n",
        "        self.fc2 = nn.Linear(pf_dim, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape [seq_len, batch_size, d_model]\n",
        "        \"\"\"\n",
        "        x = self.dropout(torch.relu(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    \"\"\"A single layer of the Transformer Encoder.\"\"\"\n",
        "    def __init__(self, d_model, n_heads, pf_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "        self.ff = PositionwiseFeedforward(d_model, pf_dim, dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src: Tensor, shape [src_len, batch_size, d_model]\n",
        "            src_mask: Tensor, mask for self-attention (hides padding)\n",
        "        \"\"\"\n",
        "        _src, _ = self.self_attn(src, src, src, src_mask)\n",
        "        src = self.norm1(src + self.dropout(_src))\n",
        "\n",
        "        _src = self.ff(src)\n",
        "        src = self.norm2(src + self.dropout(_src))\n",
        "\n",
        "        return src\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    \"\"\"A single layer of the Transformer Decoder.\"\"\"\n",
        "    def __init__(self, d_model, n_heads, pf_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.masked_self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "        self.encoder_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "        self.ff = PositionwiseFeedforward(d_model, pf_dim, dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            trg: Tensor, shape [trg_len, batch_size, d_model] (target sequence embeddings)\n",
        "            enc_src: Tensor, shape [src_len, batch_size, d_model] (encoder output)\n",
        "            trg_mask: Tensor, mask for self-attention (hides padding and future tokens)\n",
        "            src_mask: Tensor, mask for encoder-attention (hides padding in source)\n",
        "        \"\"\"\n",
        "        _trg, _ = self.masked_self_attn(trg, trg, trg, trg_mask)\n",
        "        trg = self.norm1(trg + self.dropout(_trg))\n",
        "\n",
        "        _trg, attention = self.encoder_attn(trg, enc_src, enc_src, src_mask)\n",
        "        trg = self.norm2(trg + self.dropout(_trg))\n",
        "\n",
        "        _trg = self.ff(trg)\n",
        "        trg = self.norm3(trg + self.dropout(_trg))\n",
        "\n",
        "        return trg, attention\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"The Transformer Encoder stack.\"\"\"\n",
        "    def __init__(self, input_dim, d_model, n_layers, n_heads, pf_dim, dropout, max_len=MAX_LEN):\n",
        "        super().__init__()\n",
        "        self.tok_embedding = nn.Embedding(input_dim, d_model)\n",
        "        self.pos_embedding = PositionalEncoding(d_model, dropout, max_len=5000 if max_len < 5000 else max_len)\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, pf_dim, dropout)\n",
        "                                     for _ in range(n_layers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\"scale\", torch.sqrt(torch.FloatTensor([d_model])))\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src: Tensor, shape [batch_size, src_len] (input token IDs)\n",
        "            src_mask: Tensor, mask for padding in the source sequence\n",
        "        \"\"\"\n",
        "        batch_size = src.shape[0]\n",
        "        src_len = src.shape[1]\n",
        "\n",
        "        scale = self.scale.to(src.device)\n",
        "        src = self.dropout((self.tok_embedding(src) * scale))\n",
        "\n",
        "        src = src.permute(1, 0, 2)\n",
        "\n",
        "        src = self.pos_embedding(src)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask)\n",
        "\n",
        "        return src\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"The Transformer Decoder stack.\"\"\"\n",
        "    def __init__(self, output_dim, d_model, n_layers, n_heads, pf_dim, dropout, max_len=MAX_LEN):\n",
        "        super().__init__()\n",
        "        self.tok_embedding = nn.Embedding(output_dim, d_model)\n",
        "        self.pos_embedding = PositionalEncoding(d_model, dropout, max_len=5000 if max_len < 5000 else max_len)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, pf_dim, dropout)\n",
        "                                     for _ in range(n_layers)])\n",
        "        self.fc_out = nn.Linear(d_model, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\"scale\", torch.sqrt(torch.FloatTensor([d_model])))\n",
        "\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            trg: Tensor, shape [batch_size, trg_len] (target token IDs)\n",
        "            enc_src: Tensor, shape [src_len, batch_size, d_model] (encoder output)\n",
        "            trg_mask: Tensor, mask for target self-attention\n",
        "            src_mask: Tensor, mask for encoder-decoder attention (source padding)\n",
        "        \"\"\"\n",
        "        batch_size = trg.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "\n",
        "        scale = self.scale.to(trg.device)\n",
        "        trg = self.dropout((self.tok_embedding(trg) * scale))\n",
        "\n",
        "        trg = trg.permute(1, 0, 2)\n",
        "\n",
        "        trg = self.pos_embedding(trg)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
        "\n",
        "\n",
        "        output = self.fc_out(trg)\n",
        "\n",
        "        return output, attention\n",
        "\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    \"\"\"The main Seq2Seq Transformer model.\"\"\"\n",
        "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.device = device\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        \"\"\"Creates a mask for the source sequence to ignore padding tokens.\"\"\"\n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        return src_mask.to(self.device)\n",
        "\n",
        "    def make_trg_mask(self, trg):\n",
        "        \"\"\"Creates a mask for the target sequence to hide padding and future tokens.\"\"\"\n",
        "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        trg_len = trg.shape[1]\n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device=self.device)).bool()\n",
        "\n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\n",
        "        return trg_mask.to(self.device)\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src: Tensor, shape [batch_size, src_len] (source token IDs)\n",
        "            trg: Tensor, shape [batch_size, trg_len] (target token IDs)\n",
        "        \"\"\"\n",
        "        src = src.to(self.device)\n",
        "        trg = trg.to(self.device)\n",
        "\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "\n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "\n",
        "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "\n",
        "        return output, attention\n",
        "\n",
        "\n",
        "print(\"Initializing model components...\")\n",
        "enc = Encoder(INPUT_DIM, D_MODEL, ENC_LAYERS, N_HEADS, PF_DIM, DROPOUT, MAX_LEN).to(device)\n",
        "dec = Decoder(OUTPUT_DIM, D_MODEL, DEC_LAYERS, N_HEADS, PF_DIM, DROPOUT, MAX_LEN).to(device)\n",
        "\n",
        "model = Seq2SeqTransformer(enc, dec, PAD_IDX, PAD_IDX, device).to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFj1JYDcqLu8",
        "outputId": "ba5b5029-b32a-4fd3-999d-7f7d98c37b38"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Initializing model components...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(DEVICE)\n",
        "DEVICE = torch.device('cpu')\n",
        "if not os.path.exists(MODEL_PATH):\n",
        "    print(f\"Error: Model file not found at {MODEL_PATH}\")\n",
        "    raise FileNotFoundError(f\"Model not found: {MODEL_PATH}\")\n",
        "else:\n",
        "    try:\n",
        "        print(f\"Loading entire model object from {MODEL_PATH}...\")\n",
        "        model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "        print(\"Model loaded successfully.\")\n",
        "        model.to(DEVICE)\n",
        "        model.eval()\n",
        "    except FileNotFoundError:\n",
        "        # This case is handled above, but kept for robustness\n",
        "        print(f\"Error: Model file not found at {MODEL_PATH}\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        print(\"Check model file integrity and Python environment compatibility.\")\n",
        "        raise e"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcTV_c1BqJzn",
        "outputId": "fb425f62-0cf4-4251-b8d2-89cd17cb3afb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "Loading entire model object from empathetic-transformer-basic-best (2).pt...\n",
            "Model loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_prompts = [\n",
        "    \"I just got a promotion at work! I'm so happy.\",\n",
        "    \"I feel really lonely these days. No one seems to understand me.\",\n",
        "    \"My best friend just moved to another country, and I miss them so much.\",\n",
        "    \"I failed my exam even after studying so hard. I feel so disappointed.\",\n",
        "    \"I helped a stranger today, and it made me feel really good.\",\n",
        "    \"I'm really nervous about my job interview tomorrow.\",\n",
        "    \"I just finished a marathon! I feel so accomplished.\",\n",
        "    \"My pet passed away last night, and I’m heartbroken.\",\n",
        "    \"I got stuck in traffic for hours today. It was so frustrating!\",\n",
        "    \"I found out I’m going to be a parent! I’m overjoyed but also a little scared.\",\n",
        "    \"I’ve been feeling really unmotivated lately. I don’t know what to do.\",\n",
        "    \"I had a great conversation with an old friend today. It felt amazing!\",\n",
        "    \"I lost my wallet today. Now I have to replace everything.\",\n",
        "    \"I just tried a new hobby, and I think I love it!\",\n",
        "    \"Someone criticized my work today, and it made me feel insecure.\",\n",
        "    \"I’m struggling with my mental health, and I don’t know how to talk about it.\",\n",
        "    \"My birthday is coming up, but I don’t feel excited this year.\",\n",
        "    \"I finally confronted someone who hurt me in the past. It was really hard.\",\n",
        "    \"I got my dream job! I can’t believe it’s happening.\",\n",
        "    \"I feel so stuck in life. I don’t know what my next step should be.\"\n",
        "]\n",
        "\n",
        "for prompt in test_prompts:\n",
        "            response = generate_response_sampling(\n",
        "                prompt, model, tokenizer, device, max_len=MAX_LEN,\n",
        "                strategy=\"topp\", p=0.9, temperature=0.8\n",
        "            )\n",
        "            print(f\"User: {prompt}\")\n",
        "            print(f\"Bot:  {response}\")\n",
        "            print(\"-\" * 20)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5jXXAVXRrSA",
        "outputId": "cc4f0b9b-30a0-410b-adc6-01d68c6ab3bf"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: I just got a promotion at work! I'm so happy.\n",
            "Bot:  that is a lot of fun !\n",
            "--------------------\n",
            "User: I feel really lonely these days. No one seems to understand me.\n",
            "Bot:  Yeah - I guess if I would go a good one and he can be used to get .\n",
            "--------------------\n",
            "User: My best friend just moved to another country, and I miss them so much.\n",
            "Bot:  Oh no ! Did you talk to the wedding ?\n",
            "--------------------\n",
            "User: I failed my exam even after studying so hard. I feel so disappointed.\n",
            "Bot:  That sounds really so sad . Do you do at least ?\n",
            "--------------------\n",
            "User: I helped a stranger today, and it made me feel really good.\n",
            "Bot:  That must have been a school ##_comma_ and I bet you can get ?\n",
            "--------------------\n",
            "User: I'm really nervous about my job interview tomorrow.\n",
            "Bot:  Why did you get ?\n",
            "--------------------\n",
            "User: I just finished a marathon! I feel so accomplished.\n",
            "Bot:  How do you think ?\n",
            "--------------------\n",
            "User: My pet passed away last night, and I’m heartbroken.\n",
            "Bot:  I know . How did you do it ?\n",
            "--------------------\n",
            "User: I got stuck in traffic for hours today. It was so frustrating!\n",
            "Bot:  thats great_comma_ I bet you had a good place\n",
            "--------------------\n",
            "User: I found out I’m going to be a parent! I’m overjoyed but also a little scared.\n",
            "Bot:  Oh no ! That must be so excited .\n",
            "--------------------\n",
            "User: I’ve been feeling really unmotivated lately. I don’t know what to do.\n",
            "Bot:  I bet you must be there ##_comma_ ?\n",
            "--------------------\n",
            "User: I had a great conversation with an old friend today. It felt amazing!\n",
            "Bot:  Oh no_comma_ what was it for ?\n",
            "--------------------\n",
            "User: I lost my wallet today. Now I have to replace everything.\n",
            "Bot:  I hope she was a bad animal place . What happened to them at least do\n",
            "--------------------\n",
            "User: I just tried a new hobby, and I think I love it!\n",
            "Bot:  Yes ! That sounds like you ' ve been a big car !\n",
            "--------------------\n",
            "User: Someone criticized my work today, and it made me feel insecure.\n",
            "Bot:  oh no_comma_ I ' ve had your mind .\n",
            "--------------------\n",
            "User: I’m struggling with my mental health, and I don’t know how to talk about it.\n",
            "Bot:  Oh my god that sounds like you can get it .\n",
            "--------------------\n",
            "User: My birthday is coming up, but I don’t feel excited this year.\n",
            "Bot:  Oh no ! That must have been very nerve - I ' m sure it ' s a new house to be a good man .\n",
            "--------------------\n",
            "User: I finally confronted someone who hurt me in the past. It was really hard.\n",
            "Bot:  Was your particular of least ?\n",
            "--------------------\n",
            "User: I got my dream job! I can’t believe it’s happening.\n",
            "Bot:  Oh no ! What kind of child ?\n",
            "--------------------\n",
            "User: I feel so stuck in life. I don’t know what my next step should be.\n",
            "Bot:  Nice ! I ' m sorry to hear that .\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_ruY1Ock1PW"
      },
      "source": [
        "## 5. Interactive Inference Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEA_fGEIk1PW",
        "outputId": "d93be31d-a001-4623-8918-46502737f851"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Empathetic Chatbot Ready ---\n",
            "(Device: cpu, Strategy: topk, Temp: 0.75, K: 15, P: 0.92)\n",
            "Enter your message (or type 'quit' to exit):\n",
            "You: I feel like dying\n",
            "Bot: I agree . That is a tough way to have someone .\n",
            "(Generated in 0.18 seconds)\n",
            "--------------------\n",
            "You: quit\n",
            "\n",
            "Inference finished.\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- Empathetic Chatbot Ready ---\")\n",
        "print(f\"(Device: {DEVICE}, Strategy: {DEFAULT_STRATEGY}, Temp: {DEFAULT_TEMPERATURE}, K: {DEFAULT_K}, P: {DEFAULT_P})\")\n",
        "print(\"Enter your message (or type 'quit' to exit):\")\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        input_sentence = input(\"You: \")\n",
        "        if input_sentence.lower().strip() == 'quit':\n",
        "            break\n",
        "        if not input_sentence.strip():\n",
        "            continue\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        response = generate_response_sampling(\n",
        "                input_sentence, model, tokenizer, device, max_len=MAX_LEN,\n",
        "                strategy=\"topk\", k=10, temperature=0.8\n",
        "            )\n",
        "\n",
        "        end_time = time.time()\n",
        "        print(f\"Bot: {response}\")\n",
        "        print(f\"(Generated in {end_time - start_time:.2f} seconds)\")\n",
        "        print(\"-\" * 20)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nExiting...\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn error occurred during generation: {e}\")\n",
        "        # Decide if you want to break the loop on error or continue\n",
        "        # break\n",
        "\n",
        "print(\"\\nInference finished.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yaGfU4v9rKTW"
      },
      "execution_count": 22,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.x.y"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}