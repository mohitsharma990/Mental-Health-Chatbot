# ğŸ¤— Empathetic Response Generation with Transformer-Based Seq2Seq

> A Transformer-powered dialogue system that generates **emotionally intelligent responses** for mental health-related conversations. Built using PyTorch and the EmpatheticDialogues dataset.

---

## ğŸ§  Motivation

With the rising importance of mental health and the role AI can play in providing support, our goal was to build a conversational model that understands emotional context and responds empathetically using a state-of-the-art Transformer-based Sequence-to-Sequence (Seq2Seq) architecture.

---

## ğŸ“Œ Objectives

- âœ… Implement a Transformer-based Seq2Seq model from scratch.
- âœ… Train it on emotionally grounded conversations.
- âœ… Generate empathetic, context-aware responses.
- âœ… Evaluate different embedding strategies: Trainable, GloVe, and FastText.

---

## ğŸ“š Dataset

We used the [EmpatheticDialogues dataset](https://huggingface.co/datasets/empathetic_dialogues) released by Facebook AI, consisting of:

- ğŸ’¬ 24,000+ emotion-labeled conversations
- ğŸ­ 32 emotion categories
- ğŸ“‚ Split into train (19,533), validation (2,770), test (2,547)

---

## âš™ï¸ Model Architecture

Built from scratch using PyTorch:

- âœ¨ **Transformer Encoder-Decoder** design
- ğŸ§© **Multi-head attention**, **Positional encoding**
- ğŸ”¢ Embedding choices:
  - ğŸ”¹ Trainable (Best BLEU: 0.84)
  - ğŸ”¹ GloVe (BLEU: 0.18)
  - ğŸ”¹ FastText (BLEU: 0.82)

![Transformer](https://upload.wikimedia.org/wikipedia/commons/3/3f/Transformer.png)

> *Architecture details: Encoder + Decoder stacks with attention masks, embedding layers, and autoregressive decoding using greedy, top-k, and nucleus sampling.*

---

## ğŸ› ï¸ Preprocessing & Training

- ğŸ§¹ WordPiece Tokenizer with `<PAD>`, `<SOS>`, `<EOS>`, `<UNK>`
- âœ‚ Max length: 60 tokens
- ğŸ“¦ Batch size: 32
- ğŸ” Early stopping with patience: 5
- ğŸ¯ Optimizer: Adam
- â±ï¸ Max Epochs: 50
- ğŸ”’ Reproducibility ensured by seeding `random`, `torch`, and `PYTHONHASHSEED`

---

## ğŸ” Evaluation

| Embedding Type       | BLEU Score | Convergence Epoch |
|----------------------|------------|-------------------|
| Trainable Embeddings | **0.84**   | 50                |
| GloVe                | 0.18       | 40                |
| FastText             | 0.82       | **30**            |

- ğŸ“‰ Loss and validation tracked per epoch
- ğŸ“Š BLEU scores calculated on test set
- ğŸ” Sampling: Greedy, Top-k, Top-p

---

## ğŸš€ Inference

```python
generate_response("Iâ€™m feeling lonely today", strategy="topk", k=10, temperature=0.8)
