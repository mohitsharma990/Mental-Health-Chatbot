{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "553af30c",
   "metadata": {
    "papermill": {
     "duration": 0.020409,
     "end_time": "2025-04-22T13:19:42.837520",
     "exception": false,
     "start_time": "2025-04-22T13:19:42.817111",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Improved Code with different cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf23f0c",
   "metadata": {
    "papermill": {
     "duration": 0.017869,
     "end_time": "2025-04-22T13:19:42.873088",
     "exception": false,
     "start_time": "2025-04-22T13:19:42.855219",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Basic Mental Health Chatbot with Transformer (From Scratch) - EmpatheticDialogues Dataset\n",
    "\n",
    "**Improvements:**\n",
    "*   Uses Hugging Face `tokenizers` (WordPiece) for better tokenization.\n",
    "*   Trains on the full dataset by default.\n",
    "*   Includes a validation loop and early stopping based on validation loss.\n",
    "*   Saves the best model based on validation performance.\n",
    "*   Increased default epochs (relies on early stopping).\n",
    "*   Sampling strategies (Top-k, Top-p) for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08093144",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T13:19:42.910789Z",
     "iopub.status.busy": "2025-04-22T13:19:42.910494Z",
     "iopub.status.idle": "2025-04-22T13:19:55.553989Z",
     "shell.execute_reply": "2025-04-22T13:19:55.553044Z"
    },
    "papermill": {
     "duration": 12.663208,
     "end_time": "2025-04-22T13:19:55.555283",
     "exception": false,
     "start_time": "2025-04-22T13:19:42.892075",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\r\n",
      "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.8.4.1 which is incompatible.\r\n",
      "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\r\n",
      "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.3.3.83 which is incompatible.\r\n",
      "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.9.90 which is incompatible.\r\n",
      "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.3.90 which is incompatible.\r\n",
      "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.8.93 which is incompatible.\r\n",
      "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.8.93 which is incompatible.\r\n",
      "bigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mPyTorch Version: 2.5.1+cu124\n",
      "Required libraries loaded.\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets tokenizers scikit-learn pandas -q\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau \n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import json \n",
    "\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing \n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "try:\n",
    "    import datasets\n",
    "    import tokenizers\n",
    "    import sklearn\n",
    "    import pandas\n",
    "    print(\"Required libraries loaded.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing libraries: {e}. Please ensure installs were successful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a487a941",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T13:19:55.592764Z",
     "iopub.status.busy": "2025-04-22T13:19:55.592413Z",
     "iopub.status.idle": "2025-04-22T13:19:55.688010Z",
     "shell.execute_reply": "2025-04-22T13:19:55.687141Z"
    },
    "papermill": {
     "duration": 0.114969,
     "end_time": "2025-04-22T13:19:55.689231",
     "exception": false,
     "start_time": "2025-04-22T13:19:55.574262",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DATASET_SUBSET_SIZE = None \n",
    "MAX_LEN = 60            \n",
    "VOCAB_SIZE = 20000      \n",
    "TOKENIZER_FILE = \"wp_tokenizer.json\" \n",
    "\n",
    "BATCH_SIZE = 32          \n",
    "LEARNING_RATE = 0.0005\n",
    "N_EPOCHS = 50        \n",
    "CLIP = 1.0\n",
    "PATIENCE = 10             \n",
    "\n",
    "D_MODEL = 256\n",
    "N_HEADS = 8\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "PF_DIM = 512\n",
    "DROPOUT = 0.3\n",
    "\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED) \n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "SOS_TOKEN = \"<SOS>\"\n",
    "EOS_TOKEN = \"<EOS>\"\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "SPECIAL_TOKENS = [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN, UNK_TOKEN]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369c44aa",
   "metadata": {
    "papermill": {
     "duration": 0.017737,
     "end_time": "2025-04-22T13:19:55.724840",
     "exception": false,
     "start_time": "2025-04-22T13:19:55.707103",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Load EmpatheticDialogues Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f7616a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T13:19:55.761455Z",
     "iopub.status.busy": "2025-04-22T13:19:55.761194Z",
     "iopub.status.idle": "2025-04-22T13:20:03.225867Z",
     "shell.execute_reply": "2025-04-22T13:20:03.225090Z"
    },
    "papermill": {
     "duration": 7.48447,
     "end_time": "2025-04-22T13:20:03.227055",
     "exception": false,
     "start_time": "2025-04-22T13:19:55.742585",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading EmpatheticDialogues dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52aa861f6d4c4c3ab1c657195d0e19e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "214faa08f5664589b4c580261cc9d6f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "empathetic_dialogues.py:   0%|          | 0.00/4.51k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e19c4d392404322b91dd1d28846e237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/28.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27b7afc6e03e40519980fb6e1cc5e7f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/76673 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3e39131249642a38e703c5e5abb4bd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/12030 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3edb16484f454ec4836c9a1fd63ec86d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/10943 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['conv_id', 'utterance_idx', 'context', 'prompt', 'speaker_idx', 'utterance', 'selfeval', 'tags'],\n",
      "        num_rows: 76673\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['conv_id', 'utterance_idx', 'context', 'prompt', 'speaker_idx', 'utterance', 'selfeval', 'tags'],\n",
      "        num_rows: 12030\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['conv_id', 'utterance_idx', 'context', 'prompt', 'speaker_idx', 'utterance', 'selfeval', 'tags'],\n",
      "        num_rows: 10943\n",
      "    })\n",
      "})\n",
      "Dataset loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading EmpatheticDialogues dataset...\")\n",
    "try:\n",
    "    empathetic_dialogues = load_dataset(\"empathetic_dialogues\", trust_remote_code=True)\n",
    "    print(empathetic_dialogues)\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a849135",
   "metadata": {
    "papermill": {
     "duration": 0.018337,
     "end_time": "2025-04-22T13:20:03.264870",
     "exception": false,
     "start_time": "2025-04-22T13:20:03.246533",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Extract Conversation Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11ea8213",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T13:20:03.347646Z",
     "iopub.status.busy": "2025-04-22T13:20:03.347369Z",
     "iopub.status.idle": "2025-04-22T13:20:03.354966Z",
     "shell.execute_reply": "2025-04-22T13:20:03.354350Z"
    },
    "papermill": {
     "duration": 0.02836,
     "end_time": "2025-04-22T13:20:03.356069",
     "exception": false,
     "start_time": "2025-04-22T13:20:03.327709",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_pairs_corrected(split_name, dataset, subset_size=None):\n",
    "    \"\"\"Extracts adjacent utterance pairs from a given split of the dataset.\"\"\"\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    data_split = dataset[split_name]\n",
    "    print(f\"Processing {split_name} split...\")\n",
    "    try:\n",
    "        df = data_split.to_pandas()\n",
    "        print(f\"Converted {split_name} split to Pandas DataFrame with {len(df)} rows.\")\n",
    "\n",
    "        all_conv_ids = df['conv_id'].unique()\n",
    "        if subset_size and subset_size < len(all_conv_ids):\n",
    "            print(f\"Selecting a subset of {subset_size} conversation IDs for {split_name} split.\")\n",
    "            selected_conv_ids = random.sample(list(all_conv_ids), subset_size)\n",
    "            df = df[df['conv_id'].isin(selected_conv_ids)]\n",
    "            print(f\"Subset DataFrame has {len(df)} rows after filtering by conv_id.\")\n",
    "\n",
    "        grouped = df.sort_values('utterance_idx').groupby('conv_id')\n",
    "        print(f\"Grouped into {len(grouped)} conversations.\")\n",
    "\n",
    "        processed_convs = 0\n",
    "        skipped_short_convs = 0\n",
    "        for conv_id, group in grouped:\n",
    "            utterances = group['utterance'].tolist()\n",
    "            utterances = [u.strip() for u in utterances if u.strip()] \n",
    "\n",
    "            if len(utterances) >= 2:\n",
    "                for i in range(len(utterances) - 1):\n",
    "                    inputs.append(utterances[i])\n",
    "                    targets.append(utterances[i+1])\n",
    "            else:\n",
    "                skipped_short_convs += 1\n",
    "\n",
    "            processed_convs += 1\n",
    "            if processed_convs % 10000 == 0:\n",
    "                 print(f\"  Processed {processed_convs}/{len(grouped)} conversations...\")\n",
    "\n",
    "        print(f\"Finished processing {split_name}.\")\n",
    "        print(f\"  Found {len(inputs)} input/target pairs.\")\n",
    "        if skipped_short_convs > 0:\n",
    "            print(f\"  Skipped {skipped_short_convs} conversations with less than 2 utterances.\")\n",
    "        return inputs, targets\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"Error: Pandas is required for data extraction. Please install it.\")\n",
    "        raise\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Missing expected column in dataset: {e}. Check dataset structure.\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edcf0fc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T13:20:03.395407Z",
     "iopub.status.busy": "2025-04-22T13:20:03.395190Z",
     "iopub.status.idle": "2025-04-22T13:20:04.588183Z",
     "shell.execute_reply": "2025-04-22T13:20:04.587288Z"
    },
    "papermill": {
     "duration": 1.213731,
     "end_time": "2025-04-22T13:20:04.589431",
     "exception": false,
     "start_time": "2025-04-22T13:20:03.375700",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Training pairs...\n",
      "Processing train split...\n",
      "Converted train split to Pandas DataFrame with 76673 rows.\n",
      "Grouped into 17844 conversations.\n",
      "  Processed 10000/17844 conversations...\n",
      "Finished processing train.\n",
      "  Found 58829 input/target pairs.\n",
      "  Skipped 64 conversations with less than 2 utterances.\n",
      "\n",
      "Extracting Validation pairs...\n",
      "Processing validation split...\n",
      "Converted validation split to Pandas DataFrame with 12030 rows.\n",
      "Grouped into 2763 conversations.\n",
      "Finished processing validation.\n",
      "  Found 9267 input/target pairs.\n",
      "  Skipped 5 conversations with less than 2 utterances.\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting Training pairs...\")\n",
    "train_inputs, train_targets = extract_pairs_corrected('train', empathetic_dialogues, subset_size=DATASET_SUBSET_SIZE)\n",
    "\n",
    "print(\"\\nExtracting Validation pairs...\")\n",
    "val_inputs, val_targets = extract_pairs_corrected('validation', empathetic_dialogues, subset_size=None) # Use full validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da33686",
   "metadata": {
    "papermill": {
     "duration": 0.018392,
     "end_time": "2025-04-22T13:20:04.627283",
     "exception": false,
     "start_time": "2025-04-22T13:20:04.608891",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Tokenizer Training / Loading\n",
    "Using Hugging Face `tokenizers` library with WordPiece."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0affa91c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T13:20:04.665908Z",
     "iopub.status.busy": "2025-04-22T13:20:04.665652Z",
     "iopub.status.idle": "2025-04-22T13:20:04.669619Z",
     "shell.execute_reply": "2025-04-22T13:20:04.669089Z"
    },
    "papermill": {
     "duration": 0.024572,
     "end_time": "2025-04-22T13:20:04.670557",
     "exception": false,
     "start_time": "2025-04-22T13:20:04.645985",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_training_corpus(inputs_list, targets_list):\n",
    "    \"\"\"Generator function to yield batches of text data for tokenizer training\"\"\"\n",
    "    batch_size = 1000\n",
    "    for i in range(0, len(inputs_list), batch_size):\n",
    "        yield inputs_list[i : i + batch_size] + targets_list[i : i + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f6d6b8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T13:20:04.709943Z",
     "iopub.status.busy": "2025-04-22T13:20:04.709701Z",
     "iopub.status.idle": "2025-04-22T13:20:06.165296Z",
     "shell.execute_reply": "2025-04-22T13:20:06.164497Z"
    },
    "papermill": {
     "duration": 1.476379,
     "end_time": "2025-04-22T13:20:06.166488",
     "exception": false,
     "start_time": "2025-04-22T13:20:04.690109",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training WordPiece tokenizer (target vocab_size=20000)...\n",
      "\n",
      "\n",
      "\n",
      "Tokenizer trained and saved to wp_tokenizer.json\n",
      "Tokenizer Vocabulary Size: 20000\n",
      "Special Token IDs: PAD=0, SOS=1, EOS=2, UNK=3\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(TOKENIZER_FILE):\n",
    "    print(f\"Training WordPiece tokenizer (target vocab_size={VOCAB_SIZE})...\")\n",
    "    tokenizer = Tokenizer(WordPiece(unk_token=UNK_TOKEN))\n",
    "\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    trainer = WordPieceTrainer(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        special_tokens=SPECIAL_TOKENS \n",
    "    )\n",
    "\n",
    "    tokenizer.train_from_iterator(\n",
    "        get_training_corpus(train_inputs, train_targets),\n",
    "        trainer=trainer,\n",
    "        length=len(train_inputs) + len(train_targets) \n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    tokenizer.save(TOKENIZER_FILE)\n",
    "    print(f\"Tokenizer trained and saved to {TOKENIZER_FILE}\")\n",
    "else:\n",
    "    print(f\"Loading tokenizer from {TOKENIZER_FILE}...\")\n",
    "    tokenizer = Tokenizer.from_file(TOKENIZER_FILE)\n",
    "    print(\"Tokenizer loaded.\")\n",
    "\n",
    "INPUT_DIM = tokenizer.get_vocab_size()\n",
    "OUTPUT_DIM = INPUT_DIM\n",
    "print(f\"Tokenizer Vocabulary Size: {INPUT_DIM}\")\n",
    "\n",
    "PAD_IDX = tokenizer.token_to_id(PAD_TOKEN)\n",
    "SOS_IDX = tokenizer.token_to_id(SOS_TOKEN)\n",
    "EOS_IDX = tokenizer.token_to_id(EOS_TOKEN)\n",
    "UNK_IDX = tokenizer.token_to_id(UNK_TOKEN)\n",
    "\n",
    "if None in [PAD_IDX, SOS_IDX, EOS_IDX, UNK_IDX]:\n",
    "     print(\"Error: One or more special tokens not found in tokenizer vocab!\")\n",
    "else:\n",
    "    print(f\"Special Token IDs: PAD={PAD_IDX}, SOS={SOS_IDX}, EOS={EOS_IDX}, UNK={UNK_IDX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417d8a0b",
   "metadata": {
    "papermill": {
     "duration": 0.020071,
     "end_time": "2025-04-22T13:20:06.207493",
     "exception": false,
     "start_time": "2025-04-22T13:20:06.187422",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. Numericalize, Pad, and Create Datasets/DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e563155a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T13:20:06.252422Z",
     "iopub.status.busy": "2025-04-22T13:20:06.252122Z",
     "iopub.status.idle": "2025-04-22T13:20:06.257359Z",
     "shell.execute_reply": "2025-04-22T13:20:06.256736Z"
    },
    "papermill": {
     "duration": 0.029123,
     "end_time": "2025-04-22T13:20:06.258527",
     "exception": false,
     "start_time": "2025-04-22T13:20:06.229404",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def text_to_sequence_tokenizer(text, tokenizer):\n",
    "    \"\"\"Encodes text to a list of token IDs, adding SOS/EOS.\"\"\"\n",
    "    encoded = tokenizer.encode(text)\n",
    "    return [SOS_IDX] + encoded.ids + [EOS_IDX]\n",
    "\n",
    "def sequence_to_text_tokenizer(sequence, tokenizer):\n",
    "    \"\"\"Decodes a list of token IDs back to text, removing special tokens.\"\"\"\n",
    "    ids = [idx for idx in sequence if idx not in [PAD_IDX, SOS_IDX, EOS_IDX]]\n",
    "    return tokenizer.decode(ids)\n",
    "\n",
    "def pad_sequence(seq, max_len, pad_idx):\n",
    "    \"\"\"Pads or truncates a sequence to max_len.\"\"\"\n",
    "    seq = seq[:max_len]\n",
    "    padded = seq + [pad_idx] * (max_len - len(seq))\n",
    "    return padded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5a35411",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T13:20:06.300986Z",
     "iopub.status.busy": "2025-04-22T13:20:06.300309Z",
     "iopub.status.idle": "2025-04-22T13:20:06.305471Z",
     "shell.execute_reply": "2025-04-22T13:20:06.304976Z"
    },
    "papermill": {
     "duration": 0.026527,
     "end_time": "2025-04-22T13:20:06.306468",
     "exception": false,
     "start_time": "2025-04-22T13:20:06.279941",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_data(inputs, targets, tokenizer, max_len, pad_idx):\n",
    "    \"\"\"Tokenizes and pads input and target lists.\"\"\"\n",
    "    print(f\"Tokenizing and padding sequences (MAX_LEN={max_len})...\")\n",
    "    processed_count = 0\n",
    "    total_pairs = len(inputs)\n",
    "    tokenized_inputs = []\n",
    "    tokenized_targets = []\n",
    "    for i in range(total_pairs):\n",
    "        tokenized_inputs.append(text_to_sequence_tokenizer(inputs[i], tokenizer))\n",
    "        tokenized_targets.append(text_to_sequence_tokenizer(targets[i], tokenizer))\n",
    "        processed_count += 1\n",
    "        if processed_count % 10000 == 0:\n",
    "            print(f\"  Tokenized {processed_count}/{total_pairs} pairs...\")\n",
    "\n",
    "    print(\"Padding sequences...\")\n",
    "    padded_inputs = [pad_sequence(seq, max_len, pad_idx) for seq in tokenized_inputs]\n",
    "    padded_targets = [pad_sequence(seq, max_len, pad_idx) for seq in tokenized_targets]\n",
    "    print(\"Padding complete.\")\n",
    "    return padded_inputs, padded_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f3ddb66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T13:20:06.345747Z",
     "iopub.status.busy": "2025-04-22T13:20:06.345226Z",
     "iopub.status.idle": "2025-04-22T13:20:10.079615Z",
     "shell.execute_reply": "2025-04-22T13:20:10.078813Z"
    },
    "papermill": {
     "duration": 3.755835,
     "end_time": "2025-04-22T13:20:10.081004",
     "exception": false,
     "start_time": "2025-04-22T13:20:06.325169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing and padding sequences (MAX_LEN=60)...\n",
      "  Tokenized 10000/58829 pairs...\n",
      "  Tokenized 20000/58829 pairs...\n",
      "  Tokenized 30000/58829 pairs...\n",
      "  Tokenized 40000/58829 pairs...\n",
      "  Tokenized 50000/58829 pairs...\n",
      "Padding sequences...\n",
      "Padding complete.\n",
      "Tokenizing and padding sequences (MAX_LEN=60)...\n",
      "Padding sequences...\n",
      "Padding complete.\n"
     ]
    }
   ],
   "source": [
    "padded_train_inputs, padded_train_targets = process_data(train_inputs, train_targets, tokenizer, MAX_LEN, PAD_IDX)\n",
    "padded_val_inputs, padded_val_targets = process_data(val_inputs, val_targets, tokenizer, MAX_LEN, PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d10c8e23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T13:20:10.121456Z",
     "iopub.status.busy": "2025-04-22T13:20:10.121210Z",
     "iopub.status.idle": "2025-04-22T13:20:10.125838Z",
     "shell.execute_reply": "2025-04-22T13:20:10.125293Z"
    },
    "papermill": {
     "duration": 0.02613,
     "end_time": "2025-04-22T13:20:10.126983",
     "exception": false,
     "start_time": "2025-04-22T13:20:10.100853",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DialogueDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for dialogue pairs.\"\"\"\n",
    "    def __init__(self, inputs, targets):\n",
    "        assert len(inputs) > 0, \"Input list to DialogueDataset is empty!\"\n",
    "        assert len(inputs) == len(targets), \"Inputs and targets have different lengths!\"\n",
    "        self.inputs = torch.tensor(inputs, dtype=torch.long)\n",
    "        self.targets = torch.tensor(targets, dtype=torch.long)\n",
    "        print(f\"DialogueDataset created with {len(self.inputs)} samples.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples.\"\"\"\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Fetches the sample at the given index.\"\"\"\n",
    "        return self.inputs[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29e6e308",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T13:20:10.166148Z",
     "iopub.status.busy": "2025-04-22T13:20:10.165892Z",
     "iopub.status.idle": "2025-04-22T13:20:10.989041Z",
     "shell.execute_reply": "2025-04-22T13:20:10.987883Z"
    },
    "papermill": {
     "duration": 0.844162,
     "end_time": "2025-04-22T13:20:10.990304",
     "exception": false,
     "start_time": "2025-04-22T13:20:10.146142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialogueDataset created with 58829 samples.\n",
      "DialogueDataset created with 9267 samples.\n",
      "Created Train DataLoader with 1839 batches.\n",
      "Created Validation DataLoader with 290 batches.\n",
      "Cleaned up intermediate data lists.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    train_dataset = DialogueDataset(padded_train_inputs, padded_train_targets)\n",
    "    val_dataset = DialogueDataset(padded_val_inputs, padded_val_targets)\n",
    "except AssertionError as e:\n",
    "    print(f\"Error creating dataset: {e}\")\n",
    "    raise e\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True if device=='cuda' else False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True if device=='cuda' else False)\n",
    "\n",
    "print(f\"Created Train DataLoader with {len(train_loader)} batches.\")\n",
    "print(f\"Created Validation DataLoader with {len(val_loader)} batches.\")\n",
    "\n",
    "try:\n",
    "    del padded_train_inputs, padded_train_targets, padded_val_inputs, padded_val_targets\n",
    "    del train_inputs, train_targets, val_inputs, val_targets\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    print(\"Cleaned up intermediate data lists.\")\n",
    "except NameError:\n",
    "    print(\"Intermediate lists might have already been cleaned or not generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb80ac0",
   "metadata": {
    "papermill": {
     "duration": 0.020498,
     "end_time": "2025-04-22T13:20:11.032363",
     "exception": false,
     "start_time": "2025-04-22T13:20:11.011865",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. Transformer Model Implementation (From Scratch)\n",
    "Core components of the Transformer architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccaf5bf3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T13:20:11.073261Z",
     "iopub.status.busy": "2025-04-22T13:20:11.073021Z",
     "iopub.status.idle": "2025-04-22T13:20:11.079077Z",
     "shell.execute_reply": "2025-04-22T13:20:11.078237Z"
    },
    "papermill": {
     "duration": 0.027671,
     "end_time": "2025-04-22T13:20:11.080317",
     "exception": false,
     "start_time": "2025-04-22T13:20:11.052646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Injects positional information into the input embeddings.\"\"\"\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1) \n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9126ab31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T13:20:11.119957Z",
     "iopub.status.busy": "2025-04-22T13:20:11.119731Z",
     "iopub.status.idle": "2025-04-22T13:20:11.127856Z",
     "shell.execute_reply": "2025-04-22T13:20:11.127277Z"
    },
    "papermill": {
     "duration": 0.029298,
     "end_time": "2025-04-22T13:20:11.129121",
     "exception": false,
     "start_time": "2025-04-22T13:20:11.099823",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-Head Attention mechanism.\"\"\"\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "\n",
    "        self.fc_q = nn.Linear(d_model, d_model)\n",
    "        self.fc_k = nn.Linear(d_model, d_model)\n",
    "        self.fc_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.fc_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.register_buffer(\"scale\", torch.sqrt(torch.FloatTensor([self.head_dim])))\n",
    "\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query: Tensor, shape [query_len, batch_size, d_model]\n",
    "            key: Tensor, shape [key_len, batch_size, d_model]\n",
    "            value: Tensor, shape [value_len, batch_size, d_model] (value_len == key_len)\n",
    "            mask: Tensor, shape [batch_size, 1, query_len, key_len] or broadcastable.\n",
    "                  Masks positions where attention should be zero (e.g., padding).\n",
    "        \"\"\"\n",
    "        batch_size = query.shape[1]\n",
    "\n",
    "        Q = self.fc_q(query) \n",
    "        K = self.fc_k(key)   \n",
    "        V = self.fc_v(value)  \n",
    "\n",
    "        Q = Q.view(query.shape[0], batch_size, self.n_heads, self.head_dim).permute(1, 2, 0, 3)\n",
    "        K = K.view(key.shape[0], batch_size, self.n_heads, self.head_dim).permute(1, 2, 0, 3)\n",
    "        V = V.view(value.shape[0], batch_size, self.n_heads, self.head_dim).permute(1, 2, 0, 3)\n",
    "\n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale.to(Q.device)\n",
    "\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10) \n",
    "\n",
    "        attention = torch.softmax(energy, dim=-1) \n",
    "\n",
    "        attention = self.dropout(attention)\n",
    "\n",
    "        x = torch.matmul(attention, V)\n",
    "\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        x = x.view(batch_size, query.shape[0], self.d_model)\n",
    "\n",
    "        x = x.permute(1, 0, 2)\n",
    "\n",
    "        x = self.fc_o(x) \n",
    "\n",
    "        return x, attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba3dc54d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T13:20:11.169801Z",
     "iopub.status.busy": "2025-04-22T13:20:11.169588Z",
     "iopub.status.idle": "2025-04-22T13:20:11.173685Z",
     "shell.execute_reply": "2025-04-22T13:20:11.173183Z"
    },
    "papermill": {
     "duration": 0.026198,
     "end_time": "2025-04-22T13:20:11.174718",
     "exception": false,
     "start_time": "2025-04-22T13:20:11.148520",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedforward(nn.Module):\n",
    "    \"\"\"Position-wise Feedforward Network.\"\"\"\n",
    "    def __init__(self, d_model, pf_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, pf_dim)\n",
    "        self.fc2 = nn.Linear(pf_dim, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, d_model]\n",
    "        \"\"\"\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ec0cf38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T13:20:11.213939Z",
     "iopub.status.busy": "2025-04-22T13:20:11.213627Z",
     "iopub.status.idle": "2025-04-22T13:20:11.218580Z",
     "shell.execute_reply": "2025-04-22T13:20:11.217793Z"
    },
    "papermill": {
     "duration": 0.025594,
     "end_time": "2025-04-22T13:20:11.219742",
     "exception": false,
     "start_time": "2025-04-22T13:20:11.194148",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"A single layer of the Transformer Encoder.\"\"\"\n",
    "    def __init__(self, d_model, n_heads, pf_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.ff = PositionwiseFeedforward(d_model, pf_dim, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [src_len, batch_size, d_model]\n",
    "            src_mask: Tensor, mask for self-attention (hides padding)\n",
    "        \"\"\"\n",
    "        _src, _ = self.self_attn(src, src, src, src_mask)\n",
    "        src = self.norm1(src + self.dropout(_src))\n",
    "\n",
    "        _src = self.ff(src)\n",
    "        src = self.norm2(src + self.dropout(_src))\n",
    "\n",
    "        return src "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e432ba2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T13:20:11.258025Z",
     "iopub.status.busy": "2025-04-22T13:20:11.257780Z",
     "iopub.status.idle": "2025-04-22T13:20:11.263380Z",
     "shell.execute_reply": "2025-04-22T13:20:11.262873Z"
    },
    "papermill": {
     "duration": 0.025918,
     "end_time": "2025-04-22T13:20:11.264373",
     "exception": false,
     "start_time": "2025-04-22T13:20:11.238455",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"A single layer of the Transformer Decoder.\"\"\"\n",
    "    def __init__(self, d_model, n_heads, pf_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.masked_self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.encoder_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.ff = PositionwiseFeedforward(d_model, pf_dim, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            trg: Tensor, shape [trg_len, batch_size, d_model] (target sequence embeddings)\n",
    "            enc_src: Tensor, shape [src_len, batch_size, d_model] (encoder output)\n",
    "            trg_mask: Tensor, mask for self-attention (hides padding and future tokens)\n",
    "            src_mask: Tensor, mask for encoder-attention (hides padding in source)\n",
    "        \"\"\"\n",
    "        _trg, _ = self.masked_self_attn(trg, trg, trg, trg_mask)\n",
    "        trg = self.norm1(trg + self.dropout(_trg))\n",
    "\n",
    "        _trg, attention = self.encoder_attn(trg, enc_src, enc_src, src_mask)\n",
    "        trg = self.norm2(trg + self.dropout(_trg))\n",
    "\n",
    "        _trg = self.ff(trg)\n",
    "        trg = self.norm3(trg + self.dropout(_trg))\n",
    "\n",
    "        return trg, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5fc4b6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T13:20:11.302829Z",
     "iopub.status.busy": "2025-04-22T13:20:11.302633Z",
     "iopub.status.idle": "2025-04-22T13:20:11.308459Z",
     "shell.execute_reply": "2025-04-22T13:20:11.307746Z"
    },
    "papermill": {
     "duration": 0.026081,
     "end_time": "2025-04-22T13:20:11.309519",
     "exception": false,
     "start_time": "2025-04-22T13:20:11.283438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"The Transformer Encoder stack.\"\"\"\n",
    "    def __init__(self, input_dim, d_model, n_layers, n_heads, pf_dim, dropout, max_len=MAX_LEN):\n",
    "        super().__init__()\n",
    "        self.tok_embedding = nn.Embedding(input_dim, d_model)\n",
    "        self.pos_embedding = PositionalEncoding(d_model, dropout, max_len=5000 if max_len < 5000 else max_len)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, pf_dim, dropout)\n",
    "                                     for _ in range(n_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"scale\", torch.sqrt(torch.FloatTensor([d_model])))\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [batch_size, src_len] (input token IDs)\n",
    "            src_mask: Tensor, mask for padding in the source sequence\n",
    "        \"\"\"\n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1]\n",
    "\n",
    "        scale = self.scale.to(src.device)\n",
    "        src = self.dropout((self.tok_embedding(src) * scale))\n",
    "\n",
    "        src = src.permute(1, 0, 2)\n",
    "\n",
    "        src = self.pos_embedding(src)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c28d237f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T13:20:11.348017Z",
     "iopub.status.busy": "2025-04-22T13:20:11.347797Z",
     "iopub.status.idle": "2025-04-22T13:20:11.354014Z",
     "shell.execute_reply": "2025-04-22T13:20:11.353428Z"
    },
    "papermill": {
     "duration": 0.026699,
     "end_time": "2025-04-22T13:20:11.355024",
     "exception": false,
     "start_time": "2025-04-22T13:20:11.328325",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"The Transformer Decoder stack.\"\"\"\n",
    "    def __init__(self, output_dim, d_model, n_layers, n_heads, pf_dim, dropout, max_len=MAX_LEN):\n",
    "        super().__init__()\n",
    "        self.tok_embedding = nn.Embedding(output_dim, d_model)\n",
    "        self.pos_embedding = PositionalEncoding(d_model, dropout, max_len=5000 if max_len < 5000 else max_len)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, pf_dim, dropout)\n",
    "                                     for _ in range(n_layers)])\n",
    "        self.fc_out = nn.Linear(d_model, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"scale\", torch.sqrt(torch.FloatTensor([d_model])))\n",
    "\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            trg: Tensor, shape [batch_size, trg_len] (target token IDs)\n",
    "            enc_src: Tensor, shape [src_len, batch_size, d_model] (encoder output)\n",
    "            trg_mask: Tensor, mask for target self-attention\n",
    "            src_mask: Tensor, mask for encoder-decoder attention (source padding)\n",
    "        \"\"\"\n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "\n",
    "        scale = self.scale.to(trg.device)\n",
    "        trg = self.dropout((self.tok_embedding(trg) * scale))\n",
    "\n",
    "        trg = trg.permute(1, 0, 2)\n",
    "\n",
    "        trg = self.pos_embedding(trg)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "\n",
    "\n",
    "        output = self.fc_out(trg)\n",
    "\n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8fc2172b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T13:20:11.396667Z",
     "iopub.status.busy": "2025-04-22T13:20:11.396446Z",
     "iopub.status.idle": "2025-04-22T13:20:11.403455Z",
     "shell.execute_reply": "2025-04-22T13:20:11.402761Z"
    },
    "papermill": {
     "duration": 0.029307,
     "end_time": "2025-04-22T13:20:11.404681",
     "exception": false,
     "start_time": "2025-04-22T13:20:11.375374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    \"\"\"The main Seq2Seq Transformer model.\"\"\"\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        \"\"\"Creates a mask for the source sequence to ignore padding tokens.\"\"\"\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        return src_mask.to(self.device) \n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        \"\"\"Creates a mask for the target sequence to hide padding and future tokens.\"\"\"\n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device=self.device)).bool()\n",
    "\n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        return trg_mask.to(self.device) \n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [batch_size, src_len] (source token IDs)\n",
    "            trg: Tensor, shape [batch_size, trg_len] (target token IDs)\n",
    "        \"\"\"\n",
    "        src = src.to(self.device)\n",
    "        trg = trg.to(self.device)\n",
    "\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "\n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "\n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce722a15",
   "metadata": {
    "papermill": {
     "duration": 0.020646,
     "end_time": "2025-04-22T13:20:11.448148",
     "exception": false,
     "start_time": "2025-04-22T13:20:11.427502",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6. Model Initialization\n",
    "Instantiate the Encoder, Decoder, and Seq2Seq model. Define optimizer and loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c43c8f20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T13:20:11.488774Z",
     "iopub.status.busy": "2025-04-22T13:20:11.488534Z",
     "iopub.status.idle": "2025-04-22T13:20:16.397778Z",
     "shell.execute_reply": "2025-04-22T13:20:16.396803Z"
    },
    "papermill": {
     "duration": 4.930845,
     "end_time": "2025-04-22T13:20:16.399171",
     "exception": false,
     "start_time": "2025-04-22T13:20:11.468326",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model components...\n",
      "Applying Xavier uniform initialization...\n",
      "The model has 19,333,664 trainable parameters.\n",
      "Model is on device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing model components...\")\n",
    "enc = Encoder(INPUT_DIM, D_MODEL, ENC_LAYERS, N_HEADS, PF_DIM, DROPOUT, MAX_LEN).to(device)\n",
    "dec = Decoder(OUTPUT_DIM, D_MODEL, DEC_LAYERS, N_HEADS, PF_DIM, DROPOUT, MAX_LEN).to(device)\n",
    "\n",
    "model = Seq2SeqTransformer(enc, dec, PAD_IDX, PAD_IDX, device).to(device)\n",
    "\n",
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "\n",
    "print(\"Applying Xavier uniform initialization...\")\n",
    "model.apply(initialize_weights)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=PATIENCE//2, verbose=True)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters.')\n",
    "\n",
    "print(f\"Model is on device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bf1479",
   "metadata": {
    "papermill": {
     "duration": 0.018837,
     "end_time": "2025-04-22T13:20:16.438308",
     "exception": false,
     "start_time": "2025-04-22T13:20:16.419471",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7. Training and Evaluation Functions\n",
    "Define the loops for one epoch of training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c81c6be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T13:20:16.477754Z",
     "iopub.status.busy": "2025-04-22T13:20:16.477321Z",
     "iopub.status.idle": "2025-04-22T13:20:16.482905Z",
     "shell.execute_reply": "2025-04-22T13:20:16.482384Z"
    },
    "papermill": {
     "duration": 0.026481,
     "end_time": "2025-04-22T13:20:16.483887",
     "exception": false,
     "start_time": "2025-04-22T13:20:16.457406",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \"\"\"Performs one epoch of training.\"\"\"\n",
    "    epoch_loss = 0\n",
    "    num_batches = len(iterator)\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src, trg = batch\n",
    "        src, trg = src.to(device, non_blocking=True), trg.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        trg_input = trg[:, :-1]\n",
    "        trg_output_expected = trg[:, 1:].contiguous() \n",
    "\n",
    "        output, _ = model(src, trg_input)\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output.permute(1, 0, 2).contiguous().view(-1, output_dim)\n",
    "        trg_output_expected = trg_output_expected.view(-1)\n",
    "\n",
    "        loss = criterion(output, trg_output_expected)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6dcc991",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T13:20:16.523549Z",
     "iopub.status.busy": "2025-04-22T13:20:16.523033Z",
     "iopub.status.idle": "2025-04-22T13:20:16.527930Z",
     "shell.execute_reply": "2025-04-22T13:20:16.527241Z"
    },
    "papermill": {
     "duration": 0.026113,
     "end_time": "2025-04-22T13:20:16.529165",
     "exception": false,
     "start_time": "2025-04-22T13:20:16.503052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \"\"\"Performs one epoch of evaluation.\"\"\"\n",
    "    model.eval() \n",
    "    epoch_loss = 0\n",
    "    num_batches = len(iterator)\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        for i, batch in enumerate(iterator):\n",
    "            src, trg = batch\n",
    "            src, trg = src.to(device, non_blocking=True), trg.to(device, non_blocking=True)\n",
    "\n",
    "            trg_input = trg[:, :-1]\n",
    "            trg_output_expected = trg[:, 1:].contiguous()\n",
    "\n",
    "            output, _ = model(src, trg_input) \n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output.permute(1, 0, 2).contiguous().view(-1, output_dim)\n",
    "            trg_output_expected = trg_output_expected.view(-1)\n",
    "\n",
    "            loss = criterion(output, trg_output_expected)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9825e9a6",
   "metadata": {
    "papermill": {
     "duration": 0.019097,
     "end_time": "2025-04-22T13:20:16.567350",
     "exception": false,
     "start_time": "2025-04-22T13:20:16.548253",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8. Training Loop (with Validation and Early Stopping)\n",
    "Execute the training process, saving the best model based on validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d95c01ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T13:20:16.606284Z",
     "iopub.status.busy": "2025-04-22T13:20:16.606105Z",
     "iopub.status.idle": "2025-04-22T13:53:32.240425Z",
     "shell.execute_reply": "2025-04-22T13:53:32.239652Z"
    },
    "papermill": {
     "duration": 1995.655247,
     "end_time": "2025-04-22T13:53:32.241599",
     "exception": false,
     "start_time": "2025-04-22T13:20:16.586352",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n",
      "--- Epoch 1/50 ---\n",
      "Epoch 1 Time: 1m 42s\n",
      "\tTrain Loss: 5.862 | Train PPL: 351.320\n",
      "\t Val. Loss: 5.465 |  Val. PPL: 236.174\n",
      "*** Epoch 1: Validation loss improved. Saving best model to empathetic-transformer-basic-best.pt ***\n",
      "--------------------------------------------------\n",
      "--- Epoch 2/50 ---\n",
      "Epoch 2 Time: 1m 44s\n",
      "\tTrain Loss: 4.956 | Train PPL: 142.041\n",
      "\t Val. Loss: 4.908 |  Val. PPL: 135.329\n",
      "*** Epoch 2: Validation loss improved. Saving best model to empathetic-transformer-basic-best.pt ***\n",
      "--------------------------------------------------\n",
      "--- Epoch 3/50 ---\n",
      "Epoch 3 Time: 1m 44s\n",
      "\tTrain Loss: 4.696 | Train PPL: 109.510\n",
      "\t Val. Loss: 4.801 |  Val. PPL: 121.619\n",
      "*** Epoch 3: Validation loss improved. Saving best model to empathetic-transformer-basic-best.pt ***\n",
      "--------------------------------------------------\n",
      "--- Epoch 4/50 ---\n",
      "Epoch 4 Time: 1m 44s\n",
      "\tTrain Loss: 4.571 | Train PPL:  96.632\n",
      "\t Val. Loss: 4.735 |  Val. PPL: 113.911\n",
      "*** Epoch 4: Validation loss improved. Saving best model to empathetic-transformer-basic-best.pt ***\n",
      "--------------------------------------------------\n",
      "--- Epoch 5/50 ---\n",
      "Epoch 5 Time: 1m 44s\n",
      "\tTrain Loss: 4.458 | Train PPL:  86.341\n",
      "\t Val. Loss: 4.643 |  Val. PPL: 103.877\n",
      "*** Epoch 5: Validation loss improved. Saving best model to empathetic-transformer-basic-best.pt ***\n",
      "--------------------------------------------------\n",
      "--- Epoch 6/50 ---\n",
      "Epoch 6 Time: 1m 45s\n",
      "\tTrain Loss: 4.351 | Train PPL:  77.534\n",
      "\t Val. Loss: 4.597 |  Val. PPL:  99.203\n",
      "*** Epoch 6: Validation loss improved. Saving best model to empathetic-transformer-basic-best.pt ***\n",
      "--------------------------------------------------\n",
      "--- Epoch 7/50 ---\n",
      "Epoch 7 Time: 1m 44s\n",
      "\tTrain Loss: 4.256 | Train PPL:  70.522\n",
      "\t Val. Loss: 4.565 |  Val. PPL:  96.096\n",
      "*** Epoch 7: Validation loss improved. Saving best model to empathetic-transformer-basic-best.pt ***\n",
      "--------------------------------------------------\n",
      "--- Epoch 8/50 ---\n",
      "Epoch 8 Time: 1m 44s\n",
      "\tTrain Loss: 4.179 | Train PPL:  65.288\n",
      "\t Val. Loss: 4.545 |  Val. PPL:  94.142\n",
      "*** Epoch 8: Validation loss improved. Saving best model to empathetic-transformer-basic-best.pt ***\n",
      "--------------------------------------------------\n",
      "--- Epoch 9/50 ---\n",
      "Epoch 9 Time: 1m 45s\n",
      "\tTrain Loss: 4.113 | Train PPL:  61.147\n",
      "\t Val. Loss: 4.533 |  Val. PPL:  93.041\n",
      "*** Epoch 9: Validation loss improved. Saving best model to empathetic-transformer-basic-best.pt ***\n",
      "--------------------------------------------------\n",
      "--- Epoch 10/50 ---\n",
      "Epoch 10 Time: 1m 44s\n",
      "\tTrain Loss: 4.053 | Train PPL:  57.582\n",
      "\t Val. Loss: 4.550 |  Val. PPL:  94.605\n",
      "Epoch 10: Validation loss did not improve. (1/10) Best: 4.533\n",
      "--------------------------------------------------\n",
      "--- Epoch 11/50 ---\n",
      "Epoch 11 Time: 1m 45s\n",
      "\tTrain Loss: 4.000 | Train PPL:  54.617\n",
      "\t Val. Loss: 4.540 |  Val. PPL:  93.735\n",
      "Epoch 11: Validation loss did not improve. (2/10) Best: 4.533\n",
      "--------------------------------------------------\n",
      "--- Epoch 12/50 ---\n",
      "Epoch 12 Time: 1m 45s\n",
      "\tTrain Loss: 3.953 | Train PPL:  52.093\n",
      "\t Val. Loss: 4.551 |  Val. PPL:  94.771\n",
      "Epoch 12: Validation loss did not improve. (3/10) Best: 4.533\n",
      "--------------------------------------------------\n",
      "--- Epoch 13/50 ---\n",
      "Epoch 13 Time: 1m 44s\n",
      "\tTrain Loss: 3.907 | Train PPL:  49.728\n",
      "\t Val. Loss: 4.558 |  Val. PPL:  95.415\n",
      "Epoch 13: Validation loss did not improve. (4/10) Best: 4.533\n",
      "--------------------------------------------------\n",
      "--- Epoch 14/50 ---\n",
      "Epoch 14 Time: 1m 45s\n",
      "\tTrain Loss: 3.864 | Train PPL:  47.652\n",
      "\t Val. Loss: 4.543 |  Val. PPL:  93.975\n",
      "Epoch 14: Validation loss did not improve. (5/10) Best: 4.533\n",
      "--------------------------------------------------\n",
      "--- Epoch 15/50 ---\n",
      "Epoch 15 Time: 1m 45s\n",
      "\tTrain Loss: 3.828 | Train PPL:  45.982\n",
      "\t Val. Loss: 4.556 |  Val. PPL:  95.196\n",
      "Epoch 15: Validation loss did not improve. (6/10) Best: 4.533\n",
      "--------------------------------------------------\n",
      "--- Epoch 16/50 ---\n",
      "Epoch 16 Time: 1m 44s\n",
      "\tTrain Loss: 3.652 | Train PPL:  38.546\n",
      "\t Val. Loss: 4.534 |  Val. PPL:  93.147\n",
      "Epoch 16: Validation loss did not improve. (7/10) Best: 4.533\n",
      "--------------------------------------------------\n",
      "--- Epoch 17/50 ---\n",
      "Epoch 17 Time: 1m 44s\n",
      "\tTrain Loss: 3.589 | Train PPL:  36.198\n",
      "\t Val. Loss: 4.546 |  Val. PPL:  94.277\n",
      "Epoch 17: Validation loss did not improve. (8/10) Best: 4.533\n",
      "--------------------------------------------------\n",
      "--- Epoch 18/50 ---\n",
      "Epoch 18 Time: 1m 45s\n",
      "\tTrain Loss: 3.551 | Train PPL:  34.831\n",
      "\t Val. Loss: 4.566 |  Val. PPL:  96.165\n",
      "Epoch 18: Validation loss did not improve. (9/10) Best: 4.533\n",
      "--------------------------------------------------\n",
      "--- Epoch 19/50 ---\n",
      "Epoch 19 Time: 1m 45s\n",
      "\tTrain Loss: 3.519 | Train PPL:  33.763\n",
      "\t Val. Loss: 4.586 |  Val. PPL:  98.093\n",
      "Epoch 19: Validation loss did not improve. (10/10) Best: 4.533\n",
      "\n",
      "Early stopping triggered after 10 epochs without validation loss improvement.\n",
      "\n",
      "Training Finished. Total time: 33m 15s\n",
      "Best validation loss achieved: 4.533\n",
      "Best model saved to: empathetic-transformer-basic-best.pt\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Starting Training...\")\n",
    "start_training_time = time.time()\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "model_save_path_best = 'empathetic-transformer-basic-best.pt' \n",
    "epochs_no_improve = 0\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    epoch_start_time = time.time()\n",
    "    print(f\"--- Epoch {epoch+1}/{N_EPOCHS} ---\")\n",
    "\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, val_loader, criterion)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_mins = int((epoch_end_time - epoch_start_time) / 60)\n",
    "    epoch_secs = int((epoch_end_time - epoch_start_time) % 60)\n",
    "\n",
    "    train_ppl = math.exp(train_loss) if train_loss < 100 else float('inf') \n",
    "    valid_ppl = math.exp(valid_loss) if valid_loss < 100 else float('inf')\n",
    "\n",
    "    print(f'Epoch {epoch+1} Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {train_ppl:7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {valid_ppl:7.3f}')\n",
    "\n",
    "    scheduler.step(valid_loss)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), model_save_path_best)\n",
    "        epochs_no_improve = 0\n",
    "        print(f\"*** Epoch {epoch+1}: Validation loss improved. Saving best model to {model_save_path_best} ***\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"Epoch {epoch+1}: Validation loss did not improve. ({epochs_no_improve}/{PATIENCE}) Best: {best_valid_loss:.3f}\")\n",
    "\n",
    "    if epochs_no_improve >= PATIENCE:\n",
    "        print(f\"\\nEarly stopping triggered after {PATIENCE} epochs without validation loss improvement.\")\n",
    "        break \n",
    "\n",
    "    print(\"-\" * 50) \n",
    "\n",
    "total_training_time = time.time() - start_training_time\n",
    "print(f\"\\nTraining Finished. Total time: {total_training_time // 60:.0f}m {total_training_time % 60:.0f}s\")\n",
    "print(f\"Best validation loss achieved: {best_valid_loss:.3f}\")\n",
    "print(f\"Best model saved to: {model_save_path_best}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(valid_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ac67a4",
   "metadata": {
    "papermill": {
     "duration": 0.020799,
     "end_time": "2025-04-22T13:53:32.283970",
     "exception": false,
     "start_time": "2025-04-22T13:53:32.263171",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 9. Inference Function (with Sampling)\n",
    "Generates responses using the trained model and various decoding strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c4c62bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T13:53:32.326537Z",
     "iopub.status.busy": "2025-04-22T13:53:32.325953Z",
     "iopub.status.idle": "2025-04-22T13:53:32.335628Z",
     "shell.execute_reply": "2025-04-22T13:53:32.335103Z"
    },
    "papermill": {
     "duration": 0.032027,
     "end_time": "2025-04-22T13:53:32.336639",
     "exception": false,
     "start_time": "2025-04-22T13:53:32.304612",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_response_sampling(\n",
    "    sentence,\n",
    "    model,\n",
    "    tokenizer, \n",
    "    device,\n",
    "    max_len=MAX_LEN,\n",
    "    strategy=\"topk\", \n",
    "    k=10,            \n",
    "    p=0.9,          \n",
    "    temperature=0.8  \n",
    "    ):\n",
    "    \"\"\"Generates a response using the transformer model with sampling.\"\"\"\n",
    "    assert strategy in ['greedy', 'topk', 'topp'], \"Strategy must be 'greedy', 'topk', or 'topp'\"\n",
    "    assert temperature > 0, \"Temperature must be positive\"\n",
    "\n",
    "    model.eval() \n",
    "\n",
    "    tokens = text_to_sequence_tokenizer(sentence, tokenizer)\n",
    "\n",
    "    if len(tokens) > max_len:\n",
    "         print(f\"Warning: Input sentence truncated to {max_len} tokens.\")\n",
    "         tokens = tokens[:max_len]\n",
    "\n",
    "    src_tensor = torch.LongTensor(tokens).unsqueeze(0).to(device) \n",
    "    src_mask = model.make_src_mask(src_tensor) \n",
    "\n",
    "    with torch.no_grad():\n",
    "        enc_src = model.encoder(src_tensor, src_mask) \n",
    "\n",
    "    trg_indices = [SOS_IDX] \n",
    "\n",
    "    for i in range(max_len - 1): \n",
    "        trg_tensor = torch.LongTensor(trg_indices).unsqueeze(0).to(device) \n",
    "        trg_mask = model.make_trg_mask(trg_tensor) \n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
    "\n",
    "        pred_token_logits = output[-1, 0, :] \n",
    "\n",
    "        pred_token_logits = pred_token_logits / temperature\n",
    "\n",
    "        pred_token_probs = F.softmax(pred_token_logits, dim=-1) \n",
    "\n",
    "        next_token_id = -1 \n",
    "        if strategy == 'greedy':\n",
    "            next_token_id = torch.argmax(pred_token_probs).item()\n",
    "        elif strategy == 'topk':\n",
    "            topk_probs, topk_indices = torch.topk(pred_token_probs, k=min(k, pred_token_probs.size(-1))) # Ensure k is not > vocab size\n",
    "            mask = torch.zeros_like(pred_token_probs)\n",
    "            mask.scatter_(0, topk_indices, 1.0)\n",
    "            filtered_probs = pred_token_probs * mask\n",
    "            sum_filtered_probs = torch.sum(filtered_probs)\n",
    "            if sum_filtered_probs > 1e-9: \n",
    "                 filtered_probs = filtered_probs / sum_filtered_probs\n",
    "                 next_token_id = torch.multinomial(filtered_probs, num_samples=1).item()\n",
    "            else: \n",
    "                 print(\"Warning: Top-k resulted in zero probability sum. Using EOS.\")\n",
    "                 next_token_id = EOS_IDX\n",
    "        elif strategy == 'topp':\n",
    "            sorted_probs, sorted_indices = torch.sort(pred_token_probs, descending=True)\n",
    "            cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "            sorted_indices_to_remove = cumulative_probs > p\n",
    "            sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()\n",
    "            sorted_indices_to_remove[0] = 0 \n",
    "            indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "            pred_token_probs[indices_to_remove] = 0.0\n",
    "            sum_filtered_probs = torch.sum(pred_token_probs)\n",
    "            if sum_filtered_probs > 1e-9: \n",
    "                 filtered_probs = pred_token_probs / sum_filtered_probs\n",
    "                 next_token_id = torch.multinomial(filtered_probs, num_samples=1).item()\n",
    "            else: \n",
    "                 print(\"Warning: Top-p resulted in zero probability sum. Using most likely token.\")\n",
    "                 next_token_id = sorted_indices[0].item() \n",
    "\n",
    "        trg_indices.append(next_token_id)\n",
    "\n",
    "        if next_token_id == EOS_IDX:\n",
    "            break\n",
    "\n",
    "    trg_tokens_text = sequence_to_text_tokenizer(trg_indices, tokenizer)\n",
    "\n",
    "    return trg_tokens_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cc1085",
   "metadata": {
    "papermill": {
     "duration": 0.020818,
     "end_time": "2025-04-22T13:53:32.378101",
     "exception": false,
     "start_time": "2025-04-22T13:53:32.357283",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 10. Testing the Chatbot\n",
    "Load the best model saved during training and generate responses for test prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b49a29f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T13:53:32.420711Z",
     "iopub.status.busy": "2025-04-22T13:53:32.420525Z",
     "iopub.status.idle": "2025-04-22T14:13:19.153110Z",
     "shell.execute_reply": "2025-04-22T14:13:19.152189Z"
    },
    "papermill": {
     "duration": 1186.755431,
     "end_time": "2025-04-22T14:13:19.154439",
     "exception": false,
     "start_time": "2025-04-22T13:53:32.399008",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Training pairs...\n",
      "Processing train split...\n",
      "Converted train split to Pandas DataFrame with 76673 rows.\n",
      "Grouped into 17844 conversations.\n",
      "  Processed 10000/17844 conversations...\n",
      "Finished processing train.\n",
      "  Found 58829 input/target pairs.\n",
      "  Skipped 64 conversations with less than 2 utterances.\n",
      "\n",
      "Extracting Validation pairs...\n",
      "Processing validation split...\n",
      "Converted validation split to Pandas DataFrame with 12030 rows.\n",
      "Grouped into 2763 conversations.\n",
      "Finished processing validation.\n",
      "  Found 9267 input/target pairs.\n",
      "  Skipped 5 conversations with less than 2 utterances.\n",
      "\n",
      "Calculating BLEU score on the VALIDATION data...\n",
      "Evaluating on all 9267 examples.\n",
      "Generating responses using strategy='greedy', temp=1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26e2b0929dbf47fa8012af8132ad1db7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating BLEU:   0%|          | 0/9267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "\n",
      "Validation Corpus BLEU Score (Greedy): 1.03\n",
      "Evaluating on all 9267 examples.\n",
      "Generating responses using strategy='topp', temp=0.7, p=0.9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ad869d382e741b68d76fd544e89313c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating BLEU:   0%|          | 0/9267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Validation Corpus BLEU Score (Top-p, p=0.9, T=0.7): 0.63\n",
      "\n",
      "Calculating BLEU score on the TRAINING data...\n",
      "Evaluating on a random sample of 100 examples.\n",
      "Generating responses using strategy='greedy', temp=1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e2f43106a31400cab2b72296e372525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating BLEU:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Input sentence truncated to 60 tokens.\n",
      "Warning: Input sentence truncated to 60 tokens.\n",
      "\n",
      "Training Corpus BLEU Score (Greedy, sample): 1.07\n",
      "\n",
      "Example Validation Generations (Top-p):\n",
      "--- Example 1 ---\n",
      "  Source:    My upstairs neighbors make a ton of noise at all hours of the night. It makes it difficult for me to sleep.\n",
      "  Reference: That really sucks. Maybe you should try egging their door? Or just break in and pretend you're bigfoot while they're trying to sleep.\n",
      "  Candidate: That is awesome ! I bet you had a good time .\n",
      "  Sentence BLEU: 0.59\n",
      "--- Example 2 ---\n",
      "  Source:    That really sucks. Maybe you should try egging their door? Or just break in and pretend you're bigfoot while they're trying to sleep.\n",
      "  Reference: I'm not trying to get arrested! I think I'll just wait things out until I move in two months.\n",
      "  Candidate: I am trying to do that . I ' m just so sad .\n",
      "  Sentence BLEU: 2.09\n",
      "--- Example 3 ---\n",
      "  Source:    I'm not trying to get arrested! I think I'll just wait things out until I move in two months.\n",
      "  Reference: I would go with the bigfoot option. You can get a costume on the cheap on ebay nowadays. I've used that tactic countless times and it has never failed!\n",
      "  Candidate: Good luck\n",
      "  Sentence BLEU: 0.00\n",
      "--- Example 4 ---\n",
      "  Source:    Im expecting a good bonus to be on this check coming up. I can finally go buy a new car!\n",
      "  Reference: That is some exciting news. Do you already know what kind of vehicle you want?\n",
      "  Candidate: Oh_comma_ I bet you can ' t wait for it .\n",
      "  Sentence BLEU: 1.30\n",
      "--- Example 5 ---\n",
      "  Source:    That is some exciting news. Do you already know what kind of vehicle you want?\n",
      "  Reference: Yes! Very exciting! Yes I had my eye on one all year. I cant wait\n",
      "  Candidate: I am not sure but I ' m glad it ' s just not sure .\n",
      "  Sentence BLEU: 1.43\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "import nltk \n",
    "\n",
    "\n",
    "def calculate_bleu(\n",
    "    input_sentences, \n",
    "    target_sentences, \n",
    "    model,\n",
    "    tokenizer,\n",
    "    device,\n",
    "    max_len=MAX_LEN,\n",
    "    strategy='greedy',\n",
    "    k=10,\n",
    "    p=0.9,\n",
    "    temperature=1.0,\n",
    "    max_examples=None):\n",
    "    \"\"\"\n",
    "    Calculates BLEU score over a dataset provided as separate input and target lists.\n",
    "\n",
    "    Args:\n",
    "        input_sentences (list): List of source sentences (strings).\n",
    "        target_sentences (list): List of corresponding target/reference sentences (strings).\n",
    "        model: The trained transformer model.\n",
    "        tokenizer: The tokenizer instance used by the generation function.\n",
    "        device: The device ('cuda' or 'cpu').\n",
    "        max_len: Maximum sequence length for generation.\n",
    "        strategy: Sampling strategy ('greedy', 'topk', 'topp').\n",
    "        k: k for top-k sampling.\n",
    "        p: p for top-p sampling.\n",
    "        temperature: Temperature for sampling.\n",
    "        max_examples (int, optional): Limit the number of examples to evaluate. Defaults to None (evaluate all).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (corpus_bleu_score, list_of_details)\n",
    "               corpus_bleu_score (float): The corpus BLEU-4 score (0.0-1.0).\n",
    "               list_of_details (list): List of tuples [(source, reference, candidate, sentence_bleu), ...]\n",
    "    \"\"\"\n",
    "    assert len(input_sentences) == len(target_sentences), \\\n",
    "        f\"Input and target sentence lists must have the same length ({len(input_sentences)} != {len(target_sentences)})\"\n",
    "\n",
    "    targets = []     \n",
    "    predictions = []  \n",
    "    details = []      \n",
    "\n",
    "    model.eval() \n",
    "\n",
    "    eval_data_pairs = list(zip(input_sentences, target_sentences))\n",
    "\n",
    "    if max_examples is not None and len(eval_data_pairs) > max_examples:\n",
    "        print(f\"Evaluating on a random sample of {max_examples} examples.\")\n",
    "        eval_data_pairs = random.sample(eval_data_pairs, max_examples)\n",
    "    elif max_examples is None:\n",
    "         print(f\"Evaluating on all {len(eval_data_pairs)} examples.\")\n",
    "    else: # max_examples >= len(data)\n",
    "         print(f\"Evaluating on all {len(eval_data_pairs)} examples (max_examples={max_examples}).\")\n",
    "\n",
    "\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "        tokenize_for_bleu = word_tokenize\n",
    "    except LookupError:\n",
    "        print(\"NLTK 'punkt' tokenizer model not found. Downloading...\")\n",
    "        nltk.download('punkt')\n",
    "        tokenize_for_bleu = word_tokenize\n",
    "    except Exception as e:\n",
    "        print(f\"Could not initialize nltk.word_tokenize ({e}). Falling back to simple split().\")\n",
    "        tokenize_for_bleu = lambda s: s.split()\n",
    "\n",
    "\n",
    "    print(f\"Generating responses using strategy='{strategy}', temp={temperature}\" +\n",
    "          (f\", k={k}\" if strategy=='topk' else \"\") +\n",
    "          (f\", p={p}\" if strategy=='topp' else \"\"))\n",
    "\n",
    "    for source_text, target_text in tqdm(eval_data_pairs, desc=\"Calculating BLEU\"):\n",
    "\n",
    "        candidate_text = generate_response_sampling(\n",
    "            sentence=source_text,\n",
    "            model=model,\n",
    "            tokenizer=tokenizer, \n",
    "            device=device,\n",
    "            max_len=max_len,\n",
    "            strategy=strategy,\n",
    "            k=k,\n",
    "            p=p,\n",
    "            temperature=temperature\n",
    "        )\n",
    "\n",
    "        if candidate_text is None:\n",
    "            candidate_text = \"\" \n",
    "\n",
    "        try:\n",
    "            target_text_str = str(target_text) if target_text is not None else \"\"\n",
    "            tokenized_target = [tokenize_for_bleu(target_text_str.lower())] # NLTK expects list of references\n",
    "        except Exception as e:\n",
    "             print(f\"Warning: Error tokenizing target text '{target_text}': {e}. Using empty reference.\")\n",
    "             tokenized_target = [[]]\n",
    "             target_text_str = \"\" \n",
    "\n",
    "        try:\n",
    "            candidate_text_str = str(candidate_text) if candidate_text is not None else \"\"\n",
    "            tokenized_candidate = tokenize_for_bleu(candidate_text_str.lower())\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error tokenizing candidate text '{candidate_text}': {e}. Using empty candidate.\")\n",
    "            tokenized_candidate = []\n",
    "            candidate_text_str = \"\" \n",
    "\n",
    "\n",
    "        sent_bleu = 0.0\n",
    "        if tokenized_candidate and tokenized_target and tokenized_target[0]:\n",
    "             try:\n",
    "                 sent_bleu = sentence_bleu(tokenized_target, tokenized_candidate,\n",
    "                                           smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method1)\n",
    "             except Exception as e:\n",
    "                 print(f\"Error calculating sentence BLEU for cand='{candidate_text_str}', ref='{target_text_str}': {e}\")\n",
    "                 sent_bleu = 0.0\n",
    "\n",
    "        targets.append(tokenized_target)\n",
    "        predictions.append(tokenized_candidate)\n",
    "\n",
    "        details.append((source_text, target_text_str, candidate_text_str, sent_bleu))\n",
    "\n",
    "    try:\n",
    "        smoothing = nltk.translate.bleu_score.SmoothingFunction().method1\n",
    "        corpus_bleu_score = corpus_bleu(targets, predictions, smoothing_function=smoothing)\n",
    "    except ZeroDivisionError:\n",
    "        print(\"Warning: ZeroDivisionError during corpus BLEU calculation. Check if predictions or references are systematically empty.\")\n",
    "        corpus_bleu_score = 0.0\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating corpus BLEU: {e}\")\n",
    "        corpus_bleu_score = 0.0\n",
    "\n",
    "\n",
    "    return corpus_bleu_score, details\n",
    "\n",
    "print(\"Extracting Training pairs...\")\n",
    "train_inputs, train_targets = extract_pairs_corrected('train', empathetic_dialogues, subset_size=DATASET_SUBSET_SIZE)\n",
    "\n",
    "print(\"\\nExtracting Validation pairs...\")\n",
    "val_inputs, val_targets = extract_pairs_corrected('validation', empathetic_dialogues, subset_size=None) # Use full validation set\n",
    "\n",
    "print(\"\\nCalculating BLEU score on the VALIDATION data...\")\n",
    "model.to(device)\n",
    "val_bleu_greedy, val_details_greedy = calculate_bleu(\n",
    "    input_sentences=val_inputs,   \n",
    "    target_sentences=val_targets, \n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    max_len=MAX_LEN,\n",
    "    strategy='greedy',\n",
    "    temperature=1.0,\n",
    "    max_examples=None \n",
    ")\n",
    "print(f\"\\nValidation Corpus BLEU Score (Greedy): {val_bleu_greedy * 100:.2f}\")\n",
    "\n",
    "val_bleu_topp, val_details_topp = calculate_bleu(\n",
    "    input_sentences=val_inputs,   \n",
    "    target_sentences=val_targets, \n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    max_len=MAX_LEN,\n",
    "    strategy='topp',\n",
    "    p=0.9,\n",
    "    temperature=0.7,\n",
    "    max_examples=None \n",
    ")\n",
    "print(f\"Validation Corpus BLEU Score (Top-p, p=0.9, T=0.7): {val_bleu_topp * 100:.2f}\")\n",
    "\n",
    "print(\"\\nCalculating BLEU score on the TRAINING data...\")\n",
    "train_bleu_greedy, train_details_greedy = calculate_bleu(\n",
    "    input_sentences=train_inputs,   \n",
    "    target_sentences=train_targets, \n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    max_len=MAX_LEN,\n",
    "    strategy='greedy',\n",
    "    temperature=1.0,\n",
    "    max_examples=100 \n",
    ")\n",
    "print(f\"\\nTraining Corpus BLEU Score (Greedy, sample): {train_bleu_greedy * 100:.2f}\")\n",
    "\n",
    "\n",
    "print(\"\\nExample Validation Generations (Top-p):\")\n",
    "for i, (src, ref, cand, s_bleu) in enumerate(val_details_topp[:5]): \n",
    "    print(f\"--- Example {i+1} ---\")\n",
    "    print(f\"  Source:    {src}\")\n",
    "    print(f\"  Reference: {ref}\")\n",
    "    print(f\"  Candidate: {cand}\")\n",
    "    print(f\"  Sentence BLEU: {s_bleu*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "77227e66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T14:13:19.227552Z",
     "iopub.status.busy": "2025-04-22T14:13:19.227304Z",
     "iopub.status.idle": "2025-04-22T14:13:19.231680Z",
     "shell.execute_reply": "2025-04-22T14:13:19.230954Z"
    },
    "papermill": {
     "duration": 0.049427,
     "end_time": "2025-04-22T14:13:19.233056",
     "exception": false,
     "start_time": "2025-04-22T14:13:19.183629",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_prompts = [\n",
    "    \"I just got a promotion at work! I'm so happy.\",\n",
    "    \"I feel really lonely these days. No one seems to understand me.\",\n",
    "    \"My best friend just moved to another country, and I miss them so much.\",\n",
    "    \"I failed my exam even after studying so hard. I feel so disappointed.\",\n",
    "    \"I helped a stranger today, and it made me feel really good.\",\n",
    "    \"I'm really nervous about my job interview tomorrow.\",\n",
    "    \"I just finished a marathon! I feel so accomplished.\",\n",
    "    \"My pet passed away last night, and I’m heartbroken.\",\n",
    "    \"I got stuck in traffic for hours today. It was so frustrating!\",\n",
    "    \"I found out I’m going to be a parent! I’m overjoyed but also a little scared.\",\n",
    "    \"I’ve been feeling really unmotivated lately. I don’t know what to do.\",\n",
    "    \"I had a great conversation with an old friend today. It felt amazing!\",\n",
    "    \"I lost my wallet today. Now I have to replace everything.\",\n",
    "    \"I just tried a new hobby, and I think I love it!\",\n",
    "    \"Someone criticized my work today, and it made me feel insecure.\",\n",
    "    \"I’m struggling with my mental health, and I don’t know how to talk about it.\",\n",
    "    \"My birthday is coming up, but I don’t feel excited this year.\",\n",
    "    \"I finally confronted someone who hurt me in the past. It was really hard.\",\n",
    "    \"I got my dream job! I can’t believe it’s happening.\",\n",
    "    \"I feel so stuck in life. I don’t know what my next step should be.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad00b925",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T14:13:19.289748Z",
     "iopub.status.busy": "2025-04-22T14:13:19.289486Z",
     "iopub.status.idle": "2025-04-22T14:13:21.749043Z",
     "shell.execute_reply": "2025-04-22T14:13:21.748223Z"
    },
    "papermill": {
     "duration": 2.492395,
     "end_time": "2025-04-22T14:13:21.750160",
     "exception": false,
     "start_time": "2025-04-22T14:13:19.257765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chatbot Test (with Sampling, Best Model) ---\n",
      "Loading best model state from empathetic-transformer-basic-best.pt...\n",
      "Model loaded successfully using weights_only=True.\n",
      "\n",
      "--- Testing with Top-k (k=10, temp=0.8) ---\n",
      "User: I just got a promotion at work! I'm so happy.\n",
      "Bot:  Wow_comma_ I ' m sorry to hear that_comma_ but that is a surprise to you are a new job to get a new fan .\n",
      "--------------------\n",
      "User: I feel really lonely these days. No one seems to understand me.\n",
      "Bot:  Thanks . I have my friend .\n",
      "--------------------\n",
      "User: My best friend just moved to another country, and I miss them so much.\n",
      "Bot:  That ' s good to hear . I ' m sure it ' s not too to do you think she will do the police to get it for you .\n",
      "--------------------\n",
      "User: I failed my exam even after studying so hard. I feel so disappointed.\n",
      "Bot:  What did you get ?\n",
      "--------------------\n",
      "User: I helped a stranger today, and it made me feel really good.\n",
      "Bot:  Oh that ' s so nice of you to do that . What ' s your son ?\n",
      "--------------------\n",
      "User: I'm really nervous about my job interview tomorrow.\n",
      "Bot:  Wow . What kind of car did you get to you ?\n",
      "--------------------\n",
      "User: I just finished a marathon! I feel so accomplished.\n",
      "Bot:  Oh no ! I would have been too to see the same one .\n",
      "--------------------\n",
      "User: My pet passed away last night, and I’m heartbroken.\n",
      "Bot:  Wow ! Did you do ?\n",
      "--------------------\n",
      "User: I got stuck in traffic for hours today. It was so frustrating!\n",
      "Bot:  Oh no . That must have been so stressful to be really nice of them .\n",
      "--------------------\n",
      "User: I found out I’m going to be a parent! I’m overjoyed but also a little scared.\n",
      "Bot:  That ' s awesome . Are you going to do it ?\n",
      "--------------------\n",
      "User: I’ve been feeling really unmotivated lately. I don’t know what to do.\n",
      "Bot:  That is good . I think you are going to be able to have a big job .\n",
      "--------------------\n",
      "User: I had a great conversation with an old friend today. It felt amazing!\n",
      "Bot:  What are you headed to ?\n",
      "--------------------\n",
      "User: I lost my wallet today. Now I have to replace everything.\n",
      "Bot:  Oh no ! Was he do you respond to do ?\n",
      "--------------------\n",
      "User: I just tried a new hobby, and I think I love it!\n",
      "Bot:  That ' s great ! It is great\n",
      "--------------------\n",
      "User: Someone criticized my work today, and it made me feel insecure.\n",
      "Bot:  Oh that ' s great ! Did you find it ?\n",
      "--------------------\n",
      "User: I’m struggling with my mental health, and I don’t know how to talk about it.\n",
      "Bot:  Oh_comma_ i know how you must be a good time !\n",
      "--------------------\n",
      "User: My birthday is coming up, but I don’t feel excited this year.\n",
      "Bot:  Wow_comma_ I hope that it is a good situation . It must be a bad accomplishment .\n",
      "--------------------\n",
      "User: I finally confronted someone who hurt me in the past. It was really hard.\n",
      "Bot:  What happened ? How did you get ?\n",
      "--------------------\n",
      "User: I got my dream job! I can’t believe it’s happening.\n",
      "Bot:  What did you get ?\n",
      "--------------------\n",
      "User: I feel so stuck in life. I don’t know what my next step should be.\n",
      "Bot:  That ' s really great . I have never got to be able to get it up ?\n",
      "--------------------\n",
      "\n",
      "--- Testing with Top-p (p=0.9, temp=0.8) ---\n",
      "User: I just got a promotion at work! I'm so happy.\n",
      "Bot:  That sounds like it ' s okay .\n",
      "--------------------\n",
      "User: I feel really lonely these days. No one seems to understand me.\n",
      "Bot:  I agree . I still watched the time .\n",
      "--------------------\n",
      "User: My best friend just moved to another country, and I miss them so much.\n",
      "Bot:  Yeah that is nice . How long was your favorite thing ?\n",
      "--------------------\n",
      "User: I failed my exam even after studying so hard. I feel so disappointed.\n",
      "Bot:  I am so sorry to hear that . It must have been supposed to be really tough .\n",
      "--------------------\n",
      "User: I helped a stranger today, and it made me feel really good.\n",
      "Bot:  That is nice_comma_ it take it ?\n",
      "--------------------\n",
      "User: I'm really nervous about my job interview tomorrow.\n",
      "Bot:  What did you get to do\n",
      "--------------------\n",
      "User: I just finished a marathon! I feel so accomplished.\n",
      "Bot:  That ' s so great !\n",
      "--------------------\n",
      "User: My pet passed away last night, and I’m heartbroken.\n",
      "Bot:  Do you go on your one ?\n",
      "--------------------\n",
      "User: I got stuck in traffic for hours today. It was so frustrating!\n",
      "Bot:  Sounds like there ' s really good and I hope it was his fence ?\n",
      "--------------------\n",
      "User: I found out I’m going to be a parent! I’m overjoyed but also a little scared.\n",
      "Bot:  Oh_comma_ ! I ' m sure you been able to do about your sister .\n",
      "--------------------\n",
      "User: I’ve been feeling really unmotivated lately. I don’t know what to do.\n",
      "Bot:  That ' s awesome . What kind of business ?\n",
      "--------------------\n",
      "User: I had a great conversation with an old friend today. It felt amazing!\n",
      "Bot:  Oh no ! How many your son is it ?\n",
      "--------------------\n",
      "User: I lost my wallet today. Now I have to replace everything.\n",
      "Bot:  That sounds so sad . What was there did you know ?\n",
      "--------------------\n",
      "User: I just tried a new hobby, and I think I love it!\n",
      "Bot:  Oh wow ! What about you have ?\n",
      "--------------------\n",
      "User: Someone criticized my work today, and it made me feel insecure.\n",
      "Bot:  Wow . That ' s horrible . Are you alone ?\n",
      "--------------------\n",
      "User: I’m struggling with my mental health, and I don’t know how to talk about it.\n",
      "Bot:  Oh wow ! That must know it ?\n",
      "--------------------\n",
      "User: My birthday is coming up, but I don’t feel excited this year.\n",
      "Bot:  That sounds like a bit color !\n",
      "--------------------\n",
      "User: I finally confronted someone who hurt me in the past. It was really hard.\n",
      "Bot:  Oh no ! Were you going to ride them ?\n",
      "--------------------\n",
      "User: I got my dream job! I can’t believe it’s happening.\n",
      "Bot:  I am sorry to hear that .\n",
      "--------------------\n",
      "User: I feel so stuck in life. I don’t know what my next step should be.\n",
      "Bot:  Oh wow . Sounds like you must be nervous with you had a day ?\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Chatbot Test (with Sampling, Best Model) ---\")\n",
    "\n",
    "if 'model' in locals() or 'model' in globals():\n",
    "    if os.path.exists(model_save_path_best): \n",
    "        print(f\"Loading best model state from {model_save_path_best}...\")\n",
    "        try:\n",
    "            model.load_state_dict(torch.load(model_save_path_best, map_location=device, weights_only=True))\n",
    "            print(\"Model loaded successfully using weights_only=True.\")\n",
    "        except (RuntimeError, TypeError, KeyError) as e: \n",
    "            print(f\"Could not load with weights_only=True ({e}). Trying default loading...\")\n",
    "            try:\n",
    "                 model.load_state_dict(torch.load(model_save_path_best, map_location=device))\n",
    "                 print(\"Model loaded successfully using default loading.\")\n",
    "            except Exception as e_fallback:\n",
    "                 print(f\"ERROR: Failed to load model state dict: {e_fallback}\")\n",
    "    else:\n",
    "         print(f\"Warning: Best model file '{model_save_path_best}' not found.\")\n",
    "         print(\"Testing with the model's current state (might be untrained or from last epoch).\")\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    if 'tokenizer' in locals() or 'tokenizer' in globals():\n",
    "        print(\"\\n--- Testing with Top-k (k=10, temp=0.8) ---\")\n",
    "        for prompt in test_prompts:\n",
    "            response = generate_response_sampling(\n",
    "                prompt, model, tokenizer, device, max_len=MAX_LEN, \n",
    "                strategy=\"topk\", k=10, temperature=0.8\n",
    "            )\n",
    "            print(f\"User: {prompt}\")\n",
    "            print(f\"Bot:  {response}\")\n",
    "            print(\"-\" * 20)\n",
    "\n",
    "        print(\"\\n--- Testing with Top-p (p=0.9, temp=0.8) ---\")\n",
    "        for prompt in test_prompts:\n",
    "            response = generate_response_sampling(\n",
    "                prompt, model, tokenizer, device, max_len=MAX_LEN, \n",
    "                strategy=\"topp\", p=0.9, temperature=0.8\n",
    "            )\n",
    "            print(f\"User: {prompt}\")\n",
    "            print(f\"Bot:  {response}\")\n",
    "            print(\"-\" * 20)\n",
    "    else:\n",
    "        print(\"ERROR: Tokenizer not found. Cannot run inference.\")\n",
    "\n",
    "else:\n",
    "    print(\"ERROR: Model not defined. Cannot run inference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60e8d1a",
   "metadata": {
    "papermill": {
     "duration": 0.024522,
     "end_time": "2025-04-22T14:13:21.800281",
     "exception": false,
     "start_time": "2025-04-22T14:13:21.775759",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 11. Final Thoughts and Limitations\n",
    "\n",
    "*   **Tokenizer:** Using WordPiece is a significant improvement over basic splitting, handling unknown words and morphology better.\n",
    "*   **Training:** Training on the full dataset with validation and early stopping is crucial for better generalization and preventing overfitting. Expect much longer training times.\n",
    "*   **Model Quality:** Even with these improvements, the quality depends heavily on sufficient training time, data quality, and potentially further hyperparameter tuning or model scaling. The responses should be more coherent now, but may still lack deep understanding or perfect empathy.\n",
    "*   **Safety & Evaluation:** Still lacks safety layers and proper evaluation metrics (BLEU, ROUGE, perplexity, human evaluation). This is critical for any real deployment.\n",
    "*   **Next Steps:** Consider experimenting with learning rate schedulers, different optimizers (AdamW), larger model sizes (if resources allow), or more advanced attention mechanisms. For production-level quality, fine-tuning large pre-trained models is the standard approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63519dcf",
   "metadata": {
    "papermill": {
     "duration": 0.024189,
     "end_time": "2025-04-22T14:13:21.849138",
     "exception": false,
     "start_time": "2025-04-22T14:13:21.824949",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Using GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c8edf5d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T14:13:21.900178Z",
     "iopub.status.busy": "2025-04-22T14:13:21.899930Z",
     "iopub.status.idle": "2025-04-22T14:36:28.398016Z",
     "shell.execute_reply": "2025-04-22T14:36:28.397098Z"
    },
    "papermill": {
     "duration": 1386.526106,
     "end_time": "2025-04-22T14:36:28.399608",
     "exception": false,
     "start_time": "2025-04-22T14:13:21.873502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.5.1+cu124\n",
      "Using device: cuda\n",
      "Loading EmpatheticDialogues dataset...\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['conv_id', 'utterance_idx', 'context', 'prompt', 'speaker_idx', 'utterance', 'selfeval', 'tags'],\n",
      "        num_rows: 76673\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['conv_id', 'utterance_idx', 'context', 'prompt', 'speaker_idx', 'utterance', 'selfeval', 'tags'],\n",
      "        num_rows: 12030\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['conv_id', 'utterance_idx', 'context', 'prompt', 'speaker_idx', 'utterance', 'selfeval', 'tags'],\n",
      "        num_rows: 10943\n",
      "    })\n",
      "})\n",
      "Dataset loaded successfully.\n",
      "Extracting Training pairs...\n",
      "Processing train split...\n",
      "Converted train split to Pandas DataFrame with 76673 rows.\n",
      "Grouped into 17844 conversations.\n",
      "  Processed 10000/17844 conversations...\n",
      "Finished processing train.\n",
      "  Found 58829 input/target pairs.\n",
      "  Skipped 64 conversations with less than 2 utterances.\n",
      "\n",
      "Extracting Validation pairs...\n",
      "Processing validation split...\n",
      "Converted validation split to Pandas DataFrame with 12030 rows.\n",
      "Grouped into 2763 conversations.\n",
      "Finished processing validation.\n",
      "  Found 9267 input/target pairs.\n",
      "  Skipped 5 conversations with less than 2 utterances.\n",
      "Building word vocabulary...\n",
      "Vocabulary size: 16041 (min_freq=3, including 4 special tokens)\n",
      "Found 16037 words meeting min_freq=3\n",
      "Word Token IDs: PAD=0, SOS=1, EOS=2, UNK=3\n",
      "Loading GloVe vectors from /kaggle/input/glove6b300dtxt/glove.6B.300d.txt...\n",
      "Loaded 400000 word vectors.\n",
      "Creating embedding matrix...\n",
      "Initialized embedding matrix for 16041 words.\n",
      "  Found pre-trained vectors for 8374 words in vocab.\n",
      "  7667 words in vocab not found in GloVe (using random init).\n",
      "Cleaned up GloVe index.\n",
      "Tokenizing (words) and padding sequences (MAX_LEN=60)...\n",
      "  Processed 10000/58829 pairs...\n",
      "  Processed 20000/58829 pairs...\n",
      "  Processed 30000/58829 pairs...\n",
      "  Processed 40000/58829 pairs...\n",
      "  Processed 50000/58829 pairs...\n",
      "Padding sequences...\n",
      "Padding complete.\n",
      "Tokenizing (words) and padding sequences (MAX_LEN=60)...\n",
      "Padding sequences...\n",
      "Padding complete.\n",
      "DialogueDataset created with 58829 samples.\n",
      "DialogueDataset created with 9267 samples.\n",
      "Created Train DataLoader (1839 batches)\n",
      "Created Validation DataLoader (290 batches)\n",
      "Cleaned up intermediate data lists.\n",
      "Initializing model components with GloVe embeddings...\n",
      "Encoder embeddings will be fine-tuned.\n",
      "Decoder embeddings will be fine-tuned.\n",
      "Applying Xavier uniform initialization to non-embedding layers...\n",
      "The model has 19,560,813 trainable parameters.\n",
      "Model is on device: cuda:0\n",
      "Cleaned up embedding matrix.\n",
      "Starting Training...\n",
      "--- Epoch 1/50 ---\n",
      "Epoch 1 Time: 2m 1s\n",
      "\tTrain Loss: 6.148 | Train PPL: 467.902\n",
      "\t Val. Loss: 6.068 |  Val. PPL: 431.872\n",
      "*** Epoch 1: Validation loss improved. Saving best model to glove-transformer-basic-best.pt ***\n",
      "--------------------------------------------------\n",
      "--- Epoch 2/50 ---\n",
      "Epoch 2 Time: 2m 2s\n",
      "\tTrain Loss: 5.984 | Train PPL: 396.854\n",
      "\t Val. Loss: 6.110 |  Val. PPL: 450.119\n",
      "Epoch 2: Validation loss did not improve. (1/10) Best: 6.068\n",
      "--------------------------------------------------\n",
      "--- Epoch 3/50 ---\n",
      "Epoch 3 Time: 2m 2s\n",
      "\tTrain Loss: 5.981 | Train PPL: 395.743\n",
      "\t Val. Loss: 6.338 |  Val. PPL: 565.788\n",
      "Epoch 3: Validation loss did not improve. (2/10) Best: 6.068\n",
      "--------------------------------------------------\n",
      "--- Epoch 4/50 ---\n",
      "Epoch 4 Time: 2m 1s\n",
      "\tTrain Loss: 5.968 | Train PPL: 390.585\n",
      "\t Val. Loss: 6.157 |  Val. PPL: 472.032\n",
      "Epoch 4: Validation loss did not improve. (3/10) Best: 6.068\n",
      "--------------------------------------------------\n",
      "--- Epoch 5/50 ---\n",
      "Epoch 5 Time: 2m 2s\n",
      "\tTrain Loss: 6.104 | Train PPL: 447.802\n",
      "\t Val. Loss: 6.479 |  Val. PPL: 651.274\n",
      "Epoch 5: Validation loss did not improve. (4/10) Best: 6.068\n",
      "--------------------------------------------------\n",
      "--- Epoch 6/50 ---\n",
      "Epoch 6 Time: 2m 2s\n",
      "\tTrain Loss: 6.069 | Train PPL: 432.308\n",
      "\t Val. Loss: 6.411 |  Val. PPL: 608.707\n",
      "Epoch 6: Validation loss did not improve. (5/10) Best: 6.068\n",
      "--------------------------------------------------\n",
      "--- Epoch 7/50 ---\n",
      "Epoch 7 Time: 2m 2s\n",
      "\tTrain Loss: 6.061 | Train PPL: 428.654\n",
      "\t Val. Loss: 6.400 |  Val. PPL: 601.698\n",
      "Epoch 7: Validation loss did not improve. (6/10) Best: 6.068\n",
      "--------------------------------------------------\n",
      "--- Epoch 8/50 ---\n",
      "Epoch 8 Time: 2m 2s\n",
      "\tTrain Loss: 6.055 | Train PPL: 426.360\n",
      "\t Val. Loss: 6.347 |  Val. PPL: 570.831\n",
      "Epoch 8: Validation loss did not improve. (7/10) Best: 6.068\n",
      "--------------------------------------------------\n",
      "--- Epoch 9/50 ---\n",
      "Epoch 9 Time: 2m 2s\n",
      "\tTrain Loss: 6.052 | Train PPL: 425.162\n",
      "\t Val. Loss: 6.455 |  Val. PPL: 635.967\n",
      "Epoch 9: Validation loss did not improve. (8/10) Best: 6.068\n",
      "--------------------------------------------------\n",
      "--- Epoch 10/50 ---\n",
      "Epoch 10 Time: 2m 1s\n",
      "\tTrain Loss: 6.049 | Train PPL: 423.558\n",
      "\t Val. Loss: 6.542 |  Val. PPL: 693.831\n",
      "Epoch 10: Validation loss did not improve. (9/10) Best: 6.068\n",
      "--------------------------------------------------\n",
      "--- Epoch 11/50 ---\n",
      "Epoch 11 Time: 2m 2s\n",
      "\tTrain Loss: 6.031 | Train PPL: 415.987\n",
      "\t Val. Loss: 6.522 |  Val. PPL: 680.079\n",
      "Epoch 11: Validation loss did not improve. (10/10) Best: 6.068\n",
      "\n",
      "Early stopping triggered after 10 epochs without validation loss improvement.\n",
      "\n",
      "Training Finished. Total time: 22m 23s\n",
      "Best validation loss achieved: 6.068\n",
      "Best model saved to: glove-transformer-basic-best.pt\n",
      "\n",
      "--- Chatbot Test (GloVe Embeddings, Best Model) ---\n",
      "Loading best model state from glove-transformer-basic-best.pt...\n",
      "Model loaded successfully (weights_only=True).\n",
      "\n",
      "--- Testing with Top-k (k=10, temp=0.8) ---\n",
      "User: I just got a promotion at work! I'm so happy.\n",
      "Bot:  i i a a a is the i to is to you was a to to is to i to is to to the <UNK>\n",
      "--------------------\n",
      "User: I feel really lonely these days. No one seems to understand me.\n",
      "Bot:  oh i a you you to was was <UNK> i the you to to is a a is you to it\n",
      "--------------------\n",
      "User: My best friend just moved to another country, and I miss them so much.\n",
      "Bot:  it <UNK>\n",
      "--------------------\n",
      "User: I failed my exam even after studying so hard. I feel so disappointed.\n",
      "Bot:  i a you it to is you to i to\n",
      "--------------------\n",
      "User: I helped a stranger today, and it made me feel really good.\n",
      "Bot:  did was is that\n",
      "--------------------\n",
      "User: I'm really nervous about my job interview tomorrow.\n",
      "Bot:  oh that to <UNK>\n",
      "--------------------\n",
      "\n",
      "--- Testing with Top-p (p=0.9, temp=0.8) ---\n",
      "User: I just got a promotion at work! I'm so happy.\n",
      "Bot:  i the\n",
      "--------------------\n",
      "User: I feel really lonely these days. No one seems to understand me.\n",
      "Bot:  i <UNK>\n",
      "--------------------\n",
      "User: My best friend just moved to another country, and I miss them so much.\n",
      "Bot:  i in been an get the on you all that\n",
      "--------------------\n",
      "User: I failed my exam even after studying so hard. I feel so disappointed.\n",
      "Bot:  what time have the\n",
      "--------------------\n",
      "User: I helped a stranger today, and it made me feel really good.\n",
      "Bot:  that's i a you to am you to to for <UNK>\n",
      "--------------------\n",
      "User: I'm really nervous about my job interview tomorrow.\n",
      "Bot:  you to can that my you a that are so the to so the to was was a to has on you sounds is is <UNK>\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets scikit-learn pandas -q\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import numpy as np \n",
    "\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "\n",
    "DATASET_SUBSET_SIZE = None \n",
    "MAX_LEN = 60             \n",
    "MIN_FREQ = 3             \n",
    "\n",
    "GLOVE_PATH = '/kaggle/input/glove6b300dtxt/glove.6B.300d.txt' \n",
    "EMBEDDING_DIM = 300      \n",
    "FREEZE_EMBEDDINGS = False \n",
    "\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.0005\n",
    "N_EPOCHS = 50\n",
    "CLIP = 1.0\n",
    "PATIENCE = 10\n",
    "\n",
    "D_MODEL = EMBEDDING_DIM\n",
    "N_HEADS = 10             \n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "PF_DIM = 512\n",
    "DROPOUT = 0.3\n",
    "\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "SOS_TOKEN = \"<SOS>\"\n",
    "EOS_TOKEN = \"<EOS>\"\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "SPECIAL_TOKENS = [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN, UNK_TOKEN]\n",
    "\n",
    "\n",
    "print(\"Loading EmpatheticDialogues dataset...\")\n",
    "try:\n",
    "    empathetic_dialogues = load_dataset(\"empathetic_dialogues\", trust_remote_code=True)\n",
    "    print(empathetic_dialogues)\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    raise e\n",
    "\n",
    "\n",
    "def extract_pairs_corrected(split_name, dataset, subset_size=None):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    data_split = dataset[split_name]\n",
    "    print(f\"Processing {split_name} split...\")\n",
    "    try:\n",
    "        df = data_split.to_pandas()\n",
    "        print(f\"Converted {split_name} split to Pandas DataFrame with {len(df)} rows.\")\n",
    "        all_conv_ids = df['conv_id'].unique()\n",
    "        if subset_size and subset_size < len(all_conv_ids):\n",
    "            print(f\"Selecting a subset of {subset_size} conversation IDs for {split_name} split.\")\n",
    "            selected_conv_ids = random.sample(list(all_conv_ids), subset_size)\n",
    "            df = df[df['conv_id'].isin(selected_conv_ids)]\n",
    "            print(f\"Subset DataFrame has {len(df)} rows after filtering by conv_id.\")\n",
    "        grouped = df.sort_values('utterance_idx').groupby('conv_id')\n",
    "        print(f\"Grouped into {len(grouped)} conversations.\")\n",
    "        processed_convs = 0\n",
    "        skipped_short_convs = 0\n",
    "        for conv_id, group in grouped:\n",
    "            utterances = group['utterance'].tolist()\n",
    "            utterances = [u.strip() for u in utterances if u.strip()]\n",
    "            if len(utterances) >= 2:\n",
    "                for i in range(len(utterances) - 1):\n",
    "                    inputs.append(utterances[i])\n",
    "                    targets.append(utterances[i+1])\n",
    "            else:\n",
    "                skipped_short_convs += 1\n",
    "            processed_convs += 1\n",
    "            if processed_convs % 10000 == 0: print(f\"  Processed {processed_convs}/{len(grouped)} conversations...\")\n",
    "        print(f\"Finished processing {split_name}.\")\n",
    "        print(f\"  Found {len(inputs)} input/target pairs.\")\n",
    "        if skipped_short_convs > 0: print(f\"  Skipped {skipped_short_convs} conversations with less than 2 utterances.\")\n",
    "        return inputs, targets\n",
    "    except ImportError: print(\"Error: Pandas is required.\"); raise\n",
    "    except KeyError as e: print(f\"Error: Missing expected column: {e}.\"); raise\n",
    "\n",
    "print(\"Extracting Training pairs...\")\n",
    "train_inputs, train_targets = extract_pairs_corrected('train', empathetic_dialogues, subset_size=DATASET_SUBSET_SIZE)\n",
    "print(\"\\nExtracting Validation pairs...\")\n",
    "val_inputs, val_targets = extract_pairs_corrected('validation', empathetic_dialogues, subset_size=None)\n",
    "\n",
    "\n",
    "def tokenize_words(text):\n",
    "    \"\"\"Simple word tokenizer: lowercase, basic punctuation, split.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s.,?!'-]\", \"\", text)\n",
    "    return text.split()\n",
    "\n",
    "def build_word_vocab(texts, min_freq):\n",
    "    \"\"\"Builds a word vocabulary from a list of texts.\"\"\"\n",
    "    print(\"Building word vocabulary...\")\n",
    "    counter = Counter()\n",
    "    for text in texts:\n",
    "        counter.update(tokenize_words(text))\n",
    "\n",
    "    vocab = {token: i for i, token in enumerate(SPECIAL_TOKENS)}\n",
    "    idx = len(SPECIAL_TOKENS)\n",
    "    word_count = 0\n",
    "    for word, count in counter.items():\n",
    "        if count >= min_freq:\n",
    "            vocab[word] = idx\n",
    "            idx += 1\n",
    "            word_count += 1\n",
    "\n",
    "    inv_vocab = {i: token for token, i in vocab.items()} \n",
    "    print(f\"Vocabulary size: {len(vocab)} (min_freq={min_freq}, including {len(SPECIAL_TOKENS)} special tokens)\")\n",
    "    print(f\"Found {word_count} words meeting min_freq={min_freq}\")\n",
    "    return vocab, inv_vocab\n",
    "\n",
    "all_train_texts_words = train_inputs + train_targets\n",
    "word_vocab, word_inv_vocab = build_word_vocab(all_train_texts_words, min_freq=MIN_FREQ)\n",
    "\n",
    "PAD_IDX = word_vocab[PAD_TOKEN]\n",
    "SOS_IDX = word_vocab[SOS_TOKEN]\n",
    "EOS_IDX = word_vocab[EOS_TOKEN]\n",
    "UNK_IDX = word_vocab[UNK_TOKEN]\n",
    "print(f\"Word Token IDs: PAD={PAD_IDX}, SOS={SOS_IDX}, EOS={EOS_IDX}, UNK={UNK_IDX}\")\n",
    "\n",
    "INPUT_DIM = len(word_vocab)\n",
    "OUTPUT_DIM = INPUT_DIM\n",
    "\n",
    "\n",
    "print(f\"Loading GloVe vectors from {GLOVE_PATH}...\")\n",
    "embeddings_index = {}\n",
    "loaded_vectors = 0\n",
    "try:\n",
    "    with open(GLOVE_PATH, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            if len(values[1:]) == EMBEDDING_DIM:\n",
    "                try:\n",
    "                    coefs = np.asarray(values[1:], dtype='float32')\n",
    "                    embeddings_index[word] = coefs\n",
    "                    loaded_vectors += 1\n",
    "                except ValueError:\n",
    "                    pass\n",
    "\n",
    "    print(f\"Loaded {loaded_vectors} word vectors.\")\n",
    "    if loaded_vectors == 0:\n",
    "        print(\"ERROR: No vectors loaded. Check GloVe path and dimension.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: GloVe file not found at {GLOVE_PATH}. Cannot load pre-trained embeddings.\")\n",
    "    embeddings_index = {} \n",
    "\n",
    "\n",
    "print(\"Creating embedding matrix...\")\n",
    "embedding_matrix = np.random.uniform(-0.05, 0.05, (INPUT_DIM, EMBEDDING_DIM))\n",
    "embedding_matrix = embedding_matrix.astype(np.float32) \n",
    "\n",
    "hits = 0\n",
    "misses = 0\n",
    "for word, i in word_vocab.items():\n",
    "    if i == PAD_IDX:\n",
    "        embedding_matrix[i] = np.zeros(EMBEDDING_DIM, dtype=np.float32)\n",
    "        hits += 1 \n",
    "        continue\n",
    "\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    elif word.lower() in embeddings_index:\n",
    "         embedding_matrix[i] = embeddings_index[word.lower()]\n",
    "         hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "\n",
    "print(f\"Initialized embedding matrix for {INPUT_DIM} words.\")\n",
    "print(f\"  Found pre-trained vectors for {hits} words in vocab.\")\n",
    "print(f\"  {misses} words in vocab not found in GloVe (using random init).\")\n",
    "\n",
    "embedding_matrix = torch.FloatTensor(embedding_matrix)\n",
    "\n",
    "del embeddings_index\n",
    "import gc\n",
    "gc.collect()\n",
    "print(\"Cleaned up GloVe index.\")\n",
    "\n",
    "\n",
    "def text_to_sequence_words(text, vocab):\n",
    "    tokens = tokenize_words(text)\n",
    "    return [SOS_IDX] + [vocab.get(token, UNK_IDX) for token in tokens] + [EOS_IDX]\n",
    "\n",
    "def sequence_to_text_words(sequence, inv_vocab):\n",
    "    tokens = [inv_vocab.get(idx, UNK_TOKEN) for idx in sequence if idx not in [PAD_IDX, SOS_IDX, EOS_IDX]]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def pad_sequence(seq, max_len, pad_idx):\n",
    "    seq = seq[:max_len]\n",
    "    padded = seq + [pad_idx] * (max_len - len(seq))\n",
    "    return padded\n",
    "\n",
    "def process_data_words(inputs, targets, vocab, max_len, pad_idx):\n",
    "    print(f\"Tokenizing (words) and padding sequences (MAX_LEN={max_len})...\")\n",
    "    processed_count = 0\n",
    "    total_pairs = len(inputs)\n",
    "    tokenized_inputs = []\n",
    "    tokenized_targets = []\n",
    "    for i in range(total_pairs):\n",
    "        tokenized_inputs.append(text_to_sequence_words(inputs[i], vocab))\n",
    "        tokenized_targets.append(text_to_sequence_words(targets[i], vocab))\n",
    "        processed_count += 1\n",
    "        if processed_count % 10000 == 0: print(f\"  Processed {processed_count}/{total_pairs} pairs...\")\n",
    "\n",
    "    print(\"Padding sequences...\")\n",
    "    padded_inputs = [pad_sequence(seq, max_len, pad_idx) for seq in tokenized_inputs]\n",
    "    padded_targets = [pad_sequence(seq, max_len, pad_idx) for seq in tokenized_targets]\n",
    "    print(\"Padding complete.\")\n",
    "    return padded_inputs, padded_targets\n",
    "\n",
    "padded_train_inputs, padded_train_targets = process_data_words(train_inputs, train_targets, word_vocab, MAX_LEN, PAD_IDX)\n",
    "padded_val_inputs, padded_val_targets = process_data_words(val_inputs, val_targets, word_vocab, MAX_LEN, PAD_IDX)\n",
    "\n",
    "class DialogueDataset(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        assert len(inputs) > 0, \"Input list empty!\"\n",
    "        assert len(inputs) == len(targets), \"Input/target length mismatch!\"\n",
    "        self.inputs = torch.tensor(inputs, dtype=torch.long)\n",
    "        self.targets = torch.tensor(targets, dtype=torch.long)\n",
    "        print(f\"DialogueDataset created with {len(self.inputs)} samples.\")\n",
    "    def __len__(self): return len(self.inputs)\n",
    "    def __getitem__(self, idx): return self.inputs[idx], self.targets[idx]\n",
    "\n",
    "try:\n",
    "    train_dataset = DialogueDataset(padded_train_inputs, padded_train_targets)\n",
    "    val_dataset = DialogueDataset(padded_val_inputs, padded_val_targets)\n",
    "except AssertionError as e: print(f\"Error creating dataset: {e}\"); raise e\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True if device=='cuda' else False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True if device=='cuda' else False)\n",
    "print(f\"Created Train DataLoader ({len(train_loader)} batches)\")\n",
    "print(f\"Created Validation DataLoader ({len(val_loader)} batches)\")\n",
    "\n",
    "try:\n",
    "    del padded_train_inputs, padded_train_targets, padded_val_inputs, padded_val_targets\n",
    "    del train_inputs, train_targets, val_inputs, val_targets, all_train_texts_words\n",
    "    gc.collect(); print(\"Cleaned up intermediate data lists.\")\n",
    "except NameError: print(\"Intermediate lists cleanup skipped or already done.\")\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Injects positional information into the input embeddings.\"\"\"\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-Head Attention mechanism.\"\"\"\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "        self.fc_q = nn.Linear(d_model, d_model)\n",
    "        self.fc_k = nn.Linear(d_model, d_model)\n",
    "        self.fc_v = nn.Linear(d_model, d_model)\n",
    "        self.fc_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"scale\", torch.sqrt(torch.FloatTensor([self.head_dim])))\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.shape[1]\n",
    "        Q = self.fc_q(query); K = self.fc_k(key); V = self.fc_v(value)\n",
    "        Q = Q.view(query.shape[0], batch_size, self.n_heads, self.head_dim).permute(1, 2, 0, 3)\n",
    "        K = K.view(key.shape[0], batch_size, self.n_heads, self.head_dim).permute(1, 2, 0, 3)\n",
    "        V = V.view(value.shape[0], batch_size, self.n_heads, self.head_dim).permute(1, 2, 0, 3)\n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale.to(Q.device)\n",
    "        if mask is not None: energy = energy.masked_fill(mask == 0, -1e10)\n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "        attention = self.dropout(attention)\n",
    "        x = torch.matmul(attention, V)\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        x = x.view(batch_size, query.shape[0], self.d_model)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.fc_o(x)\n",
    "        return x, attention\n",
    "\n",
    "class PositionwiseFeedforward(nn.Module):\n",
    "    \"\"\"Position-wise Feedforward Network.\"\"\"\n",
    "    def __init__(self, d_model, pf_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, pf_dim)\n",
    "        self.fc2 = nn.Linear(pf_dim, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"A single layer of the Transformer Encoder.\"\"\"\n",
    "    def __init__(self, d_model, n_heads, pf_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.ff = PositionwiseFeedforward(d_model, pf_dim, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, src, src_mask):\n",
    "        _src, _ = self.self_attn(src, src, src, src_mask)\n",
    "        src = self.norm1(src + self.dropout(_src))\n",
    "        _src = self.ff(src)\n",
    "        src = self.norm2(src + self.dropout(_src))\n",
    "        return src\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"A single layer of the Transformer Decoder.\"\"\"\n",
    "    def __init__(self, d_model, n_heads, pf_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.masked_self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.encoder_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.ff = PositionwiseFeedforward(d_model, pf_dim, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        _trg, _ = self.masked_self_attn(trg, trg, trg, trg_mask)\n",
    "        trg = self.norm1(trg + self.dropout(_trg))\n",
    "        _trg, attention = self.encoder_attn(trg, enc_src, enc_src, src_mask)\n",
    "        trg = self.norm2(trg + self.dropout(_trg))\n",
    "        _trg = self.ff(trg)\n",
    "        trg = self.norm3(trg + self.dropout(_trg))\n",
    "        return trg, attention\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, n_layers, n_heads, pf_dim, dropout,\n",
    "                 pretrained_embeddings, freeze_embeddings, max_len=MAX_LEN): \n",
    "        super().__init__()\n",
    "        self.tok_embedding = nn.Embedding(input_dim, d_model)\n",
    "        self.tok_embedding.weight.data.copy_(pretrained_embeddings)\n",
    "        if freeze_embeddings:\n",
    "            self.tok_embedding.weight.requires_grad = False\n",
    "            print(\"Encoder embeddings frozen.\")\n",
    "        else:\n",
    "            print(\"Encoder embeddings will be fine-tuned.\")\n",
    "        self.pos_embedding = PositionalEncoding(d_model, dropout, max_len=5000 if max_len < 5000 else max_len)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, pf_dim, dropout) for _ in range(n_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"scale\", torch.sqrt(torch.FloatTensor([d_model])))\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        scale = self.scale.to(src.device)\n",
    "        src = self.dropout((self.tok_embedding(src) * scale)) \n",
    "        src = src.permute(1, 0, 2)\n",
    "        src = self.pos_embedding(src)\n",
    "        for layer in self.layers: src = layer(src, src_mask)\n",
    "        return src\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, d_model, n_layers, n_heads, pf_dim, dropout,\n",
    "                 pretrained_embeddings, freeze_embeddings, max_len=MAX_LEN): \n",
    "        super().__init__()\n",
    "        self.tok_embedding = nn.Embedding(output_dim, d_model)\n",
    "        self.tok_embedding.weight.data.copy_(pretrained_embeddings)\n",
    "        if freeze_embeddings:\n",
    "            self.tok_embedding.weight.requires_grad = False\n",
    "            print(\"Decoder embeddings frozen.\")\n",
    "        else:\n",
    "            print(\"Decoder embeddings will be fine-tuned.\")\n",
    "        self.pos_embedding = PositionalEncoding(d_model, dropout, max_len=5000 if max_len < 5000 else max_len)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, pf_dim, dropout) for _ in range(n_layers)])\n",
    "        self.fc_out = nn.Linear(d_model, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"scale\", torch.sqrt(torch.FloatTensor([d_model])))\n",
    "\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        scale = self.scale.to(trg.device)\n",
    "        trg = self.dropout((self.tok_embedding(trg) * scale)) \n",
    "        trg = trg.permute(1, 0, 2)\n",
    "        trg = self.pos_embedding(trg)\n",
    "        for layer in self.layers: trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "        output = self.fc_out(trg)\n",
    "        return output, attention\n",
    "\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    \"\"\"The main Seq2Seq Transformer model.\"\"\"\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        return src_mask.to(self.device)\n",
    "    def make_trg_mask(self, trg):\n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device=self.device)).bool()\n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        return trg_mask.to(self.device)\n",
    "    def forward(self, src, trg):\n",
    "        src = src.to(self.device); trg = trg.to(self.device)\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        return output, attention\n",
    "\n",
    "\n",
    "print(\"Initializing model components with GloVe embeddings...\")\n",
    "enc = Encoder(INPUT_DIM, D_MODEL, ENC_LAYERS, N_HEADS, PF_DIM, DROPOUT,\n",
    "              embedding_matrix, FREEZE_EMBEDDINGS, MAX_LEN).to(device)\n",
    "dec = Decoder(OUTPUT_DIM, D_MODEL, DEC_LAYERS, N_HEADS, PF_DIM, DROPOUT,\n",
    "              embedding_matrix, FREEZE_EMBEDDINGS, MAX_LEN).to(device)\n",
    "\n",
    "model = Seq2SeqTransformer(enc, dec, PAD_IDX, PAD_IDX, device).to(device)\n",
    "\n",
    "def initialize_non_embedding_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1 and not isinstance(m, nn.Embedding):\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "    if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "print(\"Applying Xavier uniform initialization to non-embedding layers...\")\n",
    "model.apply(initialize_non_embedding_weights)\n",
    "\n",
    "params_to_optimize = model.parameters() if not FREEZE_EMBEDDINGS else filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = optim.AdamW(params_to_optimize, lr=LEARNING_RATE) \n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX) \n",
    "\n",
    "\n",
    "def count_parameters(model): return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters.')\n",
    "print(f\"Model is on device: {next(model.parameters()).device}\")\n",
    "\n",
    "del embedding_matrix\n",
    "gc.collect()\n",
    "print(\"Cleaned up embedding matrix.\")\n",
    "\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \"\"\"Performs one epoch of training.\"\"\"\n",
    "    model.train(); epoch_loss = 0; num_batches = len(iterator)\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src, trg = batch\n",
    "        src, trg = src.to(device, non_blocking=True), trg.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad()\n",
    "        trg_input = trg[:, :-1]; trg_output_expected = trg[:, 1:].contiguous()\n",
    "        output, _ = model(src, trg_input)\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output.permute(1, 0, 2).contiguous().view(-1, output_dim)\n",
    "        trg_output_expected = trg_output_expected.view(-1)\n",
    "        loss = criterion(output, trg_output_expected)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / num_batches\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \"\"\"Performs one epoch of evaluation.\"\"\"\n",
    "    model.eval(); epoch_loss = 0; num_batches = len(iterator)\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src, trg = batch\n",
    "            src, trg = src.to(device, non_blocking=True), trg.to(device, non_blocking=True)\n",
    "            trg_input = trg[:, :-1]; trg_output_expected = trg[:, 1:].contiguous()\n",
    "            output, _ = model(src, trg_input)\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output.permute(1, 0, 2).contiguous().view(-1, output_dim)\n",
    "            trg_output_expected = trg_output_expected.view(-1)\n",
    "            loss = criterion(output, trg_output_expected)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / num_batches\n",
    "\n",
    "\n",
    "print(\"Starting Training...\")\n",
    "start_training_time = time.time()\n",
    "best_valid_loss = float('inf')\n",
    "model_save_path_best = 'glove-transformer-basic-best.pt' \n",
    "epochs_no_improve = 0\n",
    "train_losses = []; valid_losses = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    epoch_start_time = time.time()\n",
    "    print(f\"--- Epoch {epoch+1}/{N_EPOCHS} ---\")\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, val_loader, criterion)\n",
    "    train_losses.append(train_loss); valid_losses.append(valid_loss)\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_mins = int((epoch_end_time - epoch_start_time) / 60)\n",
    "    epoch_secs = int((epoch_end_time - epoch_start_time) % 60)\n",
    "    train_ppl = math.exp(train_loss) if train_loss < 100 else float('inf')\n",
    "    valid_ppl = math.exp(valid_loss) if valid_loss < 100 else float('inf')\n",
    "    print(f'Epoch {epoch+1} Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {train_ppl:7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {valid_ppl:7.3f}')\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), model_save_path_best)\n",
    "        epochs_no_improve = 0\n",
    "        print(f\"*** Epoch {epoch+1}: Validation loss improved. Saving best model to {model_save_path_best} ***\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"Epoch {epoch+1}: Validation loss did not improve. ({epochs_no_improve}/{PATIENCE}) Best: {best_valid_loss:.3f}\")\n",
    "    if epochs_no_improve >= PATIENCE:\n",
    "        print(f\"\\nEarly stopping triggered after {PATIENCE} epochs without validation loss improvement.\")\n",
    "        break\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "total_training_time = time.time() - start_training_time\n",
    "print(f\"\\nTraining Finished. Total time: {total_training_time // 60:.0f}m {total_training_time % 60:.0f}s\")\n",
    "print(f\"Best validation loss achieved: {best_valid_loss:.3f}\")\n",
    "print(f\"Best model saved to: {model_save_path_best}\")\n",
    "\n",
    "def generate_response_sampling_words( \n",
    "    sentence,\n",
    "    model,\n",
    "    vocab,       \n",
    "    inv_vocab,   \n",
    "    device,\n",
    "    max_len=MAX_LEN,\n",
    "    strategy=\"topk\",\n",
    "    k=10,\n",
    "    p=0.9,\n",
    "    temperature=0.8\n",
    "    ):\n",
    "    assert strategy in ['greedy', 'topk', 'topp']\n",
    "    assert temperature > 0\n",
    "    model.eval()\n",
    "\n",
    "    tokens = text_to_sequence_words(sentence, vocab) \n",
    "    if len(tokens) > max_len: tokens = tokens[:max_len]\n",
    "    src_tensor = torch.LongTensor(tokens).unsqueeze(0).to(device)\n",
    "    src_mask = model.make_src_mask(src_tensor)\n",
    "\n",
    "    with torch.no_grad(): enc_src = model.encoder(src_tensor, src_mask)\n",
    "\n",
    "    trg_indices = [SOS_IDX]\n",
    "    for i in range(max_len - 1):\n",
    "        trg_tensor = torch.LongTensor(trg_indices).unsqueeze(0).to(device)\n",
    "        trg_mask = model.make_trg_mask(trg_tensor)\n",
    "        with torch.no_grad(): output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
    "        pred_token_logits = output[-1, 0, :]\n",
    "        pred_token_logits = pred_token_logits / temperature\n",
    "        pred_token_probs = F.softmax(pred_token_logits, dim=-1)\n",
    "\n",
    "        next_token_id = -1\n",
    "        if strategy == 'greedy': next_token_id = torch.argmax(pred_token_probs).item()\n",
    "        elif strategy == 'topk':\n",
    "            topk_probs, topk_indices = torch.topk(pred_token_probs, k=min(k, pred_token_probs.size(-1)))\n",
    "            mask = torch.zeros_like(pred_token_probs); mask.scatter_(0, topk_indices, 1.0)\n",
    "            filtered_probs = pred_token_probs * mask\n",
    "            sum_filtered_probs = torch.sum(filtered_probs)\n",
    "            if sum_filtered_probs > 1e-9:\n",
    "                 filtered_probs = filtered_probs / sum_filtered_probs\n",
    "                 next_token_id = torch.multinomial(filtered_probs, num_samples=1).item()\n",
    "            else: print(\"Warning: Top-k zero sum.\"); next_token_id = EOS_IDX\n",
    "        elif strategy == 'topp':\n",
    "            sorted_probs, sorted_indices = torch.sort(pred_token_probs, descending=True)\n",
    "            cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "            sorted_indices_to_remove = cumulative_probs > p\n",
    "            sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone(); sorted_indices_to_remove[0] = 0\n",
    "            indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "            pred_token_probs[indices_to_remove] = 0.0\n",
    "            sum_filtered_probs = torch.sum(pred_token_probs)\n",
    "            if sum_filtered_probs > 1e-9:\n",
    "                 filtered_probs = pred_token_probs / sum_filtered_probs\n",
    "                 next_token_id = torch.multinomial(filtered_probs, num_samples=1).item()\n",
    "            else: print(\"Warning: Top-p zero sum.\"); next_token_id = sorted_indices[0].item()\n",
    "\n",
    "        trg_indices.append(next_token_id)\n",
    "        if next_token_id == EOS_IDX: break\n",
    "\n",
    "    trg_tokens_text = sequence_to_text_words(trg_indices, inv_vocab) \n",
    "    return trg_tokens_text\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "import nltk \n",
    "\n",
    "\n",
    "def calculate_bleu(\n",
    "    input_sentences, \n",
    "    target_sentences, \n",
    "    model,\n",
    "    tokenizer,\n",
    "    device,\n",
    "    max_len=MAX_LEN,\n",
    "    strategy='greedy',\n",
    "    k=10,\n",
    "    p=0.9,\n",
    "    temperature=1.0,\n",
    "    max_examples=None):\n",
    "    \"\"\"\n",
    "    Calculates BLEU score over a dataset provided as separate input and target lists.\n",
    "\n",
    "    Args:\n",
    "        input_sentences (list): List of source sentences (strings).\n",
    "        target_sentences (list): List of corresponding target/reference sentences (strings).\n",
    "        model: The trained transformer model.\n",
    "        tokenizer: The tokenizer instance used by the generation function.\n",
    "        device: The device ('cuda' or 'cpu').\n",
    "        max_len: Maximum sequence length for generation.\n",
    "        strategy: Sampling strategy ('greedy', 'topk', 'topp').\n",
    "        k: k for top-k sampling.\n",
    "        p: p for top-p sampling.\n",
    "        temperature: Temperature for sampling.\n",
    "        max_examples (int, optional): Limit the number of examples to evaluate. Defaults to None (evaluate all).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (corpus_bleu_score, list_of_details)\n",
    "               corpus_bleu_score (float): The corpus BLEU-4 score (0.0-1.0).\n",
    "               list_of_details (list): List of tuples [(source, reference, candidate, sentence_bleu), ...]\n",
    "    \"\"\"\n",
    "    assert len(input_sentences) == len(target_sentences), \\\n",
    "        f\"Input and target sentence lists must have the same length ({len(input_sentences)} != {len(target_sentences)})\"\n",
    "\n",
    "    targets = []     \n",
    "    predictions = [] \n",
    "    details = []      \n",
    "\n",
    "    model.eval() \n",
    "\n",
    "    eval_data_pairs = list(zip(input_sentences, target_sentences))\n",
    "\n",
    "    if max_examples is not None and len(eval_data_pairs) > max_examples:\n",
    "        print(f\"Evaluating on a random sample of {max_examples} examples.\")\n",
    "        eval_data_pairs = random.sample(eval_data_pairs, max_examples)\n",
    "    elif max_examples is None:\n",
    "         print(f\"Evaluating on all {len(eval_data_pairs)} examples.\")\n",
    "    else: # max_examples >= len(data)\n",
    "         print(f\"Evaluating on all {len(eval_data_pairs)} examples (max_examples={max_examples}).\")\n",
    "\n",
    "\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "        tokenize_for_bleu = word_tokenize\n",
    "    except LookupError:\n",
    "        print(\"NLTK 'punkt' tokenizer model not found. Downloading...\")\n",
    "        nltk.download('punkt')\n",
    "        tokenize_for_bleu = word_tokenize\n",
    "    except Exception as e:\n",
    "        print(f\"Could not initialize nltk.word_tokenize ({e}). Falling back to simple split().\")\n",
    "        tokenize_for_bleu = lambda s: s.split()\n",
    "\n",
    "\n",
    "    print(f\"Generating responses using strategy='{strategy}', temp={temperature}\" +\n",
    "          (f\", k={k}\" if strategy=='topk' else \"\") +\n",
    "          (f\", p={p}\" if strategy=='topp' else \"\"))\n",
    "\n",
    "    for source_text, target_text in tqdm(eval_data_pairs, desc=\"Calculating BLEU\"):\n",
    "\n",
    "        candidate_text = generate_response_sampling(\n",
    "            sentence=source_text,\n",
    "            model=model,\n",
    "            tokenizer=tokenizer, \n",
    "            device=device,\n",
    "            max_len=max_len,\n",
    "            strategy=strategy,\n",
    "            k=k,\n",
    "            p=p,\n",
    "            temperature=temperature\n",
    "        )\n",
    "\n",
    "        if candidate_text is None:\n",
    "            candidate_text = \"\" \n",
    "\n",
    "        try:\n",
    "            target_text_str = str(target_text) if target_text is not None else \"\"\n",
    "            tokenized_target = [tokenize_for_bleu(target_text_str.lower())] \n",
    "        except Exception as e:\n",
    "             print(f\"Warning: Error tokenizing target text '{target_text}': {e}. Using empty reference.\")\n",
    "             tokenized_target = [[]]\n",
    "             target_text_str = \"\" \n",
    "\n",
    "        try:\n",
    "            candidate_text_str = str(candidate_text) if candidate_text is not None else \"\"\n",
    "            tokenized_candidate = tokenize_for_bleu(candidate_text_str.lower())\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error tokenizing candidate text '{candidate_text}': {e}. Using empty candidate.\")\n",
    "            tokenized_candidate = []\n",
    "            candidate_text_str = \"\" \n",
    "\n",
    "\n",
    "        sent_bleu = 0.0\n",
    "        if tokenized_candidate and tokenized_target and tokenized_target[0]:\n",
    "             try:\n",
    "                 sent_bleu = sentence_bleu(tokenized_target, tokenized_candidate,\n",
    "                                           smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method1)\n",
    "             except Exception as e:\n",
    "                 print(f\"Error calculating sentence BLEU for cand='{candidate_text_str}', ref='{target_text_str}': {e}\")\n",
    "                 sent_bleu = 0.0\n",
    "\n",
    "        targets.append(tokenized_target)\n",
    "        predictions.append(tokenized_candidate)\n",
    "\n",
    "        details.append((source_text, target_text_str, candidate_text_str, sent_bleu))\n",
    "\n",
    "    try:\n",
    "        smoothing = nltk.translate.bleu_score.SmoothingFunction().method1\n",
    "        corpus_bleu_score = corpus_bleu(targets, predictions, smoothing_function=smoothing)\n",
    "    except ZeroDivisionError:\n",
    "        print(\"Warning: ZeroDivisionError during corpus BLEU calculation. Check if predictions or references are systematically empty.\")\n",
    "        corpus_bleu_score = 0.0\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating corpus BLEU: {e}\")\n",
    "        corpus_bleu_score = 0.0\n",
    "\n",
    "\n",
    "    return corpus_bleu_score, details\n",
    "\n",
    "# print(\"Extracting Training pairs...\")\n",
    "# train_inputs, train_targets = extract_pairs_corrected('train', empathetic_dialogues, subset_size=DATASET_SUBSET_SIZE)\n",
    "\n",
    "# print(\"\\nExtracting Validation pairs...\")\n",
    "# val_inputs, val_targets = extract_pairs_corrected('validation', empathetic_dialogues, subset_size=None) \n",
    "\n",
    "# print(\"\\nCalculating BLEU score on the VALIDATION data...\")\n",
    "# model.to(device)\n",
    "# val_bleu_greedy, val_details_greedy = calculate_bleu(\n",
    "#     input_sentences=val_inputs,   \n",
    "#     target_sentences=val_targets, \n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     device=device,\n",
    "#     max_len=MAX_LEN,\n",
    "#     strategy='greedy',\n",
    "#     temperature=1.0,\n",
    "#     max_examples=100 \n",
    "# )\n",
    "# print(f\"\\nValidation Corpus BLEU Score (Greedy): {val_bleu_greedy * 100:.2f}\")\n",
    "\n",
    "# val_bleu_topp, val_details_topp = calculate_bleu(\n",
    "#     input_sentences=val_inputs[:100],   \n",
    "#     target_sentences=val_targets[:100], \n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     device=device,\n",
    "#     max_len=MAX_LEN,\n",
    "#     strategy='topp',\n",
    "#     p=0.9,\n",
    "#     temperature=0.7,\n",
    "#     max_examples=100 \n",
    "# )\n",
    "# print(f\"Validation Corpus BLEU Score (Top-p, p=0.9, T=0.7): {val_bleu_topp * 100:.2f}\")\n",
    "\n",
    "# print(\"\\nCalculating BLEU score on the TRAINING data...\")\n",
    "# train_bleu_greedy, train_details_greedy = calculate_bleu(\n",
    "#     input_sentences=train_inputs[:100],   \n",
    "#     target_sentences=train_targets[:100], \n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     device=device,\n",
    "#     max_len=MAX_LEN,\n",
    "#     strategy='greedy',\n",
    "#     temperature=1.0,\n",
    "#     max_examples=100 \n",
    "# )\n",
    "# print(f\"\\nTraining Corpus BLEU Score (Greedy, sample): {train_bleu_greedy * 100:.2f}\")\n",
    "\n",
    "\n",
    "# print(\"\\nExample Validation Generations (Top-p):\")\n",
    "# for i, (src, ref, cand, s_bleu) in enumerate(val_details_topp[:5]): \n",
    "#     print(f\"--- Example {i+1} ---\")\n",
    "#     print(f\"  Source:    {src}\")\n",
    "#     print(f\"  Reference: {ref}\")\n",
    "#     print(f\"  Candidate: {cand}\")\n",
    "#     print(f\"  Sentence BLEU: {s_bleu*100:.2f}\")\n",
    "test_prompts = [\n",
    "    \"I just got a promotion at work! I'm so happy.\",\n",
    "    \"I feel really lonely these days. No one seems to understand me.\",\n",
    "    \"My best friend just moved to another country, and I miss them so much.\",\n",
    "    \"I failed my exam even after studying so hard. I feel so disappointed.\",\n",
    "    \"I helped a stranger today, and it made me feel really good.\",\n",
    "    \"I'm really nervous about my job interview tomorrow.\",\n",
    "]\n",
    "\n",
    "print(\"\\n--- Chatbot Test (GloVe Embeddings, Best Model) ---\")\n",
    "\n",
    "if 'model' in locals() or 'model' in globals():\n",
    "    if os.path.exists(model_save_path_best):\n",
    "        print(f\"Loading best model state from {model_save_path_best}...\")\n",
    "        try:\n",
    "            model.load_state_dict(torch.load(model_save_path_best, map_location=device, weights_only=True))\n",
    "            print(\"Model loaded successfully (weights_only=True).\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load with weights_only=True ({e}). Trying default...\")\n",
    "            try:\n",
    "                 model.load_state_dict(torch.load(model_save_path_best, map_location=device))\n",
    "                 print(\"Model loaded successfully (default).\")\n",
    "            except Exception as e_fallback: print(f\"ERROR loading model: {e_fallback}\"); raise e_fallback\n",
    "    else:\n",
    "         print(f\"Warning: Best model file '{model_save_path_best}' not found. Using current model state.\")\n",
    "    model.to(device) # Ensure model is on correct device\n",
    "\n",
    "    # --- Run Tests ---\n",
    "    if 'word_vocab' in locals() and 'word_inv_vocab' in locals():\n",
    "        print(\"\\n--- Testing with Top-k (k=10, temp=0.8) ---\")\n",
    "        for prompt in test_prompts:\n",
    "            response = generate_response_sampling_words( \n",
    "                prompt, model, word_vocab, word_inv_vocab, device, max_len=MAX_LEN,\n",
    "                strategy=\"topk\", k=10, temperature=0.8\n",
    "            )\n",
    "            print(f\"User: {prompt}\")\n",
    "            print(f\"Bot:  {response}\")\n",
    "            print(\"-\" * 20)\n",
    "\n",
    "        print(\"\\n--- Testing with Top-p (p=0.9, temp=0.8) ---\")\n",
    "        for prompt in test_prompts:\n",
    "            response = generate_response_sampling_words( \n",
    "                prompt, model, word_vocab, word_inv_vocab, device, max_len=MAX_LEN,\n",
    "                strategy=\"topp\", p=0.9, temperature=0.8\n",
    "            )\n",
    "            print(f\"User: {prompt}\")\n",
    "            print(f\"Bot:  {response}\")\n",
    "            print(\"-\" * 20)\n",
    "    else:\n",
    "        print(\"ERROR: Word vocabulary not found. Cannot run inference.\")\n",
    "else:\n",
    "    print(\"ERROR: Model not defined. Cannot run inference.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5563369c",
   "metadata": {
    "papermill": {
     "duration": 0.025843,
     "end_time": "2025-04-22T14:36:28.458080",
     "exception": false,
     "start_time": "2025-04-22T14:36:28.432237",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Using Fastext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27496e8",
   "metadata": {
    "papermill": {
     "duration": 0.025594,
     "end_time": "2025-04-22T14:36:28.510435",
     "exception": false,
     "start_time": "2025-04-22T14:36:28.484841",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Basic Mental Health Chatbot with Transformer (From Scratch) - FastText Embeddings\n",
    "\n",
    "**Improvements:**\n",
    "*   Uses pre-trained FastText word embeddings (good OOV handling).\n",
    "*   Uses a word-level tokenizer.\n",
    "*   Includes validation loop and early stopping.\n",
    "*   Saves the best model based on validation performance.\n",
    "*   Sampling strategies (Top-k, Top-p) for inference.\n",
    "\n",
    "**Disclaimer:** Educational example, NOT for real mental health support. Requires FastText file download and upload to Kaggle Datasets. Training time depends on data size and compute resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db322d75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T14:36:28.565034Z",
     "iopub.status.busy": "2025-04-22T14:36:28.564353Z",
     "iopub.status.idle": "2025-04-22T14:36:31.726480Z",
     "shell.execute_reply": "2025-04-22T14:36:31.725611Z"
    },
    "papermill": {
     "duration": 3.191667,
     "end_time": "2025-04-22T14:36:31.727830",
     "exception": false,
     "start_time": "2025-04-22T14:36:28.536163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.5.1+cu124\n",
      "Core libraries loaded.\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets scikit-learn pandas -q\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau \n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import numpy as np \n",
    "import gc \n",
    "\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "try:\n",
    "    import datasets, sklearn, pandas\n",
    "    print(\"Core libraries loaded.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing libraries: {e}. Please ensure installs were successful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0c79ae1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T14:36:31.784090Z",
     "iopub.status.busy": "2025-04-22T14:36:31.783800Z",
     "iopub.status.idle": "2025-04-22T14:36:31.792943Z",
     "shell.execute_reply": "2025-04-22T14:36:31.792327Z"
    },
    "papermill": {
     "duration": 0.038249,
     "end_time": "2025-04-22T14:36:31.793934",
     "exception": false,
     "start_time": "2025-04-22T14:36:31.755685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DATASET_SUBSET_SIZE = None \n",
    "MAX_LEN = 60             \n",
    "MIN_FREQ = 3             \n",
    "\n",
    "FASTTEXT_PATH = '/kaggle/input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "FREEZE_EMBEDDINGS = False \n",
    "\n",
    "BATCH_SIZE = 64          \n",
    "LEARNING_RATE = 0.0005\n",
    "N_EPOCHS = 50            \n",
    "CLIP = 1.0\n",
    "PATIENCE = 10            \n",
    "\n",
    "D_MODEL = EMBEDDING_DIM\n",
    "N_HEADS = 10\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "PF_DIM = 512             \n",
    "DROPOUT = 0.3         \n",
    "\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED) \n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define Special Tokens\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "SOS_TOKEN = \"<SOS>\"\n",
    "EOS_TOKEN = \"<EOS>\"\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "SPECIAL_TOKENS = [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN, UNK_TOKEN]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd5f672",
   "metadata": {
    "papermill": {
     "duration": 0.027059,
     "end_time": "2025-04-22T14:36:31.851357",
     "exception": false,
     "start_time": "2025-04-22T14:36:31.824298",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Load EmpatheticDialogues Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "134dc413",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T14:36:31.905537Z",
     "iopub.status.busy": "2025-04-22T14:36:31.905323Z",
     "iopub.status.idle": "2025-04-22T14:36:32.615115Z",
     "shell.execute_reply": "2025-04-22T14:36:32.614453Z"
    },
    "papermill": {
     "duration": 0.737857,
     "end_time": "2025-04-22T14:36:32.616377",
     "exception": false,
     "start_time": "2025-04-22T14:36:31.878520",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading EmpatheticDialogues dataset...\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['conv_id', 'utterance_idx', 'context', 'prompt', 'speaker_idx', 'utterance', 'selfeval', 'tags'],\n",
      "        num_rows: 76673\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['conv_id', 'utterance_idx', 'context', 'prompt', 'speaker_idx', 'utterance', 'selfeval', 'tags'],\n",
      "        num_rows: 12030\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['conv_id', 'utterance_idx', 'context', 'prompt', 'speaker_idx', 'utterance', 'selfeval', 'tags'],\n",
      "        num_rows: 10943\n",
      "    })\n",
      "})\n",
      "Dataset loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading EmpatheticDialogues dataset...\")\n",
    "try:\n",
    "    empathetic_dialogues = load_dataset(\"empathetic_dialogues\", trust_remote_code=True)\n",
    "    print(empathetic_dialogues)\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb528e09",
   "metadata": {
    "papermill": {
     "duration": 0.026657,
     "end_time": "2025-04-22T14:36:32.670155",
     "exception": false,
     "start_time": "2025-04-22T14:36:32.643498",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Extract Conversation Pairs\n",
    "Group utterances by conversation ID and create input/target pairs from adjacent turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0d3bac82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T14:36:32.728091Z",
     "iopub.status.busy": "2025-04-22T14:36:32.727217Z",
     "iopub.status.idle": "2025-04-22T14:36:32.735804Z",
     "shell.execute_reply": "2025-04-22T14:36:32.735107Z"
    },
    "papermill": {
     "duration": 0.036996,
     "end_time": "2025-04-22T14:36:32.736860",
     "exception": false,
     "start_time": "2025-04-22T14:36:32.699864",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_pairs_corrected(split_name, dataset, subset_size=None):\n",
    "    \"\"\"Extracts adjacent utterance pairs from a given split of the dataset.\"\"\"\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    data_split = dataset[split_name]\n",
    "    print(f\"Processing {split_name} split...\")\n",
    "    try:\n",
    "        df = data_split.to_pandas()\n",
    "        print(f\"Converted {split_name} split to Pandas DataFrame with {len(df)} rows.\")\n",
    "\n",
    "        all_conv_ids = df['conv_id'].unique()\n",
    "        if subset_size and subset_size < len(all_conv_ids):\n",
    "            print(f\"Selecting a subset of {subset_size} conversation IDs for {split_name} split.\")\n",
    "            selected_conv_ids = random.sample(list(all_conv_ids), subset_size)\n",
    "            df = df[df['conv_id'].isin(selected_conv_ids)]\n",
    "            print(f\"Subset DataFrame has {len(df)} rows after filtering by conv_id.\")\n",
    "\n",
    "        grouped = df.sort_values('utterance_idx').groupby('conv_id')\n",
    "        print(f\"Grouped into {len(grouped)} conversations.\")\n",
    "\n",
    "        processed_convs = 0\n",
    "        skipped_short_convs = 0\n",
    "        for conv_id, group in grouped:\n",
    "            utterances = group['utterance'].tolist()\n",
    "            utterances = [u.strip() for u in utterances if u.strip()] \n",
    "\n",
    "            if len(utterances) >= 2:\n",
    "                for i in range(len(utterances) - 1):\n",
    "                    inputs.append(utterances[i])\n",
    "                    targets.append(utterances[i+1])\n",
    "            else:\n",
    "                skipped_short_convs += 1\n",
    "\n",
    "            processed_convs += 1\n",
    "            if processed_convs % 10000 == 0:\n",
    "                 print(f\"  Processed {processed_convs}/{len(grouped)} conversations...\")\n",
    "\n",
    "        print(f\"Finished processing {split_name}.\")\n",
    "        print(f\"  Found {len(inputs)} input/target pairs.\")\n",
    "        if skipped_short_convs > 0:\n",
    "            print(f\"  Skipped {skipped_short_convs} conversations with less than 2 utterances.\")\n",
    "        return inputs, targets\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"Error: Pandas is required for data extraction. Please install it.\")\n",
    "        raise\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Missing expected column in dataset: {e}. Check dataset structure.\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cd35dda5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T14:36:32.790107Z",
     "iopub.status.busy": "2025-04-22T14:36:32.789855Z",
     "iopub.status.idle": "2025-04-22T14:36:33.747843Z",
     "shell.execute_reply": "2025-04-22T14:36:33.747104Z"
    },
    "papermill": {
     "duration": 0.986228,
     "end_time": "2025-04-22T14:36:33.749228",
     "exception": false,
     "start_time": "2025-04-22T14:36:32.763000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Training pairs...\n",
      "Processing train split...\n",
      "Converted train split to Pandas DataFrame with 76673 rows.\n",
      "Grouped into 17844 conversations.\n",
      "  Processed 10000/17844 conversations...\n",
      "Finished processing train.\n",
      "  Found 58829 input/target pairs.\n",
      "  Skipped 64 conversations with less than 2 utterances.\n",
      "\n",
      "Extracting Validation pairs...\n",
      "Processing validation split...\n",
      "Converted validation split to Pandas DataFrame with 12030 rows.\n",
      "Grouped into 2763 conversations.\n",
      "Finished processing validation.\n",
      "  Found 9267 input/target pairs.\n",
      "  Skipped 5 conversations with less than 2 utterances.\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting Training pairs...\")\n",
    "train_inputs, train_targets = extract_pairs_corrected('train', empathetic_dialogues, subset_size=DATASET_SUBSET_SIZE)\n",
    "\n",
    "print(\"\\nExtracting Validation pairs...\")\n",
    "val_inputs, val_targets = extract_pairs_corrected('validation', empathetic_dialogues, subset_size=None) # Use full validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547d84e7",
   "metadata": {
    "papermill": {
     "duration": 0.029232,
     "end_time": "2025-04-22T14:36:33.817744",
     "exception": false,
     "start_time": "2025-04-22T14:36:33.788512",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Word Tokenizer and Vocabulary Building\n",
    "Using a simple word-level tokenizer and building vocabulary based on word frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a855cad2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T14:36:33.872106Z",
     "iopub.status.busy": "2025-04-22T14:36:33.871389Z",
     "iopub.status.idle": "2025-04-22T14:36:33.877654Z",
     "shell.execute_reply": "2025-04-22T14:36:33.877133Z"
    },
    "papermill": {
     "duration": 0.034784,
     "end_time": "2025-04-22T14:36:33.878725",
     "exception": false,
     "start_time": "2025-04-22T14:36:33.843941",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_words(text):\n",
    "    \"\"\"Simple word tokenizer: lowercase, basic punctuation, split.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s.,?!'-]\", \"\", text)\n",
    "    tokens = text.split()\n",
    "    return [token for token in tokens if token] \n",
    "\n",
    "def build_word_vocab(texts, min_freq):\n",
    "    \"\"\"Builds a word vocabulary from a list of texts.\"\"\"\n",
    "    print(\"Building word vocabulary...\")\n",
    "    counter = Counter()\n",
    "    processed_texts = 0\n",
    "    for text in texts:\n",
    "        counter.update(tokenize_words(text))\n",
    "        processed_texts += 1\n",
    "        if processed_texts % 10000 == 0:\n",
    "            print(f\"  Processed {processed_texts}/{len(texts)} texts for vocab counting...\")\n",
    "\n",
    "\n",
    "    vocab = {token: i for i, token in enumerate(SPECIAL_TOKENS)}\n",
    "    idx = len(SPECIAL_TOKENS)\n",
    "    word_count = 0\n",
    "    for word, count in sorted(counter.items()):\n",
    "        if count >= min_freq:\n",
    "            if word not in vocab: \n",
    "                vocab[word] = idx\n",
    "                idx += 1\n",
    "                word_count += 1\n",
    "\n",
    "    inv_vocab = {i: token for token, i in vocab.items()} \n",
    "    print(f\"Vocabulary size: {len(vocab)} (min_freq={min_freq}, including {len(SPECIAL_TOKENS)} special tokens)\")\n",
    "    print(f\"Found {word_count} words meeting min_freq={min_freq}\")\n",
    "    return vocab, inv_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7b826f52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T14:36:33.945424Z",
     "iopub.status.busy": "2025-04-22T14:36:33.944498Z",
     "iopub.status.idle": "2025-04-22T14:36:34.531498Z",
     "shell.execute_reply": "2025-04-22T14:36:34.530783Z"
    },
    "papermill": {
     "duration": 0.625361,
     "end_time": "2025-04-22T14:36:34.532594",
     "exception": false,
     "start_time": "2025-04-22T14:36:33.907233",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building word vocabulary...\n",
      "  Processed 10000/117658 texts for vocab counting...\n",
      "  Processed 20000/117658 texts for vocab counting...\n",
      "  Processed 30000/117658 texts for vocab counting...\n",
      "  Processed 40000/117658 texts for vocab counting...\n",
      "  Processed 50000/117658 texts for vocab counting...\n",
      "  Processed 60000/117658 texts for vocab counting...\n",
      "  Processed 70000/117658 texts for vocab counting...\n",
      "  Processed 80000/117658 texts for vocab counting...\n",
      "  Processed 90000/117658 texts for vocab counting...\n",
      "  Processed 100000/117658 texts for vocab counting...\n",
      "  Processed 110000/117658 texts for vocab counting...\n",
      "Vocabulary size: 16041 (min_freq=3, including 4 special tokens)\n",
      "Found 16037 words meeting min_freq=3\n",
      "Word Token IDs: PAD=0, SOS=1, EOS=2, UNK=3\n"
     ]
    }
   ],
   "source": [
    "all_train_texts_words = train_inputs + train_targets\n",
    "word_vocab, word_inv_vocab = build_word_vocab(all_train_texts_words, min_freq=MIN_FREQ)\n",
    "\n",
    "PAD_IDX = word_vocab.get(PAD_TOKEN) \n",
    "SOS_IDX = word_vocab.get(SOS_TOKEN)\n",
    "EOS_IDX = word_vocab.get(EOS_TOKEN)\n",
    "UNK_IDX = word_vocab.get(UNK_TOKEN)\n",
    "\n",
    "if None in [PAD_IDX, SOS_IDX, EOS_IDX, UNK_IDX]:\n",
    "    raise ValueError(\"Error: One or more special tokens not found in vocabulary!\")\n",
    "else:\n",
    "    print(f\"Word Token IDs: PAD={PAD_IDX}, SOS={SOS_IDX}, EOS={EOS_IDX}, UNK={UNK_IDX}\")\n",
    "\n",
    "INPUT_DIM = len(word_vocab)\n",
    "OUTPUT_DIM = INPUT_DIM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65c8539",
   "metadata": {
    "papermill": {
     "duration": 0.028147,
     "end_time": "2025-04-22T14:36:34.588104",
     "exception": false,
     "start_time": "2025-04-22T14:36:34.559957",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. Load Pre-trained FastText Embeddings\n",
    "Load word vectors from the specified FastText file. Ensure the path and dimension match your uploaded file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cfef11a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T14:36:34.641265Z",
     "iopub.status.busy": "2025-04-22T14:36:34.640977Z",
     "iopub.status.idle": "2025-04-22T14:39:16.701821Z",
     "shell.execute_reply": "2025-04-22T14:39:16.701086Z"
    },
    "papermill": {
     "duration": 162.116828,
     "end_time": "2025-04-22T14:39:16.731080",
     "exception": false,
     "start_time": "2025-04-22T14:36:34.614252",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FastText vectors from /kaggle/input/fasttext-crawl-300d-2m/crawl-300d-2M.vec...\n",
      "Skipped header line: 2000000 300\n",
      "\n",
      "Loaded 2000000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading FastText vectors from {FASTTEXT_PATH}...\")\n",
    "embeddings_index = {}\n",
    "loaded_vectors = 0\n",
    "skipped_lines = 0\n",
    "lines_to_skip = 1 \n",
    "\n",
    "try:\n",
    "    with open(FASTTEXT_PATH, 'r', encoding='utf-8') as f:\n",
    "        for i in range(lines_to_skip):\n",
    "            try:\n",
    "                header = next(f)\n",
    "                print(f\"Skipped header line: {header.strip()}\")\n",
    "            except StopIteration:\n",
    "                print(\"Warning: File seems empty or has no header to skip.\")\n",
    "                break\n",
    "\n",
    "        ln = lines_to_skip\n",
    "        for line in f:\n",
    "            ln += 1\n",
    "            values = line.rstrip().split(' ') \n",
    "            if len(values) == EMBEDDING_DIM + 1:\n",
    "                 word = values[0]\n",
    "                 try:\n",
    "                     coefs = np.asarray(values[1:], dtype='float32')\n",
    "                     embeddings_index[word] = coefs\n",
    "                     loaded_vectors += 1\n",
    "                 except ValueError:\n",
    "                     skipped_lines += 1\n",
    "                     pass\n",
    "            else:\n",
    "                 skipped_lines += 1\n",
    "\n",
    "\n",
    "    print(f\"\\nLoaded {loaded_vectors} word vectors.\")\n",
    "    if skipped_lines > 0:\n",
    "        print(f\"Skipped {skipped_lines} lines due to parsing errors or incorrect dimensions.\")\n",
    "    if loaded_vectors == 0:\n",
    "        print(\"ERROR: No vectors loaded. Check FastText path, dimension, and file format.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: FastText file not found at {FASTTEXT_PATH}. Cannot load pre-trained embeddings.\")\n",
    "    embeddings_index = {} \n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during file reading: {e}\")\n",
    "    embeddings_index = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22912fce",
   "metadata": {
    "papermill": {
     "duration": 0.026173,
     "end_time": "2025-04-22T14:39:16.783881",
     "exception": false,
     "start_time": "2025-04-22T14:39:16.757708",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. Create Embedding Matrix\n",
    "Initialize the embedding layer weights using the loaded FastText vectors. Words not found in FastText will keep random initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c784ca94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T14:39:16.838640Z",
     "iopub.status.busy": "2025-04-22T14:39:16.838418Z",
     "iopub.status.idle": "2025-04-22T14:39:17.835746Z",
     "shell.execute_reply": "2025-04-22T14:39:17.835053Z"
    },
    "papermill": {
     "duration": 1.026532,
     "end_time": "2025-04-22T14:39:17.836879",
     "exception": false,
     "start_time": "2025-04-22T14:39:16.810347",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embedding matrix...\n",
      "Initialized embedding matrix for 16041 words in vocab.\n",
      "  Found pre-trained vectors for 11571 words (incl. lowercase matches).\n",
      "  4469 words not found in FastText (will use random init).\n",
      "Cleaned up FastText index from memory.\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating embedding matrix...\")\n",
    "embedding_matrix = np.random.uniform(-0.05, 0.05, (INPUT_DIM, EMBEDDING_DIM))\n",
    "embedding_matrix = embedding_matrix.astype(np.float32) \n",
    "\n",
    "hits = 0\n",
    "misses = 0\n",
    "for word, i in word_vocab.items():\n",
    "    if i == PAD_IDX:\n",
    "        embedding_matrix[i] = np.zeros(EMBEDDING_DIM, dtype=np.float32)\n",
    "        continue\n",
    "\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    elif word.lower() in embeddings_index:\n",
    "         embedding_matrix[i] = embeddings_index[word.lower()]\n",
    "         hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "\n",
    "print(f\"Initialized embedding matrix for {INPUT_DIM} words in vocab.\")\n",
    "print(f\"  Found pre-trained vectors for {hits} words (incl. lowercase matches).\")\n",
    "print(f\"  {misses} words not found in FastText (will use random init).\")\n",
    "\n",
    "embedding_matrix = torch.FloatTensor(embedding_matrix)\n",
    "\n",
    "del embeddings_index\n",
    "gc.collect()\n",
    "print(\"Cleaned up FastText index from memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10da7869",
   "metadata": {
    "papermill": {
     "duration": 0.027052,
     "end_time": "2025-04-22T14:39:17.891985",
     "exception": false,
     "start_time": "2025-04-22T14:39:17.864933",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6. Numericalize, Pad, and Create Datasets/DataLoaders (Word-based)\n",
    "Convert text sequences to padded numerical sequences and create PyTorch Datasets and DataLoaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d8557a96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T14:39:17.948755Z",
     "iopub.status.busy": "2025-04-22T14:39:17.948241Z",
     "iopub.status.idle": "2025-04-22T14:39:17.953053Z",
     "shell.execute_reply": "2025-04-22T14:39:17.952476Z"
    },
    "papermill": {
     "duration": 0.034174,
     "end_time": "2025-04-22T14:39:17.954173",
     "exception": false,
     "start_time": "2025-04-22T14:39:17.919999",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def text_to_sequence_words(text, vocab):\n",
    "    \"\"\"Converts text to a sequence of word IDs, adding SOS/EOS.\"\"\"\n",
    "    tokens = tokenize_words(text) \n",
    "    return [SOS_IDX] + [vocab.get(token, UNK_IDX) for token in tokens] + [EOS_IDX]\n",
    "\n",
    "def sequence_to_text_words(sequence, inv_vocab):\n",
    "    \"\"\"Converts a sequence of word IDs back to text.\"\"\"\n",
    "    tokens = [inv_vocab.get(idx, UNK_TOKEN) for idx in sequence if idx not in [PAD_IDX, SOS_IDX, EOS_IDX]]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def pad_sequence(seq, max_len, pad_idx):\n",
    "    \"\"\"Pads or truncates a sequence to max_len.\"\"\"\n",
    "    seq = seq[:max_len]\n",
    "    padded = seq + [pad_idx] * (max_len - len(seq))\n",
    "    return padded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "169b9dc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T14:39:18.009633Z",
     "iopub.status.busy": "2025-04-22T14:39:18.009453Z",
     "iopub.status.idle": "2025-04-22T14:39:18.014865Z",
     "shell.execute_reply": "2025-04-22T14:39:18.014196Z"
    },
    "papermill": {
     "duration": 0.035323,
     "end_time": "2025-04-22T14:39:18.016093",
     "exception": false,
     "start_time": "2025-04-22T14:39:17.980770",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_data_words(inputs, targets, vocab, max_len, pad_idx):\n",
    "    \"\"\"Tokenizes (words) and pads input and target lists.\"\"\"\n",
    "    print(f\"Tokenizing (words) and padding sequences (MAX_LEN={max_len})...\")\n",
    "    processed_count = 0\n",
    "    total_pairs = len(inputs)\n",
    "    tokenized_inputs = []\n",
    "    tokenized_targets = []\n",
    "    for i in range(total_pairs):\n",
    "        tokenized_inputs.append(text_to_sequence_words(inputs[i], vocab))\n",
    "        tokenized_targets.append(text_to_sequence_words(targets[i], vocab))\n",
    "        processed_count += 1\n",
    "        if processed_count % 20000 == 0:\n",
    "            print(f\"  Processed {processed_count}/{total_pairs} pairs...\")\n",
    "\n",
    "    print(f\"Finished tokenizing {total_pairs} pairs.\")\n",
    "    print(\"Padding sequences...\")\n",
    "    padded_inputs = [pad_sequence(seq, max_len, pad_idx) for seq in tokenized_inputs]\n",
    "    padded_targets = [pad_sequence(seq, max_len, pad_idx) for seq in tokenized_targets]\n",
    "    print(\"Padding complete.\")\n",
    "    return padded_inputs, padded_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2223f5e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T14:39:18.070530Z",
     "iopub.status.busy": "2025-04-22T14:39:18.070319Z",
     "iopub.status.idle": "2025-04-22T14:39:19.453216Z",
     "shell.execute_reply": "2025-04-22T14:39:19.452459Z"
    },
    "papermill": {
     "duration": 1.411166,
     "end_time": "2025-04-22T14:39:19.454338",
     "exception": false,
     "start_time": "2025-04-22T14:39:18.043172",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing (words) and padding sequences (MAX_LEN=60)...\n",
      "  Processed 20000/58829 pairs...\n",
      "  Processed 40000/58829 pairs...\n",
      "Finished tokenizing 58829 pairs.\n",
      "Padding sequences...\n",
      "Padding complete.\n",
      "Tokenizing (words) and padding sequences (MAX_LEN=60)...\n",
      "Finished tokenizing 9267 pairs.\n",
      "Padding sequences...\n",
      "Padding complete.\n"
     ]
    }
   ],
   "source": [
    "padded_train_inputs, padded_train_targets = process_data_words(train_inputs, train_targets, word_vocab, MAX_LEN, PAD_IDX)\n",
    "padded_val_inputs, padded_val_targets = process_data_words(val_inputs, val_targets, word_vocab, MAX_LEN, PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f7dd1630",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T14:39:19.510890Z",
     "iopub.status.busy": "2025-04-22T14:39:19.510637Z",
     "iopub.status.idle": "2025-04-22T14:39:19.515708Z",
     "shell.execute_reply": "2025-04-22T14:39:19.514863Z"
    },
    "papermill": {
     "duration": 0.034199,
     "end_time": "2025-04-22T14:39:19.516982",
     "exception": false,
     "start_time": "2025-04-22T14:39:19.482783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DialogueDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for dialogue pairs.\"\"\"\n",
    "    def __init__(self, inputs, targets):\n",
    "        assert len(inputs) > 0, \"Input list to DialogueDataset is empty!\"\n",
    "        assert len(inputs) == len(targets), \"Inputs and targets have different lengths!\"\n",
    "        self.inputs = torch.tensor(inputs, dtype=torch.long)\n",
    "        self.targets = torch.tensor(targets, dtype=torch.long)\n",
    "        print(f\"DialogueDataset created with {len(self.inputs)} samples.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples.\"\"\"\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Fetches the sample at the given index.\"\"\"\n",
    "        return self.inputs[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b952c623",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T14:39:19.572944Z",
     "iopub.status.busy": "2025-04-22T14:39:19.572714Z",
     "iopub.status.idle": "2025-04-22T14:39:20.366536Z",
     "shell.execute_reply": "2025-04-22T14:39:20.365748Z"
    },
    "papermill": {
     "duration": 0.822898,
     "end_time": "2025-04-22T14:39:20.367759",
     "exception": false,
     "start_time": "2025-04-22T14:39:19.544861",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialogueDataset created with 58829 samples.\n",
      "DialogueDataset created with 9267 samples.\n",
      "Created Train DataLoader with 920 batches (Batch Size: 64).\n",
      "Created Validation DataLoader with 145 batches (Batch Size: 64).\n",
      "Cleaned up intermediate data lists from memory.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    train_dataset = DialogueDataset(padded_train_inputs, padded_train_targets)\n",
    "    val_dataset = DialogueDataset(padded_val_inputs, padded_val_targets)\n",
    "except AssertionError as e:\n",
    "    print(f\"Error creating dataset: {e}\")\n",
    "    raise e\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True if device=='cuda' else False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True if device=='cuda' else False)\n",
    "\n",
    "print(f\"Created Train DataLoader with {len(train_loader)} batches (Batch Size: {BATCH_SIZE}).\")\n",
    "print(f\"Created Validation DataLoader with {len(val_loader)} batches (Batch Size: {BATCH_SIZE}).\")\n",
    "\n",
    "try:\n",
    "    del padded_train_inputs, padded_train_targets, padded_val_inputs, padded_val_targets\n",
    "    del train_inputs, train_targets, val_inputs, val_targets, all_train_texts_words\n",
    "    gc.collect()\n",
    "    print(\"Cleaned up intermediate data lists from memory.\")\n",
    "except NameError:\n",
    "    print(\"Intermediate lists cleanup skipped or already done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773e2f56",
   "metadata": {
    "papermill": {
     "duration": 0.027689,
     "end_time": "2025-04-22T14:39:20.424373",
     "exception": false,
     "start_time": "2025-04-22T14:39:20.396684",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7. Transformer Model Implementation (with Pre-trained Embeddings)\n",
    "Core components of the Transformer architecture. Encoder and Decoder are modified to accept and use the pre-trained embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "de2a3d9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T14:39:20.481025Z",
     "iopub.status.busy": "2025-04-22T14:39:20.480742Z",
     "iopub.status.idle": "2025-04-22T14:39:20.486313Z",
     "shell.execute_reply": "2025-04-22T14:39:20.485765Z"
    },
    "papermill": {
     "duration": 0.035479,
     "end_time": "2025-04-22T14:39:20.487297",
     "exception": false,
     "start_time": "2025-04-22T14:39:20.451818",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Injects positional information into the input embeddings.\"\"\"\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b0377a23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T14:39:20.542708Z",
     "iopub.status.busy": "2025-04-22T14:39:20.542504Z",
     "iopub.status.idle": "2025-04-22T14:39:20.549765Z",
     "shell.execute_reply": "2025-04-22T14:39:20.549190Z"
    },
    "papermill": {
     "duration": 0.035884,
     "end_time": "2025-04-22T14:39:20.550875",
     "exception": false,
     "start_time": "2025-04-22T14:39:20.514991",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-Head Attention mechanism.\"\"\"\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "        self.fc_q = nn.Linear(d_model, d_model)\n",
    "        self.fc_k = nn.Linear(d_model, d_model)\n",
    "        self.fc_v = nn.Linear(d_model, d_model)\n",
    "        self.fc_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"scale\", torch.sqrt(torch.FloatTensor([self.head_dim])))\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.shape[1]\n",
    "        Q = self.fc_q(query); K = self.fc_k(key); V = self.fc_v(value)\n",
    "        Q = Q.view(query.shape[0], batch_size, self.n_heads, self.head_dim).permute(1, 2, 0, 3)\n",
    "        K = K.view(key.shape[0], batch_size, self.n_heads, self.head_dim).permute(1, 2, 0, 3)\n",
    "        V = V.view(value.shape[0], batch_size, self.n_heads, self.head_dim).permute(1, 2, 0, 3)\n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale.to(Q.device)\n",
    "        if mask is not None: energy = energy.masked_fill(mask == 0, -1e10)\n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "        attention = self.dropout(attention)\n",
    "        x = torch.matmul(attention, V)\n",
    "        x = x.permute(0, 2, 1, 3).contiguous().view(batch_size, query.shape[0], self.d_model)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.fc_o(x) # Final linear layer\n",
    "        return x, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "19a8a20e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T14:39:20.606186Z",
     "iopub.status.busy": "2025-04-22T14:39:20.605982Z",
     "iopub.status.idle": "2025-04-22T14:39:20.610082Z",
     "shell.execute_reply": "2025-04-22T14:39:20.609506Z"
    },
    "papermill": {
     "duration": 0.032757,
     "end_time": "2025-04-22T14:39:20.611114",
     "exception": false,
     "start_time": "2025-04-22T14:39:20.578357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedforward(nn.Module):\n",
    "    \"\"\"Position-wise Feedforward Network.\"\"\"\n",
    "    def __init__(self, d_model, pf_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, pf_dim)\n",
    "        self.fc2 = nn.Linear(pf_dim, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eb410b6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T14:39:20.666846Z",
     "iopub.status.busy": "2025-04-22T14:39:20.666645Z",
     "iopub.status.idle": "2025-04-22T14:39:20.671238Z",
     "shell.execute_reply": "2025-04-22T14:39:20.670664Z"
    },
    "papermill": {
     "duration": 0.033343,
     "end_time": "2025-04-22T14:39:20.672333",
     "exception": false,
     "start_time": "2025-04-22T14:39:20.638990",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"A single layer of the Transformer Encoder.\"\"\"\n",
    "    def __init__(self, d_model, n_heads, pf_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.ff = PositionwiseFeedforward(d_model, pf_dim, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, src, src_mask):\n",
    "        _src, _ = self.self_attn(src, src, src, src_mask)\n",
    "        src = self.norm1(src + self.dropout(_src))\n",
    "        _src = self.ff(src)\n",
    "        src = self.norm2(src + self.dropout(_src))\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "321492e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T14:39:20.728597Z",
     "iopub.status.busy": "2025-04-22T14:39:20.728398Z",
     "iopub.status.idle": "2025-04-22T14:39:20.733613Z",
     "shell.execute_reply": "2025-04-22T14:39:20.733111Z"
    },
    "papermill": {
     "duration": 0.03478,
     "end_time": "2025-04-22T14:39:20.734604",
     "exception": false,
     "start_time": "2025-04-22T14:39:20.699824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"A single layer of the Transformer Decoder.\"\"\"\n",
    "    def __init__(self, d_model, n_heads, pf_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.masked_self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.encoder_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.ff = PositionwiseFeedforward(d_model, pf_dim, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        _trg, _ = self.masked_self_attn(trg, trg, trg, trg_mask)\n",
    "        trg = self.norm1(trg + self.dropout(_trg))\n",
    "        _trg, attention = self.encoder_attn(trg, enc_src, enc_src, src_mask)\n",
    "        trg = self.norm2(trg + self.dropout(_trg))\n",
    "        _trg = self.ff(trg)\n",
    "        trg = self.norm3(trg + self.dropout(_trg))\n",
    "        return trg, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "42a2b1b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T14:39:20.789461Z",
     "iopub.status.busy": "2025-04-22T14:39:20.789253Z",
     "iopub.status.idle": "2025-04-22T14:39:20.795326Z",
     "shell.execute_reply": "2025-04-22T14:39:20.794818Z"
    },
    "papermill": {
     "duration": 0.034299,
     "end_time": "2025-04-22T14:39:20.796339",
     "exception": false,
     "start_time": "2025-04-22T14:39:20.762040",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"The Transformer Encoder stack, initialized with pre-trained embeddings.\"\"\"\n",
    "    def __init__(self, input_dim, d_model, n_layers, n_heads, pf_dim, dropout,\n",
    "                 pretrained_embeddings, freeze_embeddings, max_len=MAX_LEN): \n",
    "        super().__init__()\n",
    "        self.tok_embedding = nn.Embedding(input_dim, d_model)\n",
    "        print(\"Loading pre-trained weights into Encoder Embedding layer...\")\n",
    "        self.tok_embedding.weight.data.copy_(pretrained_embeddings)\n",
    "        if freeze_embeddings:\n",
    "            self.tok_embedding.weight.requires_grad = False\n",
    "            print(\"Encoder embeddings frozen.\")\n",
    "        else:\n",
    "            self.tok_embedding.weight.requires_grad = True \n",
    "            print(\"Encoder embeddings will be fine-tuned.\")\n",
    "\n",
    "        self.pos_embedding = PositionalEncoding(d_model, dropout, max_len=5000 if max_len < 5000 else max_len)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, pf_dim, dropout) for _ in range(n_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"scale\", torch.sqrt(torch.FloatTensor([d_model])))\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        \"\"\"Forward pass for the Encoder.\"\"\"\n",
    "        scale = self.scale.to(src.device)\n",
    "        embedded = self.dropout((self.tok_embedding(src) * scale)) \n",
    "        embedded = embedded.permute(1, 0, 2) \n",
    "        pos_embedded = self.pos_embedding(embedded)\n",
    "        enc_output = pos_embedded\n",
    "        for layer in self.layers:\n",
    "            enc_output = layer(enc_output, src_mask)\n",
    "        return enc_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6257494a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T14:39:20.852418Z",
     "iopub.status.busy": "2025-04-22T14:39:20.852210Z",
     "iopub.status.idle": "2025-04-22T14:39:20.858455Z",
     "shell.execute_reply": "2025-04-22T14:39:20.857894Z"
    },
    "papermill": {
     "duration": 0.035574,
     "end_time": "2025-04-22T14:39:20.859530",
     "exception": false,
     "start_time": "2025-04-22T14:39:20.823956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"The Transformer Decoder stack, initialized with pre-trained embeddings.\"\"\"\n",
    "    def __init__(self, output_dim, d_model, n_layers, n_heads, pf_dim, dropout,\n",
    "                 pretrained_embeddings, freeze_embeddings, max_len=MAX_LEN): \n",
    "        super().__init__()\n",
    "        self.tok_embedding = nn.Embedding(output_dim, d_model)\n",
    "        print(\"Loading pre-trained weights into Decoder Embedding layer...\")\n",
    "        self.tok_embedding.weight.data.copy_(pretrained_embeddings)\n",
    "        if freeze_embeddings:\n",
    "            self.tok_embedding.weight.requires_grad = False\n",
    "            print(\"Decoder embeddings frozen.\")\n",
    "        else:\n",
    "            self.tok_embedding.weight.requires_grad = True \n",
    "            print(\"Decoder embeddings will be fine-tuned.\")\n",
    "\n",
    "        self.pos_embedding = PositionalEncoding(d_model, dropout, max_len=5000 if max_len < 5000 else max_len)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, pf_dim, dropout) for _ in range(n_layers)])\n",
    "        self.fc_out = nn.Linear(d_model, output_dim) \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"scale\", torch.sqrt(torch.FloatTensor([d_model])))\n",
    "\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \"\"\"Forward pass for the Decoder.\"\"\"\n",
    "        scale = self.scale.to(trg.device)\n",
    "        embedded = self.dropout((self.tok_embedding(trg) * scale)) \n",
    "        embedded = embedded.permute(1, 0, 2) \n",
    "        pos_embedded = self.pos_embedding(embedded)\n",
    "        dec_output = pos_embedded\n",
    "        attention = None \n",
    "        for layer in self.layers:\n",
    "            dec_output, attention = layer(dec_output, enc_src, trg_mask, src_mask)\n",
    "        output = self.fc_out(dec_output) \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "37ed83c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T14:39:20.915730Z",
     "iopub.status.busy": "2025-04-22T14:39:20.915526Z",
     "iopub.status.idle": "2025-04-22T14:39:20.921411Z",
     "shell.execute_reply": "2025-04-22T14:39:20.920723Z"
    },
    "papermill": {
     "duration": 0.035449,
     "end_time": "2025-04-22T14:39:20.922413",
     "exception": false,
     "start_time": "2025-04-22T14:39:20.886964",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    \"\"\"The main Seq2Seq Transformer model.\"\"\"\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        \"\"\"Creates a mask for the source sequence to ignore padding tokens.\"\"\"\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        return src_mask.to(self.device) \n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        \"\"\"Creates a mask for the target sequence to hide padding and future tokens.\"\"\"\n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device=self.device)).bool()\n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        return trg_mask.to(self.device) \n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        \"\"\"Forward pass for the entire Seq2Seq model.\"\"\"\n",
    "        src = src.to(self.device); trg = trg.to(self.device) \n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_src = self.encoder(src, src_mask) \n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask) \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcedc9a7",
   "metadata": {
    "papermill": {
     "duration": 0.02719,
     "end_time": "2025-04-22T14:39:20.976885",
     "exception": false,
     "start_time": "2025-04-22T14:39:20.949695",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8. Model Initialization (Using Pre-trained Matrix)\n",
    "Instantiate the model components, loading the pre-trained FastText embedding matrix. Define optimizer and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "019146b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T14:39:21.033585Z",
     "iopub.status.busy": "2025-04-22T14:39:21.033379Z",
     "iopub.status.idle": "2025-04-22T14:39:21.400954Z",
     "shell.execute_reply": "2025-04-22T14:39:21.400064Z"
    },
    "papermill": {
     "duration": 0.396619,
     "end_time": "2025-04-22T14:39:21.402293",
     "exception": false,
     "start_time": "2025-04-22T14:39:21.005674",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model components with FastText embeddings...\n",
      "Loading pre-trained weights into Encoder Embedding layer...\n",
      "Encoder embeddings will be fine-tuned.\n",
      "Loading pre-trained weights into Decoder Embedding layer...\n",
      "Decoder embeddings will be fine-tuned.\n",
      "Applying Xavier uniform initialization to non-embedding layers...\n",
      "The model has 19,560,813 trainable parameters.\n",
      "Model is on device: cuda:0\n",
      "Cleaned up embedding matrix from host memory.\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing model components with FastText embeddings...\")\n",
    "if 'embedding_matrix' not in locals() or not isinstance(embedding_matrix, torch.Tensor):\n",
    "     raise ValueError(\"Embedding matrix not created or not a tensor. Please run Cell 15.\")\n",
    "\n",
    "enc = Encoder(INPUT_DIM, D_MODEL, ENC_LAYERS, N_HEADS, PF_DIM, DROPOUT,\n",
    "              embedding_matrix, FREEZE_EMBEDDINGS, MAX_LEN).to(device)\n",
    "dec = Decoder(OUTPUT_DIM, D_MODEL, DEC_LAYERS, N_HEADS, PF_DIM, DROPOUT,\n",
    "              embedding_matrix, FREEZE_EMBEDDINGS, MAX_LEN).to(device)\n",
    "\n",
    "model = Seq2SeqTransformer(enc, dec, PAD_IDX, PAD_IDX, device).to(device)\n",
    "\n",
    "def initialize_non_embedding_weights(m):\n",
    "    \"\"\"Initializes non-embedding layers with Xavier uniform.\"\"\"\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1 and not isinstance(m, nn.Embedding):\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "    if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "print(\"Applying Xavier uniform initialization to non-embedding layers...\")\n",
    "model.apply(initialize_non_embedding_weights)\n",
    "\n",
    "params_to_optimize = model.parameters() if not FREEZE_EMBEDDINGS else filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = optim.AdamW(params_to_optimize, lr=LEARNING_RATE) \n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Counts trainable parameters.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters.')\n",
    "if next(model.parameters()).is_cuda:\n",
    "    print(f\"Model is on device: cuda:{next(model.parameters()).device.index}\")\n",
    "else:\n",
    "     print(f\"Model is on device: {next(model.parameters()).device}\")\n",
    "\n",
    "\n",
    "del embedding_matrix\n",
    "gc.collect()\n",
    "print(\"Cleaned up embedding matrix from host memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678f3fd5",
   "metadata": {
    "papermill": {
     "duration": 0.02751,
     "end_time": "2025-04-22T14:39:21.458010",
     "exception": false,
     "start_time": "2025-04-22T14:39:21.430500",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 9. Training and Evaluation Functions\n",
    "Define the loops for one epoch of training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a6ad3063",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T14:39:21.513863Z",
     "iopub.status.busy": "2025-04-22T14:39:21.513619Z",
     "iopub.status.idle": "2025-04-22T14:39:21.519239Z",
     "shell.execute_reply": "2025-04-22T14:39:21.518656Z"
    },
    "papermill": {
     "duration": 0.034811,
     "end_time": "2025-04-22T14:39:21.520340",
     "exception": false,
     "start_time": "2025-04-22T14:39:21.485529",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \"\"\"Performs one epoch of training.\"\"\"\n",
    "    model.train() \n",
    "    epoch_loss = 0\n",
    "    num_batches = len(iterator)\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src, trg = batch\n",
    "        src, trg = src.to(device, non_blocking=True), trg.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        trg_input = trg[:, :-1] \n",
    "        trg_output_expected = trg[:, 1:].contiguous() \n",
    "\n",
    "        output, _ = model(src, trg_input)\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output.permute(1, 0, 2).contiguous().view(-1, output_dim)\n",
    "        trg_output_expected = trg_output_expected.view(-1)\n",
    "\n",
    "        loss = criterion(output, trg_output_expected)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f0a3983b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T14:39:21.576487Z",
     "iopub.status.busy": "2025-04-22T14:39:21.576283Z",
     "iopub.status.idle": "2025-04-22T14:39:21.581279Z",
     "shell.execute_reply": "2025-04-22T14:39:21.580633Z"
    },
    "papermill": {
     "duration": 0.034274,
     "end_time": "2025-04-22T14:39:21.582342",
     "exception": false,
     "start_time": "2025-04-22T14:39:21.548068",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \"\"\"Performs one epoch of evaluation.\"\"\"\n",
    "    model.eval() \n",
    "    epoch_loss = 0\n",
    "    num_batches = len(iterator)\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        for i, batch in enumerate(iterator):\n",
    "            src, trg = batch\n",
    "            src, trg = src.to(device, non_blocking=True), trg.to(device, non_blocking=True)\n",
    "\n",
    "            trg_input = trg[:, :-1]\n",
    "            trg_output_expected = trg[:, 1:].contiguous()\n",
    "\n",
    "            output, _ = model(src, trg_input) \n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output.permute(1, 0, 2).contiguous().view(-1, output_dim)\n",
    "            trg_output_expected = trg_output_expected.view(-1)\n",
    "\n",
    "            loss = criterion(output, trg_output_expected)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d991ba5c",
   "metadata": {
    "papermill": {
     "duration": 0.027195,
     "end_time": "2025-04-22T14:39:21.637448",
     "exception": false,
     "start_time": "2025-04-22T14:39:21.610253",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 10. Training Loop (with Validation and Early Stopping)\n",
    "Execute the training process, saving the best model based on validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "65873c60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T14:39:21.693732Z",
     "iopub.status.busy": "2025-04-22T14:39:21.693528Z",
     "iopub.status.idle": "2025-04-22T15:30:20.250785Z",
     "shell.execute_reply": "2025-04-22T15:30:20.249965Z"
    },
    "papermill": {
     "duration": 3058.615853,
     "end_time": "2025-04-22T15:30:20.280931",
     "exception": false,
     "start_time": "2025-04-22T14:39:21.665078",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n",
      "--- Epoch 1/50 ---\n",
      "Epoch 1 Time: 1m 56s\n",
      "\tTrain Loss: 5.9466 | Train PPL: 382.4571\n",
      "\t Val. Loss: 5.2857 |  Val. PPL: 197.4930\n",
      "*** Epoch 1: Validation loss improved. Saving best model to fasttext-transformer-basic-best.pt ***\n",
      "------------------------------------------------------------\n",
      "--- Epoch 2/50 ---\n",
      "Epoch 2 Time: 1m 57s\n",
      "\tTrain Loss: 5.1694 | Train PPL: 175.8071\n",
      "\t Val. Loss: 4.9002 |  Val. PPL: 134.3170\n",
      "*** Epoch 2: Validation loss improved. Saving best model to fasttext-transformer-basic-best.pt ***\n",
      "------------------------------------------------------------\n",
      "--- Epoch 3/50 ---\n",
      "Epoch 3 Time: 1m 57s\n",
      "\tTrain Loss: 4.8690 | Train PPL: 130.1935\n",
      "\t Val. Loss: 4.7175 |  Val. PPL: 111.8925\n",
      "*** Epoch 3: Validation loss improved. Saving best model to fasttext-transformer-basic-best.pt ***\n",
      "------------------------------------------------------------\n",
      "--- Epoch 4/50 ---\n",
      "Epoch 4 Time: 1m 57s\n",
      "\tTrain Loss: 4.6910 | Train PPL: 108.9594\n",
      "\t Val. Loss: 4.6156 |  Val. PPL: 101.0434\n",
      "*** Epoch 4: Validation loss improved. Saving best model to fasttext-transformer-basic-best.pt ***\n",
      "------------------------------------------------------------\n",
      "--- Epoch 5/50 ---\n",
      "Epoch 5 Time: 1m 57s\n",
      "\tTrain Loss: 4.5689 | Train PPL: 96.4362\n",
      "\t Val. Loss: 4.5427 |  Val. PPL: 93.9480\n",
      "*** Epoch 5: Validation loss improved. Saving best model to fasttext-transformer-basic-best.pt ***\n",
      "------------------------------------------------------------\n",
      "--- Epoch 6/50 ---\n",
      "Epoch 6 Time: 1m 57s\n",
      "\tTrain Loss: 4.4709 | Train PPL: 87.4340\n",
      "\t Val. Loss: 4.5000 |  Val. PPL: 90.0155\n",
      "*** Epoch 6: Validation loss improved. Saving best model to fasttext-transformer-basic-best.pt ***\n",
      "------------------------------------------------------------\n",
      "--- Epoch 7/50 ---\n",
      "Epoch 7 Time: 1m 57s\n",
      "\tTrain Loss: 4.3914 | Train PPL: 80.7498\n",
      "\t Val. Loss: 4.4582 |  Val. PPL: 86.3357\n",
      "*** Epoch 7: Validation loss improved. Saving best model to fasttext-transformer-basic-best.pt ***\n",
      "------------------------------------------------------------\n",
      "--- Epoch 8/50 ---\n",
      "Epoch 8 Time: 1m 57s\n",
      "\tTrain Loss: 4.3203 | Train PPL: 75.2115\n",
      "\t Val. Loss: 4.4304 |  Val. PPL: 83.9635\n",
      "*** Epoch 8: Validation loss improved. Saving best model to fasttext-transformer-basic-best.pt ***\n",
      "------------------------------------------------------------\n",
      "--- Epoch 9/50 ---\n",
      "Epoch 9 Time: 1m 57s\n",
      "\tTrain Loss: 4.2569 | Train PPL: 70.5875\n",
      "\t Val. Loss: 4.4209 |  Val. PPL: 83.1718\n",
      "*** Epoch 9: Validation loss improved. Saving best model to fasttext-transformer-basic-best.pt ***\n",
      "------------------------------------------------------------\n",
      "--- Epoch 10/50 ---\n",
      "Epoch 10 Time: 1m 57s\n",
      "\tTrain Loss: 4.2010 | Train PPL: 66.7541\n",
      "\t Val. Loss: 4.3978 |  Val. PPL: 81.2719\n",
      "*** Epoch 10: Validation loss improved. Saving best model to fasttext-transformer-basic-best.pt ***\n",
      "------------------------------------------------------------\n",
      "--- Epoch 11/50 ---\n",
      "Epoch 11 Time: 1m 57s\n",
      "\tTrain Loss: 4.1485 | Train PPL: 63.3390\n",
      "\t Val. Loss: 4.3899 |  Val. PPL: 80.6353\n",
      "*** Epoch 11: Validation loss improved. Saving best model to fasttext-transformer-basic-best.pt ***\n",
      "------------------------------------------------------------\n",
      "--- Epoch 12/50 ---\n",
      "Epoch 12 Time: 1m 57s\n",
      "\tTrain Loss: 4.0948 | Train PPL: 60.0280\n",
      "\t Val. Loss: 4.3836 |  Val. PPL: 80.1274\n",
      "*** Epoch 12: Validation loss improved. Saving best model to fasttext-transformer-basic-best.pt ***\n",
      "------------------------------------------------------------\n",
      "--- Epoch 13/50 ---\n",
      "Epoch 13 Time: 1m 57s\n",
      "\tTrain Loss: 4.0521 | Train PPL: 57.5209\n",
      "\t Val. Loss: 4.3876 |  Val. PPL: 80.4494\n",
      "Epoch 13: Validation loss did not improve. (1/10) Best Val Loss: 4.3836\n",
      "------------------------------------------------------------\n",
      "--- Epoch 14/50 ---\n",
      "Epoch 14 Time: 1m 57s\n",
      "\tTrain Loss: 4.0066 | Train PPL: 54.9592\n",
      "\t Val. Loss: 4.3867 |  Val. PPL: 80.3732\n",
      "Epoch 14: Validation loss did not improve. (2/10) Best Val Loss: 4.3836\n",
      "------------------------------------------------------------\n",
      "--- Epoch 15/50 ---\n",
      "Epoch 15 Time: 1m 57s\n",
      "\tTrain Loss: 3.9672 | Train PPL: 52.8357\n",
      "\t Val. Loss: 4.3949 |  Val. PPL: 81.0355\n",
      "Epoch 15: Validation loss did not improve. (3/10) Best Val Loss: 4.3836\n",
      "------------------------------------------------------------\n",
      "--- Epoch 16/50 ---\n",
      "Epoch 16 Time: 1m 57s\n",
      "\tTrain Loss: 3.9295 | Train PPL: 50.8822\n",
      "\t Val. Loss: 4.3825 |  Val. PPL: 80.0386\n",
      "*** Epoch 16: Validation loss improved. Saving best model to fasttext-transformer-basic-best.pt ***\n",
      "------------------------------------------------------------\n",
      "--- Epoch 17/50 ---\n",
      "Epoch 17 Time: 1m 57s\n",
      "\tTrain Loss: 3.8936 | Train PPL: 49.0855\n",
      "\t Val. Loss: 4.3951 |  Val. PPL: 81.0544\n",
      "Epoch 17: Validation loss did not improve. (1/10) Best Val Loss: 4.3825\n",
      "------------------------------------------------------------\n",
      "--- Epoch 18/50 ---\n",
      "Epoch 18 Time: 1m 57s\n",
      "\tTrain Loss: 3.8643 | Train PPL: 47.6695\n",
      "\t Val. Loss: 4.4034 |  Val. PPL: 81.7287\n",
      "Epoch 18: Validation loss did not improve. (2/10) Best Val Loss: 4.3825\n",
      "------------------------------------------------------------\n",
      "--- Epoch 19/50 ---\n",
      "Epoch 19 Time: 1m 57s\n",
      "\tTrain Loss: 3.8313 | Train PPL: 46.1221\n",
      "\t Val. Loss: 4.4101 |  Val. PPL: 82.2761\n",
      "Epoch 19: Validation loss did not improve. (3/10) Best Val Loss: 4.3825\n",
      "------------------------------------------------------------\n",
      "--- Epoch 20/50 ---\n",
      "Epoch 20 Time: 1m 57s\n",
      "\tTrain Loss: 3.8014 | Train PPL: 44.7655\n",
      "\t Val. Loss: 4.4185 |  Val. PPL: 82.9716\n",
      "Epoch 20: Validation loss did not improve. (4/10) Best Val Loss: 4.3825\n",
      "------------------------------------------------------------\n",
      "--- Epoch 21/50 ---\n",
      "Epoch 21 Time: 1m 57s\n",
      "\tTrain Loss: 3.7729 | Train PPL: 43.5040\n",
      "\t Val. Loss: 4.4254 |  Val. PPL: 83.5490\n",
      "Epoch 21: Validation loss did not improve. (5/10) Best Val Loss: 4.3825\n",
      "------------------------------------------------------------\n",
      "--- Epoch 22/50 ---\n",
      "Epoch 22 Time: 1m 57s\n",
      "\tTrain Loss: 3.7491 | Train PPL: 42.4844\n",
      "\t Val. Loss: 4.4386 |  Val. PPL: 84.6558\n",
      "Epoch 22: Validation loss did not improve. (6/10) Best Val Loss: 4.3825\n",
      "------------------------------------------------------------\n",
      "--- Epoch 23/50 ---\n",
      "Epoch 23 Time: 1m 57s\n",
      "\tTrain Loss: 3.7226 | Train PPL: 41.3715\n",
      "\t Val. Loss: 4.4446 |  Val. PPL: 85.1693\n",
      "Epoch 23: Validation loss did not improve. (7/10) Best Val Loss: 4.3825\n",
      "------------------------------------------------------------\n",
      "--- Epoch 24/50 ---\n",
      "Epoch 24 Time: 1m 57s\n",
      "\tTrain Loss: 3.7016 | Train PPL: 40.5135\n",
      "\t Val. Loss: 4.4588 |  Val. PPL: 86.3849\n",
      "Epoch 24: Validation loss did not improve. (8/10) Best Val Loss: 4.3825\n",
      "------------------------------------------------------------\n",
      "--- Epoch 25/50 ---\n",
      "Epoch 25 Time: 1m 57s\n",
      "\tTrain Loss: 3.6799 | Train PPL: 39.6433\n",
      "\t Val. Loss: 4.4699 |  Val. PPL: 87.3464\n",
      "Epoch 25: Validation loss did not improve. (9/10) Best Val Loss: 4.3825\n",
      "------------------------------------------------------------\n",
      "--- Epoch 26/50 ---\n",
      "Epoch 26 Time: 1m 57s\n",
      "\tTrain Loss: 3.6585 | Train PPL: 38.8017\n",
      "\t Val. Loss: 4.4771 |  Val. PPL: 87.9819\n",
      "Epoch 26: Validation loss did not improve. (10/10) Best Val Loss: 4.3825\n",
      "\n",
      "Early stopping triggered after 10 epochs without validation loss improvement.\n",
      "\n",
      "Training Finished. Total time: 50m 59s\n",
      "Best validation loss achieved: 4.3825\n",
      "Best model saved to: fasttext-transformer-basic-best.pt\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting Training...\")\n",
    "start_training_time = time.time()\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "model_save_path_best = 'fasttext-transformer-basic-best.pt' \n",
    "epochs_no_improve = 0\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    epoch_start_time = time.time()\n",
    "    print(f\"--- Epoch {epoch+1}/{N_EPOCHS} ---\")\n",
    "\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, val_loader, criterion)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_mins = int((epoch_end_time - epoch_start_time) / 60)\n",
    "    epoch_secs = int((epoch_end_time - epoch_start_time) % 60)\n",
    "\n",
    "    try:\n",
    "        train_ppl = math.exp(train_loss)\n",
    "    except OverflowError:\n",
    "        train_ppl = float('inf')\n",
    "    try:\n",
    "        valid_ppl = math.exp(valid_loss)\n",
    "    except OverflowError:\n",
    "        valid_ppl = float('inf')\n",
    "\n",
    "\n",
    "    print(f'Epoch {epoch+1} Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.4f} | Train PPL: {train_ppl:7.4f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.4f} |  Val. PPL: {valid_ppl:7.4f}')\n",
    "\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), model_save_path_best)\n",
    "        epochs_no_improve = 0 \n",
    "        print(f\"*** Epoch {epoch+1}: Validation loss improved. Saving best model to {model_save_path_best} ***\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"Epoch {epoch+1}: Validation loss did not improve. ({epochs_no_improve}/{PATIENCE}) Best Val Loss: {best_valid_loss:.4f}\")\n",
    "\n",
    "    if epochs_no_improve >= PATIENCE:\n",
    "        print(f\"\\nEarly stopping triggered after {PATIENCE} epochs without validation loss improvement.\")\n",
    "        break \n",
    "\n",
    "    print(\"-\" * 60) \n",
    "\n",
    "total_training_time = time.time() - start_training_time\n",
    "print(f\"\\nTraining Finished. Total time: {total_training_time // 60:.0f}m {total_training_time % 60:.0f}s\")\n",
    "print(f\"Best validation loss achieved: {best_valid_loss:.4f}\")\n",
    "if os.path.exists(model_save_path_best):\n",
    "    print(f\"Best model saved to: {model_save_path_best}\")\n",
    "else:\n",
    "    print(\"Warning: No best model was saved (perhaps training stopped early or loss never improved).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd20fc5",
   "metadata": {
    "papermill": {
     "duration": 0.028725,
     "end_time": "2025-04-22T15:30:20.338512",
     "exception": false,
     "start_time": "2025-04-22T15:30:20.309787",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 11. Inference Function (with Sampling, Word-based)\n",
    "Generates responses using the trained model and word-level vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b309635f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T15:30:20.397461Z",
     "iopub.status.busy": "2025-04-22T15:30:20.396996Z",
     "iopub.status.idle": "2025-04-22T15:30:20.407313Z",
     "shell.execute_reply": "2025-04-22T15:30:20.406706Z"
    },
    "papermill": {
     "duration": 0.041132,
     "end_time": "2025-04-22T15:30:20.408313",
     "exception": false,
     "start_time": "2025-04-22T15:30:20.367181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_response_sampling_words( \n",
    "    sentence,\n",
    "    model,\n",
    "    vocab,      \n",
    "    inv_vocab,   \n",
    "    device,\n",
    "    max_len=MAX_LEN,\n",
    "    strategy=\"topk\", \n",
    "    k=10,            \n",
    "    p=0.9,           \n",
    "    temperature=0.8  \n",
    "    ):\n",
    "    \"\"\"Generates a response using the transformer model with sampling (word-level).\"\"\"\n",
    "    assert strategy in ['greedy', 'topk', 'topp'], \"Strategy must be 'greedy', 'topk', or 'topp'\"\n",
    "    assert temperature > 0, \"Temperature must be positive\"\n",
    "\n",
    "    model.eval() \n",
    "\n",
    "    tokens = text_to_sequence_words(sentence, vocab) \n",
    "    if len(tokens) > max_len:\n",
    "        print(f\"Warning: Input sentence truncated from {len(tokens)} to {max_len} tokens.\")\n",
    "        tokens = tokens[:max_len]\n",
    "\n",
    "    src_tensor = torch.LongTensor(tokens).unsqueeze(0).to(device) \n",
    "    src_mask = model.make_src_mask(src_tensor) \n",
    "\n",
    "    with torch.no_grad():\n",
    "        enc_src = model.encoder(src_tensor, src_mask) \n",
    "\n",
    "    trg_indices = [SOS_IDX] \n",
    "\n",
    "    for i in range(max_len - 1): \n",
    "        trg_tensor = torch.LongTensor(trg_indices).unsqueeze(0).to(device) \n",
    "        trg_mask = model.make_trg_mask(trg_tensor) \n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
    "\n",
    "        pred_token_logits = output[-1, 0, :] \n",
    "\n",
    "        if temperature > 1e-6:\n",
    "            pred_token_logits = pred_token_logits / temperature\n",
    "        else:\n",
    "             strategy = 'greedy' \n",
    "\n",
    "        pred_token_probs = F.softmax(pred_token_logits, dim=-1) \n",
    "\n",
    "        next_token_id = -1 \n",
    "        if strategy == 'greedy':\n",
    "            next_token_id = torch.argmax(pred_token_probs).item()\n",
    "        elif strategy == 'topk':\n",
    "            effective_k = min(k, pred_token_probs.size(-1))\n",
    "            if effective_k <= 0: \n",
    "                 print(\"Warning: k=0 in top-k sampling. Using EOS.\")\n",
    "                 next_token_id = EOS_IDX\n",
    "            else:\n",
    "                topk_probs, topk_indices = torch.topk(pred_token_probs, k=effective_k)\n",
    "                mask = torch.zeros_like(pred_token_probs)\n",
    "                mask.scatter_(0, topk_indices, 1.0)\n",
    "                filtered_probs = pred_token_probs * mask\n",
    "                sum_filtered_probs = torch.sum(filtered_probs)\n",
    "                if sum_filtered_probs > 1e-9: \n",
    "                     filtered_probs = filtered_probs / sum_filtered_probs\n",
    "                     next_token_id = torch.multinomial(filtered_probs, num_samples=1).item()\n",
    "                else: \n",
    "                     print(\"Warning: Top-k resulted in zero probability sum. Using EOS.\")\n",
    "                     next_token_id = EOS_IDX\n",
    "        elif strategy == 'topp':\n",
    "            sorted_probs, sorted_indices = torch.sort(pred_token_probs, descending=True)\n",
    "            cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "            sorted_indices_to_remove = cumulative_probs > p\n",
    "            if sorted_indices_to_remove.shape[0] > 1: \n",
    "                 sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()\n",
    "            sorted_indices_to_remove[0] = 0 \n",
    "            indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "            pred_token_probs[indices_to_remove] = 0.0\n",
    "            sum_filtered_probs = torch.sum(pred_token_probs)\n",
    "            if sum_filtered_probs > 1e-9: \n",
    "                 filtered_probs = pred_token_probs / sum_filtered_probs\n",
    "                 next_token_id = torch.multinomial(filtered_probs, num_samples=1).item()\n",
    "            else: \n",
    "                 print(\"Warning: Top-p resulted in zero probability sum. Using most likely token.\")\n",
    "                 next_token_id = sorted_indices[0].item()\n",
    "\n",
    "        trg_indices.append(next_token_id)\n",
    "\n",
    "        if next_token_id == EOS_IDX:\n",
    "            break\n",
    "\n",
    "    trg_tokens_text = sequence_to_text_words(trg_indices, word_inv_vocab) \n",
    "    return trg_tokens_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74efcaf6",
   "metadata": {
    "papermill": {
     "duration": 0.028369,
     "end_time": "2025-04-22T15:30:20.465214",
     "exception": false,
     "start_time": "2025-04-22T15:30:20.436845",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 12. Testing the Chatbot (FastText)\n",
    "Load the best model saved during training and generate responses for test prompts using the word-level inference function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a03c22d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T15:30:20.523890Z",
     "iopub.status.busy": "2025-04-22T15:30:20.523674Z",
     "iopub.status.idle": "2025-04-22T15:30:20.529605Z",
     "shell.execute_reply": "2025-04-22T15:30:20.528956Z"
    },
    "papermill": {
     "duration": 0.036701,
     "end_time": "2025-04-22T15:30:20.530676",
     "exception": false,
     "start_time": "2025-04-22T15:30:20.493975",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from nltk.translate.bleu_score import corpus_bleu, sentence_bleu\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# import random\n",
    "# from tqdm.auto import tqdm\n",
    "# import nltk \n",
    "\n",
    "\n",
    "# def calculate_bleu(\n",
    "#     input_sentences, \n",
    "#     target_sentences, \n",
    "#     model,\n",
    "#     tokenizer,\n",
    "#     device,\n",
    "#     max_len=MAX_LEN,\n",
    "#     strategy='greedy',\n",
    "#     k=10,\n",
    "#     p=0.9,\n",
    "#     temperature=1.0,\n",
    "#     max_examples=None):\n",
    "#     \"\"\"\n",
    "#     Calculates BLEU score over a dataset provided as separate input and target lists.\n",
    "\n",
    "#     Args:\n",
    "#         input_sentences (list): List of source sentences (strings).\n",
    "#         target_sentences (list): List of corresponding target/reference sentences (strings).\n",
    "#         model: The trained transformer model.\n",
    "#         tokenizer: The tokenizer instance used by the generation function.\n",
    "#         device: The device ('cuda' or 'cpu').\n",
    "#         max_len: Maximum sequence length for generation.\n",
    "#         strategy: Sampling strategy ('greedy', 'topk', 'topp').\n",
    "#         k: k for top-k sampling.\n",
    "#         p: p for top-p sampling.\n",
    "#         temperature: Temperature for sampling.\n",
    "#         max_examples (int, optional): Limit the number of examples to evaluate. Defaults to None (evaluate all).\n",
    "\n",
    "#     Returns:\n",
    "#         tuple: (corpus_bleu_score, list_of_details)\n",
    "#                corpus_bleu_score (float): The corpus BLEU-4 score (0.0-1.0).\n",
    "#                list_of_details (list): List of tuples [(source, reference, candidate, sentence_bleu), ...]\n",
    "#     \"\"\"\n",
    "#     assert len(input_sentences) == len(target_sentences), \\\n",
    "#         f\"Input and target sentence lists must have the same length ({len(input_sentences)} != {len(target_sentences)})\"\n",
    "\n",
    "#     targets = []     \n",
    "#     predictions = []  \n",
    "#     details = []      \n",
    "\n",
    "#     model.eval() \n",
    "\n",
    "#     eval_data_pairs = list(zip(input_sentences, target_sentences))\n",
    "\n",
    "#     if max_examples is not None and len(eval_data_pairs) > max_examples:\n",
    "#         print(f\"Evaluating on a random sample of {max_examples} examples.\")\n",
    "#         eval_data_pairs = random.sample(eval_data_pairs, max_examples)\n",
    "#     elif max_examples is None:\n",
    "#          print(f\"Evaluating on all {len(eval_data_pairs)} examples.\")\n",
    "#     else: # max_examples >= len(data)\n",
    "#          print(f\"Evaluating on all {len(eval_data_pairs)} examples (max_examples={max_examples}).\")\n",
    "\n",
    "\n",
    "#     try:\n",
    "#         nltk.data.find('tokenizers/punkt')\n",
    "#         tokenize_for_bleu = word_tokenize\n",
    "#     except LookupError:\n",
    "#         print(\"NLTK 'punkt' tokenizer model not found. Downloading...\")\n",
    "#         nltk.download('punkt')\n",
    "#         tokenize_for_bleu = word_tokenize\n",
    "#     except Exception as e:\n",
    "#         print(f\"Could not initialize nltk.word_tokenize ({e}). Falling back to simple split().\")\n",
    "#         tokenize_for_bleu = lambda s: s.split()\n",
    "\n",
    "\n",
    "#     print(f\"Generating responses using strategy='{strategy}', temp={temperature}\" +\n",
    "#           (f\", k={k}\" if strategy=='topk' else \"\") +\n",
    "#           (f\", p={p}\" if strategy=='topp' else \"\"))\n",
    "\n",
    "#     for source_text, target_text in tqdm(eval_data_pairs, desc=\"Calculating BLEU\"):\n",
    "\n",
    "#         candidate_text = generate_response_sampling(\n",
    "#             sentence=source_text,\n",
    "#             model=model,\n",
    "#             tokenizer=tokenizer, \n",
    "#             device=device,\n",
    "#             max_len=max_len,\n",
    "#             strategy=strategy,\n",
    "#             k=k,\n",
    "#             p=p,\n",
    "#             temperature=temperature\n",
    "#         )\n",
    "\n",
    "#         if candidate_text is None:\n",
    "#             candidate_text = \"\" \n",
    "\n",
    "#         try:\n",
    "#             target_text_str = str(target_text) if target_text is not None else \"\"\n",
    "#             tokenized_target = [tokenize_for_bleu(target_text_str.lower())] \n",
    "#         except Exception as e:\n",
    "#              print(f\"Warning: Error tokenizing target text '{target_text}': {e}. Using empty reference.\")\n",
    "#              tokenized_target = [[]]\n",
    "#              target_text_str = \"\" \n",
    "\n",
    "#         try:\n",
    "#             candidate_text_str = str(candidate_text) if candidate_text is not None else \"\"\n",
    "#             tokenized_candidate = tokenize_for_bleu(candidate_text_str.lower())\n",
    "#         except Exception as e:\n",
    "#             print(f\"Warning: Error tokenizing candidate text '{candidate_text}': {e}. Using empty candidate.\")\n",
    "#             tokenized_candidate = []\n",
    "#             candidate_text_str = \"\" \n",
    "\n",
    "\n",
    "#         sent_bleu = 0.0\n",
    "#         if tokenized_candidate and tokenized_target and tokenized_target[0]:\n",
    "#              try:\n",
    "#                  sent_bleu = sentence_bleu(tokenized_target, tokenized_candidate,\n",
    "#                                            smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method1)\n",
    "#              except Exception as e:\n",
    "#                  print(f\"Error calculating sentence BLEU for cand='{candidate_text_str}', ref='{target_text_str}': {e}\")\n",
    "#                  sent_bleu = 0.0\n",
    "\n",
    "#         targets.append(tokenized_target)\n",
    "#         predictions.append(tokenized_candidate)\n",
    "\n",
    "#         details.append((source_text, target_text_str, candidate_text_str, sent_bleu))\n",
    "\n",
    "#     try:\n",
    "#         smoothing = nltk.translate.bleu_score.SmoothingFunction().method1\n",
    "#         corpus_bleu_score = corpus_bleu(targets, predictions, smoothing_function=smoothing)\n",
    "#     except ZeroDivisionError:\n",
    "#         print(\"Warning: ZeroDivisionError during corpus BLEU calculation. Check if predictions or references are systematically empty.\")\n",
    "#         corpus_bleu_score = 0.0\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error calculating corpus BLEU: {e}\")\n",
    "#         corpus_bleu_score = 0.0\n",
    "\n",
    "\n",
    "#     return corpus_bleu_score, details\n",
    "\n",
    "# print(\"Extracting Training pairs...\")\n",
    "# train_inputs, train_targets = extract_pairs_corrected('train', empathetic_dialogues, subset_size=DATASET_SUBSET_SIZE)\n",
    "\n",
    "# print(\"\\nExtracting Validation pairs...\")\n",
    "# val_inputs, val_targets = extract_pairs_corrected('validation', empathetic_dialogues, subset_size=None) # Use full validation set\n",
    "\n",
    "# print(\"\\nCalculating BLEU score on the VALIDATION data...\")\n",
    "# model.to(device)\n",
    "# val_bleu_greedy, val_details_greedy = calculate_bleu(\n",
    "#     input_sentences=val_inputs,   \n",
    "#     target_sentences=val_targets, \n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     device=device,\n",
    "#     max_len=MAX_LEN,\n",
    "#     strategy='greedy',\n",
    "#     temperature=1.0,\n",
    "#     max_examples=100 \n",
    "# )\n",
    "# print(f\"\\nValidation Corpus BLEU Score (Greedy): {val_bleu_greedy * 100:.2f}\")\n",
    "\n",
    "# val_bleu_topp, val_details_topp = calculate_bleu(\n",
    "#     input_sentences=val_inputs[:100],   \n",
    "#     target_sentences=val_targets[:100], \n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     device=device,\n",
    "#     max_len=MAX_LEN,\n",
    "#     strategy='topp',\n",
    "#     p=0.9,\n",
    "#     temperature=0.7,\n",
    "#     max_examples=100 \n",
    "# )\n",
    "# print(f\"Validation Corpus BLEU Score (Top-p, p=0.9, T=0.7): {val_bleu_topp * 100:.2f}\")\n",
    "\n",
    "# print(\"\\nCalculating BLEU score on the TRAINING data...\")\n",
    "# train_bleu_greedy, train_details_greedy = calculate_bleu(\n",
    "#     input_sentences=train_inputs[:100],   \n",
    "#     target_sentences=train_targets[:100], \n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     device=device,\n",
    "#     max_len=MAX_LEN,\n",
    "#     strategy='greedy',\n",
    "#     temperature=1.0,\n",
    "#     max_examples=100 \n",
    "# )\n",
    "# print(f\"\\nTraining Corpus BLEU Score (Greedy, sample): {train_bleu_greedy * 100:.2f}\")\n",
    "\n",
    "\n",
    "# print(\"\\nExample Validation Generations (Top-p):\")\n",
    "# for i, (src, ref, cand, s_bleu) in enumerate(val_details_topp[:5]): \n",
    "#     print(f\"--- Example {i+1} ---\")\n",
    "#     print(f\"  Source:    {src}\")\n",
    "#     print(f\"  Reference: {ref}\")\n",
    "#     print(f\"  Candidate: {cand}\")\n",
    "#     print(f\"  Sentence BLEU: {s_bleu*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "de987451",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T15:30:20.589776Z",
     "iopub.status.busy": "2025-04-22T15:30:20.589574Z",
     "iopub.status.idle": "2025-04-22T15:30:20.593168Z",
     "shell.execute_reply": "2025-04-22T15:30:20.592678Z"
    },
    "papermill": {
     "duration": 0.034609,
     "end_time": "2025-04-22T15:30:20.594254",
     "exception": false,
     "start_time": "2025-04-22T15:30:20.559645",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_prompts = [\n",
    "    \"I just got a promotion at work! I'm so happy.\",\n",
    "    \"I feel really lonely these days. No one seems to understand me.\",\n",
    "    \"My best friend just moved to another country, and I miss them so much.\",\n",
    "    \"I failed my exam even after studying so hard. I feel so disappointed.\",\n",
    "    \"I helped a stranger today, and it made me feel really good.\",\n",
    "    \"I'm really nervous about my job interview tomorrow.\",\n",
    "    \"I just finished a marathon! I feel so accomplished.\",\n",
    "    \"My pet passed away last night, and I’m heartbroken.\",\n",
    "    \"I got stuck in traffic for hours today. It was so frustrating!\",\n",
    "    \"I found out I’m going to be a parent! I’m overjoyed but also a little scared.\",\n",
    "    \"I’ve been feeling really unmotivated lately. I don’t know what to do.\",\n",
    "    \"I had a great conversation with an old friend today. It felt amazing!\",\n",
    "    \"I lost my wallet today. Now I have to replace everything.\",\n",
    "    \"I just tried a new hobby, and I think I love it!\",\n",
    "    \"Someone criticized my work today, and it made me feel insecure.\",\n",
    "    \"I’m struggling with my mental health, and I don’t know how to talk about it.\",\n",
    "    \"My birthday is coming up, but I don’t feel excited this year.\",\n",
    "    \"I finally confronted someone who hurt me in the past. It was really hard.\",\n",
    "    \"I got my dream job! I can’t believe it’s happening.\",\n",
    "    \"I feel so stuck in life. I don’t know what my next step should be.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "52611ffe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T15:30:20.653024Z",
     "iopub.status.busy": "2025-04-22T15:30:20.652799Z",
     "iopub.status.idle": "2025-04-22T15:30:22.616098Z",
     "shell.execute_reply": "2025-04-22T15:30:22.615391Z"
    },
    "papermill": {
     "duration": 1.994177,
     "end_time": "2025-04-22T15:30:22.617349",
     "exception": false,
     "start_time": "2025-04-22T15:30:20.623172",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chatbot Test (FastText Embeddings, Best Model) ---\n",
      "Loading best model state from fasttext-transformer-basic-best.pt...\n",
      "Model loaded successfully using weights_only=True.\n",
      "\n",
      "--- Testing with Top-k (k=10, temp=0.8) ---\n",
      "User: I just got a promotion at work! I'm so happy.\n",
      "Bot:  what did you do?\n",
      "--------------------\n",
      "User: I feel really lonely these days. No one seems to understand me.\n",
      "Bot:  i hope you feel better soon.\n",
      "--------------------\n",
      "User: My best friend just moved to another country, and I miss them so much.\n",
      "Bot:  i love them as much as well as a hobby?\n",
      "--------------------\n",
      "User: I failed my exam even after studying so hard. I feel so disappointed.\n",
      "Bot:  i hope it was a good day?\n",
      "--------------------\n",
      "User: I helped a stranger today, and it made me feel really good.\n",
      "Bot:  i know it is a good feeling about the same! i am happy for you.\n",
      "--------------------\n",
      "User: I'm really nervous about my job interview tomorrow.\n",
      "Bot:  that's awesome. did you get to the job?\n",
      "--------------------\n",
      "User: I just finished a marathon! I feel so accomplished.\n",
      "Bot:  that's really great! i bet you are excited\n",
      "--------------------\n",
      "User: My pet passed away last night, and I’m heartbroken.\n",
      "Bot:  wowcomma that's a relief! how did it go?\n",
      "--------------------\n",
      "User: I got stuck in traffic for hours today. It was so frustrating!\n",
      "Bot:  i bet that was so embarrassing. did you have a big time?\n",
      "--------------------\n",
      "User: I found out I’m going to be a parent! I’m overjoyed but also a little scared.\n",
      "Bot:  oh no! that's awful. how do you feel about it?\n",
      "--------------------\n",
      "User: I’ve been feeling really unmotivated lately. I don’t know what to do.\n",
      "Bot:  you can get another doggo?\n",
      "--------------------\n",
      "User: I had a great conversation with an old friend today. It felt amazing!\n",
      "Bot:  that's awesome! what are you going to do?\n",
      "--------------------\n",
      "User: I lost my wallet today. Now I have to replace everything.\n",
      "Bot:  i hate when that happens.\n",
      "--------------------\n",
      "User: I just tried a new hobby, and I think I love it!\n",
      "Bot:  it is good to hear! i hope it works out for you.\n",
      "--------------------\n",
      "User: Someone criticized my work today, and it made me feel insecure.\n",
      "Bot:  what was it about?\n",
      "--------------------\n",
      "User: I’m struggling with my mental health, and I don’t know how to talk about it.\n",
      "Bot:  that is a good attitude to have!\n",
      "--------------------\n",
      "User: My birthday is coming up, but I don’t feel excited this year.\n",
      "Bot:  that's good that you did great! i have a great time!\n",
      "--------------------\n",
      "User: I finally confronted someone who hurt me in the past. It was really hard.\n",
      "Bot:  i would have been grossed out if i could not get into the <UNK>\n",
      "--------------------\n",
      "User: I got my dream job! I can’t believe it’s happening.\n",
      "Bot:  that is so sweet. i bet you were so proud of her.\n",
      "--------------------\n",
      "User: I feel so stuck in life. I don’t know what my next step should be.\n",
      "Bot:  i am glad you have a great time!\n",
      "--------------------\n",
      "\n",
      "--- Testing with Top-p (p=0.9, temp=0.8) ---\n",
      "User: I just got a promotion at work! I'm so happy.\n",
      "Bot:  wow! what kind of car did you get?\n",
      "--------------------\n",
      "User: I feel really lonely these days. No one seems to understand me.\n",
      "Bot:  i like that\n",
      "--------------------\n",
      "User: My best friend just moved to another country, and I miss them so much.\n",
      "Bot:  that's good to hear. hopefully you can find someone soon.\n",
      "--------------------\n",
      "User: I failed my exam even after studying so hard. I feel so disappointed.\n",
      "Bot:  that's terrible. how did you feel about it?\n",
      "--------------------\n",
      "User: I helped a stranger today, and it made me feel really good.\n",
      "Bot:  that's so true! i hope you can enjoy your vacation!\n",
      "--------------------\n",
      "User: I'm really nervous about my job interview tomorrow.\n",
      "Bot:  you must have been very proud of her!\n",
      "--------------------\n",
      "User: I just finished a marathon! I feel so accomplished.\n",
      "Bot:  that is greatcomma i hope you have a great time!\n",
      "--------------------\n",
      "User: My pet passed away last night, and I’m heartbroken.\n",
      "Bot:  oh no! what happened?\n",
      "--------------------\n",
      "User: I got stuck in traffic for hours today. It was so frustrating!\n",
      "Bot:  that is awesome i am sorry to hear that. i hope you are able to get another dog and he has the best time you will have an experience fixed it\n",
      "--------------------\n",
      "User: I found out I’m going to be a parent! I’m overjoyed but also a little scared.\n",
      "Bot:  oh wow! that's a relief! i hope you find someone else soon\n",
      "--------------------\n",
      "User: I’ve been feeling really unmotivated lately. I don’t know what to do.\n",
      "Bot:  i love that too. i love that would be a good <UNK>\n",
      "--------------------\n",
      "User: I had a great conversation with an old friend today. It felt amazing!\n",
      "Bot:  what did you have to do after?\n",
      "--------------------\n",
      "User: I lost my wallet today. Now I have to replace everything.\n",
      "Bot:  oh nocomma that is terrible. how did it turn out?\n",
      "--------------------\n",
      "User: I just tried a new hobby, and I think I love it!\n",
      "Bot:  you are welcomecomma how are you going to do?\n",
      "--------------------\n",
      "User: Someone criticized my work today, and it made me feel insecure.\n",
      "Bot:  oh no! what kind of restaurant was it?\n",
      "--------------------\n",
      "User: I’m struggling with my mental health, and I don’t know how to talk about it.\n",
      "Bot:  i understand that. but i am so sorry you had to be able to have some good memories and your life.\n",
      "--------------------\n",
      "User: My birthday is coming up, but I don’t feel excited this year.\n",
      "Bot:  i would be toocomma why would you do?\n",
      "--------------------\n",
      "User: I finally confronted someone who hurt me in the past. It was really hard.\n",
      "Bot:  i am so sorry to hear that. i hope he learns his lesson!\n",
      "--------------------\n",
      "User: I got my dream job! I can’t believe it’s happening.\n",
      "Bot:  oh that's awesome. are you able to find him?\n",
      "--------------------\n",
      "User: I feel so stuck in life. I don’t know what my next step should be.\n",
      "Bot:  i am sure it will get some good times\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Chatbot Test (FastText Embeddings, Best Model) ---\")\n",
    "\n",
    "if 'model' in locals() or 'model' in globals():\n",
    "    if os.path.exists(model_save_path_best):\n",
    "        print(f\"Loading best model state from {model_save_path_best}...\")\n",
    "        try:\n",
    "            model.load_state_dict(torch.load(model_save_path_best, map_location=device, weights_only=True))\n",
    "            print(\"Model loaded successfully using weights_only=True.\")\n",
    "        except (RuntimeError, TypeError, KeyError, AttributeError) as e: \n",
    "            print(f\"Could not load with weights_only=True ({e}). Trying default loading...\")\n",
    "            try:\n",
    "                 model.load_state_dict(torch.load(model_save_path_best, map_location=device))\n",
    "                 print(\"Model loaded successfully using default loading.\")\n",
    "            except Exception as e_fallback:\n",
    "                 print(f\"ERROR: Failed to load model state dict: {e_fallback}\")\n",
    "                 raise e_fallback\n",
    "    else:\n",
    "         print(f\"Warning: Best model file '{model_save_path_best}' not found.\")\n",
    "         print(\"Testing with the model's current state (might be untrained or from last epoch).\")\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    if 'word_vocab' in locals() and 'word_inv_vocab' in locals():\n",
    "        print(\"\\n--- Testing with Top-k (k=10, temp=0.8) ---\")\n",
    "        for prompt in test_prompts:\n",
    "            response = generate_response_sampling_words( \n",
    "                prompt, model, word_vocab, word_inv_vocab, device, max_len=MAX_LEN,\n",
    "                strategy=\"topk\", k=10, temperature=0.8\n",
    "            )\n",
    "            print(f\"User: {prompt}\")\n",
    "            print(f\"Bot:  {response}\")\n",
    "            print(\"-\" * 20)\n",
    "\n",
    "        print(\"\\n--- Testing with Top-p (p=0.9, temp=0.8) ---\")\n",
    "        for prompt in test_prompts:\n",
    "            response = generate_response_sampling_words( \n",
    "                prompt, model, word_vocab, word_inv_vocab, device, max_len=MAX_LEN,\n",
    "                strategy=\"topp\", p=0.9, temperature=0.8\n",
    "            )\n",
    "            print(f\"User: {prompt}\")\n",
    "            print(f\"Bot:  {response}\")\n",
    "            print(\"-\" * 20)\n",
    "    else:\n",
    "        print(\"ERROR: Word vocabulary not found. Cannot run inference.\")\n",
    "else:\n",
    "    print(\"ERROR: Model not defined. Cannot run inference.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d805795b",
   "metadata": {
    "papermill": {
     "duration": 0.02949,
     "end_time": "2025-04-22T15:30:22.677373",
     "exception": false,
     "start_time": "2025-04-22T15:30:22.647883",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5504,
     "sourceId": 8240,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 14154,
     "sourceId": 19053,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7848.262737,
   "end_time": "2025-04-22T15:30:26.722058",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-22T13:19:38.459321",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "010acd2dc58f4615baac9345630415a5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "04ba45ae86be47529186101032518f80": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ed582f60f99a4a0bba4947108725e623",
       "placeholder": "​",
       "style": "IPY_MODEL_ba5a9b1293b7411eb54ed82d49276f6d",
       "tabbable": null,
       "tooltip": null,
       "value": "Calculating BLEU: 100%"
      }
     },
     "067eff80583e477092ef5196289ce8ff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f3b9cbc1a1de451cb05fdd5896d1e388",
       "placeholder": "​",
       "style": "IPY_MODEL_ad9bf91a1b4d4a038ac3a77dc553d36f",
       "tabbable": null,
       "tooltip": null,
       "value": " 1/1 [00:00&lt;00:00, 202.50it/s]"
      }
     },
     "07125736f95345858c6200a7a7c3d39e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_16933901dc284e0d9741b9e06bc8484c",
       "max": 100.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f394beab8cf643c2bd99cdb016bfd50b",
       "tabbable": null,
       "tooltip": null,
       "value": 100.0
      }
     },
     "0749b99d977c404194e641d66fc20bbe": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0a423606ff4f413dbc9d4f925522bbd2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_40ae4e31519344fa97f8a2cd0a12aad8",
       "max": 28022709.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f060404dd8c54697bae581fd459c25a9",
       "tabbable": null,
       "tooltip": null,
       "value": 28022709.0
      }
     },
     "10e60c00c0b74a22a3d73238bb3c359b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ddc4a33eddb44e83b1a4e9f316cff7bd",
       "placeholder": "​",
       "style": "IPY_MODEL_d8e77360ccad46a08bea68e1cb35a4a5",
       "tabbable": null,
       "tooltip": null,
       "value": " 4.51k/4.51k [00:00&lt;00:00, 580kB/s]"
      }
     },
     "11866f4685f74d139487ff87e44b97bc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "126d621e1bb949c695cb33e11426cb92": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "131e64d48bf74e85a315e24d2b289181": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2683d53815ca4fe1a723f6205d52da4d",
       "placeholder": "​",
       "style": "IPY_MODEL_a1ecec9e00894f26b3ab653d4f5a27c2",
       "tabbable": null,
       "tooltip": null,
       "value": " 9267/9267 [10:15&lt;00:00, 14.71it/s]"
      }
     },
     "1689ff76320e4bf0b22a661df51ccbf7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "16933901dc284e0d9741b9e06bc8484c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1953fddb269142708d0944f66d3e8281": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1df30100e7de4cdd9d808a3ac0990cf4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_770eddb5db774b82b051fce30b4a5713",
       "placeholder": "​",
       "style": "IPY_MODEL_c054f40681b04e93af50389696377b8c",
       "tabbable": null,
       "tooltip": null,
       "value": " 28.0M/28.0M [00:00&lt;00:00, 87.8MB/s]"
      }
     },
     "1f23b93612cc437b957b7f846e0e93e1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2082308e433a4aed9419c974bd240c44": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "214faa08f5664589b4c580261cc9d6f1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_fb24ff609d3a4dedb9c67d802e679ef1",
        "IPY_MODEL_a7f5a5524dde4fda9c226827939639e3",
        "IPY_MODEL_10e60c00c0b74a22a3d73238bb3c359b"
       ],
       "layout": "IPY_MODEL_22e9afd78f4d43b4a0ca3357f1e58513",
       "tabbable": null,
       "tooltip": null
      }
     },
     "216e2b1ac4b14400a1be601710bf41f8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_eac310d7ed674473b59a3baec14a74e0",
        "IPY_MODEL_755c77c1aaab440cabb97b1d2d4a3976",
        "IPY_MODEL_067eff80583e477092ef5196289ce8ff"
       ],
       "layout": "IPY_MODEL_1f23b93612cc437b957b7f846e0e93e1",
       "tabbable": null,
       "tooltip": null
      }
     },
     "22e9afd78f4d43b4a0ca3357f1e58513": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "237e64144f6840249fc1e867aa7c49fc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "2683d53815ca4fe1a723f6205d52da4d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "26e2b0929dbf47fa8012af8132ad1db7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_76d2a37b28be4ecb98e1486ffa575e04",
        "IPY_MODEL_7f48de88f5e747c3bed5b46c1481ac64",
        "IPY_MODEL_55b77934ef07470e812ffdcbd40e9215"
       ],
       "layout": "IPY_MODEL_1953fddb269142708d0944f66d3e8281",
       "tabbable": null,
       "tooltip": null
      }
     },
     "27b7afc6e03e40519980fb6e1cc5e7f0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_4b9b0cb440cf4fa5bcbfcab8e8902340",
        "IPY_MODEL_86182f6c905e4089a532835df26ddd30",
        "IPY_MODEL_31d69b01bf8140a0bd0299e947c6ed04"
       ],
       "layout": "IPY_MODEL_0749b99d977c404194e641d66fc20bbe",
       "tabbable": null,
       "tooltip": null
      }
     },
     "2eb7388f14ec4c7cb4419e0d79ec1461": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b8135f5cfba54766857c4e1528d57d62",
       "placeholder": "​",
       "style": "IPY_MODEL_2082308e433a4aed9419c974bd240c44",
       "tabbable": null,
       "tooltip": null,
       "value": "Generating test split: 100%"
      }
     },
     "2f38ac54092f49b1926181a1077d7de0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "312275533d684770b2128b8c4872e961": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "31d69b01bf8140a0bd0299e947c6ed04": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_bd15a09d9a5b4685a752eac959ae3c35",
       "placeholder": "​",
       "style": "IPY_MODEL_e07cd06f5a334d56b556f0d09b83fa6f",
       "tabbable": null,
       "tooltip": null,
       "value": " 76673/76673 [00:03&lt;00:00, 23780.06 examples/s]"
      }
     },
     "36cab4ec936842268e04d0e1fc5ee518": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "38e1f1a81cb14729a50c75d4da11779d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_fa26a5fb569647dcbfdb53eba81b4122",
       "placeholder": "​",
       "style": "IPY_MODEL_8747452acc50476081f28e06bab048ad",
       "tabbable": null,
       "tooltip": null,
       "value": "Downloading data: 100%"
      }
     },
     "3c4da8e9d577457bac1b49ddc8968764": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "3edb16484f454ec4836c9a1fd63ec86d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_2eb7388f14ec4c7cb4419e0d79ec1461",
        "IPY_MODEL_cc3378b682a74de1a02d65efc20f7633",
        "IPY_MODEL_59620cd55f274e2dbe0c7636e33fdcc1"
       ],
       "layout": "IPY_MODEL_4ce72c2db30d4a0b8c8c5eb8986f46c6",
       "tabbable": null,
       "tooltip": null
      }
     },
     "407ca254f587420b9d4f51a880a3be97": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "40ae4e31519344fa97f8a2cd0a12aad8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4273308a56c244208381460cb7a9398d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4432cc29965846f4b35bb5d60ed2a070": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "46e1f8362739447a9e95bcf5c22dee01": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "49123588212546ba86c433e47790f0ad": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4b9b0cb440cf4fa5bcbfcab8e8902340": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_59c8b82ad5d143dabcdedf19f1f8250d",
       "placeholder": "​",
       "style": "IPY_MODEL_3c4da8e9d577457bac1b49ddc8968764",
       "tabbable": null,
       "tooltip": null,
       "value": "Generating train split: 100%"
      }
     },
     "4ce72c2db30d4a0b8c8c5eb8986f46c6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "52aa861f6d4c4c3ab1c657195d0e19e3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a881f9cf3f314567b998b02b7af57837",
        "IPY_MODEL_d59c574c67094d068433f5843a015009",
        "IPY_MODEL_b16ab03c6d3040c390bfd79164f08fe4"
       ],
       "layout": "IPY_MODEL_1689ff76320e4bf0b22a661df51ccbf7",
       "tabbable": null,
       "tooltip": null
      }
     },
     "55b77934ef07470e812ffdcbd40e9215": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_126d621e1bb949c695cb33e11426cb92",
       "placeholder": "​",
       "style": "IPY_MODEL_ab117741d61743648d257483769bf98b",
       "tabbable": null,
       "tooltip": null,
       "value": " 9267/9267 [09:21&lt;00:00, 16.05it/s]"
      }
     },
     "59620cd55f274e2dbe0c7636e33fdcc1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6ec71ab51e3747018efb35505aba4c6d",
       "placeholder": "​",
       "style": "IPY_MODEL_237e64144f6840249fc1e867aa7c49fc",
       "tabbable": null,
       "tooltip": null,
       "value": " 10943/10943 [00:00&lt;00:00, 11311.10 examples/s]"
      }
     },
     "59c8b82ad5d143dabcdedf19f1f8250d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5b12930c63914d20b2ecc807761115f1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5c00555fd2394081b7b36c78b23824fe": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5e19c4d392404322b91dd1d28846e237": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_38e1f1a81cb14729a50c75d4da11779d",
        "IPY_MODEL_0a423606ff4f413dbc9d4f925522bbd2",
        "IPY_MODEL_1df30100e7de4cdd9d808a3ac0990cf4"
       ],
       "layout": "IPY_MODEL_7e31e4cf51244de0a77529e87f18dd02",
       "tabbable": null,
       "tooltip": null
      }
     },
     "5f640ecd4b5a4e6cbba9a61b5d89b182": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "67db4fcd45f04b8da8fe17a79d63280f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "6ad869d382e741b68d76fd544e89313c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_04ba45ae86be47529186101032518f80",
        "IPY_MODEL_fa70d628767b402b91109bb14fb9e642",
        "IPY_MODEL_131e64d48bf74e85a315e24d2b289181"
       ],
       "layout": "IPY_MODEL_cd5a3f16765b48d8adee4f59454a9213",
       "tabbable": null,
       "tooltip": null
      }
     },
     "6d45031e952c4b60a5665e55af7d7ca7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6e2f43106a31400cab2b72296e372525": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e2f9670f03d14452aac9cbcc8f007b89",
        "IPY_MODEL_07125736f95345858c6200a7a7c3d39e",
        "IPY_MODEL_cebc82735be44e428e1ff00d3ba72f3d"
       ],
       "layout": "IPY_MODEL_e42dcce6208b4f63b4e0e7632721f3b8",
       "tabbable": null,
       "tooltip": null
      }
     },
     "6ec71ab51e3747018efb35505aba4c6d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "72c303a23cab4f2fa7130251aa61aa87": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "755c77c1aaab440cabb97b1d2d4a3976": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_407ca254f587420b9d4f51a880a3be97",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_72c303a23cab4f2fa7130251aa61aa87",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "76d2a37b28be4ecb98e1486ffa575e04": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_36cab4ec936842268e04d0e1fc5ee518",
       "placeholder": "​",
       "style": "IPY_MODEL_b7f02d5e36b34178be87ffa40fb5a16d",
       "tabbable": null,
       "tooltip": null,
       "value": "Calculating BLEU: 100%"
      }
     },
     "770eddb5db774b82b051fce30b4a5713": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "79abd248fd324e75879dea8ec9a9e360": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_312275533d684770b2128b8c4872e961",
       "placeholder": "​",
       "style": "IPY_MODEL_49123588212546ba86c433e47790f0ad",
       "tabbable": null,
       "tooltip": null,
       "value": " 12030/12030 [00:01&lt;00:00, 11247.03 examples/s]"
      }
     },
     "7e15685f46764a0abcf8bfd51bde1080": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7e31e4cf51244de0a77529e87f18dd02": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7f48de88f5e747c3bed5b46c1481ac64": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ccac1c78753b4ad6bd7315159532712a",
       "max": 9267.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_2f38ac54092f49b1926181a1077d7de0",
       "tabbable": null,
       "tooltip": null,
       "value": 9267.0
      }
     },
     "86182f6c905e4089a532835df26ddd30": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9dfab3384b3b4ba587613caa8bc9b482",
       "max": 76673.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_11866f4685f74d139487ff87e44b97bc",
       "tabbable": null,
       "tooltip": null,
       "value": 76673.0
      }
     },
     "8747452acc50476081f28e06bab048ad": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8a8c8d8970794055a55f80ae60bfa980": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8b99411a704c46848264efb7eb1a2976": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8dc070512e6c40ddb805138d0426d29a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f9317dc02b104a8db6dde308d8a0bf88",
       "max": 12030.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_4432cc29965846f4b35bb5d60ed2a070",
       "tabbable": null,
       "tooltip": null,
       "value": 12030.0
      }
     },
     "96e8f14556c649b6aa75c4f7d5bc3b80": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "9bb1930cbc4d4522b345cfa4ce06572e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9cd1d6a67fec44e1863f20f80f406efd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9cde3b5bad684686888ee8647fc5653e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9dfab3384b3b4ba587613caa8bc9b482": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9f3f7734caeb4e31ba023f07292b0104": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9ff8b176e3924d74a40946e1f1385ddc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a1ecec9e00894f26b3ab653d4f5a27c2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a7f5a5524dde4fda9c226827939639e3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d73879b228b843fa881a44a4b57c5df6",
       "max": 4514.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f5f8b9c760df4ae1af757cd7a7dfdc21",
       "tabbable": null,
       "tooltip": null,
       "value": 4514.0
      }
     },
     "a881f9cf3f314567b998b02b7af57837": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_4273308a56c244208381460cb7a9398d",
       "placeholder": "​",
       "style": "IPY_MODEL_9cde3b5bad684686888ee8647fc5653e",
       "tabbable": null,
       "tooltip": null,
       "value": "README.md: 100%"
      }
     },
     "ab117741d61743648d257483769bf98b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ad9bf91a1b4d4a038ac3a77dc553d36f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ae70a68fb8fc4396b879e739bbae7a61": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "b16ab03c6d3040c390bfd79164f08fe4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9ff8b176e3924d74a40946e1f1385ddc",
       "placeholder": "​",
       "style": "IPY_MODEL_5b12930c63914d20b2ecc807761115f1",
       "tabbable": null,
       "tooltip": null,
       "value": " 7.15k/7.15k [00:00&lt;00:00, 702kB/s]"
      }
     },
     "b4f2ec44ffcd4b188713ec0bfc81aa29": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b7f02d5e36b34178be87ffa40fb5a16d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b8135f5cfba54766857c4e1528d57d62": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ba5a9b1293b7411eb54ed82d49276f6d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "bd15a09d9a5b4685a752eac959ae3c35": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c054f40681b04e93af50389696377b8c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c874328f47c64cd3ae0f05490c9830b4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_5f640ecd4b5a4e6cbba9a61b5d89b182",
       "placeholder": "​",
       "style": "IPY_MODEL_46e1f8362739447a9e95bcf5c22dee01",
       "tabbable": null,
       "tooltip": null,
       "value": "Generating validation split: 100%"
      }
     },
     "cc3378b682a74de1a02d65efc20f7633": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_5c00555fd2394081b7b36c78b23824fe",
       "max": 10943.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_96e8f14556c649b6aa75c4f7d5bc3b80",
       "tabbable": null,
       "tooltip": null,
       "value": 10943.0
      }
     },
     "ccac1c78753b4ad6bd7315159532712a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cd5a3f16765b48d8adee4f59454a9213": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cebc82735be44e428e1ff00d3ba72f3d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d6feca94fe5e4535a03c444c8bb6b7ed",
       "placeholder": "​",
       "style": "IPY_MODEL_9bb1930cbc4d4522b345cfa4ce06572e",
       "tabbable": null,
       "tooltip": null,
       "value": " 100/100 [00:06&lt;00:00, 17.55it/s]"
      }
     },
     "d530b46ce219481a9a1b13cf94a336f5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "d59c574c67094d068433f5843a015009": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6d45031e952c4b60a5665e55af7d7ca7",
       "max": 7152.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_ae70a68fb8fc4396b879e739bbae7a61",
       "tabbable": null,
       "tooltip": null,
       "value": 7152.0
      }
     },
     "d6feca94fe5e4535a03c444c8bb6b7ed": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d73879b228b843fa881a44a4b57c5df6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d8e77360ccad46a08bea68e1cb35a4a5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ddc4a33eddb44e83b1a4e9f316cff7bd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e07cd06f5a334d56b556f0d09b83fa6f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e2f9670f03d14452aac9cbcc8f007b89": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_010acd2dc58f4615baac9345630415a5",
       "placeholder": "​",
       "style": "IPY_MODEL_9cd1d6a67fec44e1863f20f80f406efd",
       "tabbable": null,
       "tooltip": null,
       "value": "Calculating BLEU: 100%"
      }
     },
     "e3e39131249642a38e703c5e5abb4bd3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_c874328f47c64cd3ae0f05490c9830b4",
        "IPY_MODEL_8dc070512e6c40ddb805138d0426d29a",
        "IPY_MODEL_79abd248fd324e75879dea8ec9a9e360"
       ],
       "layout": "IPY_MODEL_b4f2ec44ffcd4b188713ec0bfc81aa29",
       "tabbable": null,
       "tooltip": null
      }
     },
     "e42dcce6208b4f63b4e0e7632721f3b8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "eac310d7ed674473b59a3baec14a74e0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8a8c8d8970794055a55f80ae60bfa980",
       "placeholder": "​",
       "style": "IPY_MODEL_8b99411a704c46848264efb7eb1a2976",
       "tabbable": null,
       "tooltip": null,
       "value": "Computing checksums: 100%"
      }
     },
     "ed582f60f99a4a0bba4947108725e623": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f060404dd8c54697bae581fd459c25a9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "f394beab8cf643c2bd99cdb016bfd50b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "f3b9cbc1a1de451cb05fdd5896d1e388": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f5f8b9c760df4ae1af757cd7a7dfdc21": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "f9317dc02b104a8db6dde308d8a0bf88": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fa26a5fb569647dcbfdb53eba81b4122": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fa70d628767b402b91109bb14fb9e642": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9f3f7734caeb4e31ba023f07292b0104",
       "max": 9267.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d530b46ce219481a9a1b13cf94a336f5",
       "tabbable": null,
       "tooltip": null,
       "value": 9267.0
      }
     },
     "fb24ff609d3a4dedb9c67d802e679ef1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7e15685f46764a0abcf8bfd51bde1080",
       "placeholder": "​",
       "style": "IPY_MODEL_67db4fcd45f04b8da8fe17a79d63280f",
       "tabbable": null,
       "tooltip": null,
       "value": "empathetic_dialogues.py: 100%"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
